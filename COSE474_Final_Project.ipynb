{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3OMiXq2SjQ-4",
        "outputId": "170cadda-e7a7-429c-f789-e91d06751704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "fatal: destination path 'MIP' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP\n",
            "fatal: destination path 'Dassl.pytorch' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/Dassl.pytorch\n",
            "Requirement already satisfied: flake8==3.7.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.7.9)\n",
            "Requirement already satisfied: yapf==0.29.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.29.0)\n",
            "Requirement already satisfied: isort==4.3.21 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.3.21)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.1.8)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.19.0a20241209)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2024.9.11)\n",
            "Requirement already satisfied: wilds==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.3)\n",
            "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.26.4)\n",
            "Requirement already satisfied: ogb>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.3.6)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.2.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (11.0.0)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.20.1+cu121)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 11)) (0.2.13)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.2.3)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13)) (0.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP\n",
            "Requirement already satisfied: ftfy==6.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.66.6)\n",
            "Requirement already satisfied: learn2learn==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.1.1->-r requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: gym>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.25.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.20.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: gsutil in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (5.32)\n",
            "Requirement already satisfied: qpth>=0.0.15 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.0.18)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.0.8)\n",
            "Requirement already satisfied: cvxpy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.5.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.3.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (11.0.0)\n",
            "Requirement already satisfied: argcomplete>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.5.2)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.7)\n",
            "Requirement already satisfied: fasteners>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.19)\n",
            "Requirement already satisfied: gcs-oauth2-boto-plugin>=3.2 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2)\n",
            "Requirement already satisfied: google-apitools>=0.5.32 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.5.32)\n",
            "Requirement already satisfied: httplib2==0.20.4 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.20.4)\n",
            "Requirement already satisfied: google-reauth>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.1)\n",
            "Requirement already satisfied: monotonic>=1.4 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.6)\n",
            "Requirement already satisfied: pyOpenSSL<=24.2.1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (24.2.1)\n",
            "Requirement already satisfied: retry-decorator>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.1.1)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: google-auth==2.17.0 in /usr/local/lib/python3.10/dist-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.17.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.11.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2==0.20.4->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.8.30)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.6.7.post3)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.0.14)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2.7)\n",
            "Requirement already satisfied: boto>=2.29.1 in /usr/local/lib/python3.10/dist-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.49.0)\n",
            "Requirement already satisfied: oauth2client>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.1.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: pyu2f in /usr/local/lib/python3.10/dist-packages (from google-reauth>=0.1.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.5)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (43.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.17.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.7.post4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.22)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import tarfile\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/MyDrive/Colab Notebooks/\n",
        "\n",
        "!git clone https://github.com/hihello122/MIP.git\n",
        "%cd MIP/\n",
        "\n",
        "\n",
        "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
        "%cd Dassl.pytorch/\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "!cp -r dassl ../\n",
        "# Install this library (no need to re-build if the source code is modified)\n",
        "# !python setup.py develop\n",
        "%cd ..\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IQt24Tghz0sO",
        "outputId": "3db74811-5896-40d7-99d0-d49e988de910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/MIP\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/Dassl.pytorch\n",
            "Requirement already satisfied: flake8==3.7.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.7.9)\n",
            "Requirement already satisfied: yapf==0.29.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.29.0)\n",
            "Requirement already satisfied: isort==4.3.21 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.3.21)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.1.8)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.19.0a20241209)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2024.9.11)\n",
            "Requirement already satisfied: wilds==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.3)\n",
            "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.26.4)\n",
            "Requirement already satisfied: ogb>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.3.6)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.2.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (11.0.0)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.20.1+cu121)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->-r requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 11)) (0.2.13)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.2.3)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13)) (0.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP\n",
            "Requirement already satisfied: ftfy==6.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.66.6)\n",
            "Requirement already satisfied: learn2learn==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.1.1->-r requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: gym>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.25.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.20.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: gsutil in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (5.32)\n",
            "Requirement already satisfied: qpth>=0.0.15 in /usr/local/lib/python3.10/dist-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.0.18)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.0.8)\n",
            "Requirement already satisfied: cvxpy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.5.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.3.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (11.0.0)\n",
            "Requirement already satisfied: argcomplete>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.5.2)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.7)\n",
            "Requirement already satisfied: fasteners>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.19)\n",
            "Requirement already satisfied: gcs-oauth2-boto-plugin>=3.2 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2)\n",
            "Requirement already satisfied: google-apitools>=0.5.32 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.5.32)\n",
            "Requirement already satisfied: httplib2==0.20.4 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.20.4)\n",
            "Requirement already satisfied: google-reauth>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.1)\n",
            "Requirement already satisfied: monotonic>=1.4 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.6)\n",
            "Requirement already satisfied: pyOpenSSL<=24.2.1,>=0.13 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (24.2.1)\n",
            "Requirement already satisfied: retry-decorator>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.1.1)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: google-auth==2.17.0 in /usr/local/lib/python3.10/dist-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.17.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.7.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.11.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2==0.20.4->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.8.30)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.6.7.post3)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.0.14)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.10/dist-packages (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2.7)\n",
            "Requirement already satisfied: boto>=2.29.1 in /usr/local/lib/python3.10/dist-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.49.0)\n",
            "Requirement already satisfied: oauth2client>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.1.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: pyu2f in /usr/local/lib/python3.10/dist-packages (from google-reauth>=0.1.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.5)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (43.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.17.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.7.post4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL<=24.2.1,>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.22)\n"
          ]
        }
      ],
      "source": [
        "#When you restart the sessioin please run this cell\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(os.getcwd())\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/MIP/Dassl.pytorch\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YSGWoTSLknJS",
        "outputId": "eb04ca82-956e-4272-b920-8c11eebbfa5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/data\n",
            "mkdir: cannot create directory ‘eurosat’: File exists\n",
            "--2024-12-08 14:03:15--  http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
            "Resolving madm.dfki.de (madm.dfki.de)... 131.246.195.183\n",
            "Connecting to madm.dfki.de (madm.dfki.de)|131.246.195.183|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94280567 (90M) [application/zip]\n",
            "Saving to: ‘EuroSAT.zip.1’\n",
            "\n",
            "EuroSAT.zip.1       100%[===================>]  89.91M  22.4MB/s    in 4.8s    \n",
            "\n",
            "2024-12-08 14:03:21 (18.6 MB/s) - ‘EuroSAT.zip.1’ saved [94280567/94280567]\n",
            "\n",
            "--2024-12-08 14:03:21--  http://eurosat.zip/\n",
            "Resolving eurosat.zip (eurosat.zip)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘eurosat.zip’\n",
            "FINISHED --2024-12-08 14:03:21--\n",
            "Total wall clock time: 5.4s\n",
            "Downloaded: 1 files, 90M in 4.8s (18.6 MB/s)\n",
            "Archive:  EuroSAT.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of EuroSAT.zip or\n",
            "        EuroSAT.zip.zip, and cannot find EuroSAT.zip.ZIP, period.\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/data/eurosat\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
            "To: /content/drive/MyDrive/Colab Notebooks/MIP/data/eurosat/split_zhou_EuroSAT.json\n",
            "100% 3.01M/3.01M [00:00<00:00, 126MB/s]\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/data\n"
          ]
        }
      ],
      "source": [
        "%mkdir output\n",
        "%mkdir data\n",
        "\n",
        "%cd data\n",
        "%mkdir eurosat\n",
        "!wget  http://madm.dfki.de/files/sentinel/EuroSAT.zip EuroSAT.zip\n",
        "\n",
        "!unzip -o EuroSAT.zip -d eurosat/\n",
        "\n",
        "%cd eurosat\n",
        "!gdown 1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
        "\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tVYtLkE6kyrP",
        "outputId": "0cad86e8-edb4-4863-cd2d-b3812f502350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1501r8Ber4nNKvmlFVQZ8SeUHTcdTTEqs\n",
            "To: /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "100% 724k/724k [00:00<00:00, 48.2MB/s]\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/data\n"
          ]
        }
      ],
      "source": [
        "%mkdir oxford_pets\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "import os\n",
        "\n",
        "destination_folder = \"oxford_pets\"\n",
        "images = os.path.join(os.getcwd(),\"images.tar.gz\")\n",
        "annotataions = os.path.join(os.getcwd(),\"annotations.tar.gz\")\n",
        "\n",
        "with tarfile.open(images, \"r:gz\") as tar:\n",
        "        tar.extractall(path=destination_folder)\n",
        "with tarfile.open(annotataions, \"r:gz\") as tar:\n",
        "        tar.extractall(path=destination_folder)\n",
        "\n",
        "%cd oxford_pets\n",
        "\n",
        "!gdown 1501r8Ber4nNKvmlFVQZ8SeUHTcdTTEqs\n",
        "\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GbVhlGM4kyny",
        "outputId": "f0f01e14-9203-45fd-85c7-cbf31dcb8a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘oxford_flowers’: File exists\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_flowers\n",
            "--2024-12-08 14:15:07--  https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/flowers/102/imagelabels.mat [following]\n",
            "--2024-12-08 14:15:07--  https://thor.robots.ox.ac.uk/flowers/102/imagelabels.mat\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 502 [application/octet-stream]\n",
            "Saving to: ‘imagelabels.mat’\n",
            "\n",
            "imagelabels.mat     100%[===================>]     502  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-08 14:15:08 (71.1 MB/s) - ‘imagelabels.mat’ saved [502/502]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AkcxCXeK_RCGCEC_GvmWxjcjaNhu-at0\n",
            "To: /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_flowers/cat_to_name.json\n",
            "100% 2.21k/2.21k [00:00<00:00, 8.96MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Pp0sRXzZFZq15zVOzKjKBu4A9i01nozT\n",
            "To: /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_flowers/split_zhou_OxfordFlowers.json\n",
            "100% 771k/771k [00:00<00:00, 85.5MB/s]\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/data\n"
          ]
        }
      ],
      "source": [
        "%mkdir oxford_flowers\n",
        "#!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "\n",
        "destination_folder = \"oxford_flowers\"\n",
        "images = os.path.join(os.getcwd(),\"102flowers.tgz\")\n",
        "\n",
        "with tarfile.open(images, \"r:gz\") as tar:\n",
        "        tar.extractall(path=destination_folder)\n",
        "\n",
        "%cd oxford_flowers\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
        "!gdown 1AkcxCXeK_RCGCEC_GvmWxjcjaNhu-at0\n",
        "!gdown 1Pp0sRXzZFZq15zVOzKjKBu4A9i01nozT\n",
        "\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSn07by9xL39",
        "outputId": "378ce2e4-9c77-483d-b1c7-1b535bf466fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/tensorboard)\n",
            "epoch [1/10] batch [20/1632] time 0.042 (0.147) data 0.000 (0.030) loss 0.0505 (1.4900) lr 1.0000e-05 eta 0:39:52\n",
            "epoch [1/10] batch [40/1632] time 0.044 (0.094) data 0.000 (0.015) loss 0.2128 (1.7098) lr 1.0000e-05 eta 0:25:35\n",
            "epoch [1/10] batch [60/1632] time 0.040 (0.077) data 0.000 (0.010) loss 0.0027 (1.7344) lr 1.0000e-05 eta 0:20:51\n",
            "epoch [1/10] batch [80/1632] time 0.043 (0.068) data 0.000 (0.008) loss 0.6045 (1.8449) lr 1.0000e-05 eta 0:18:27\n",
            "epoch [1/10] batch [100/1632] time 0.045 (0.063) data 0.000 (0.006) loss 0.5786 (1.6547) lr 1.0000e-05 eta 0:16:59\n",
            "epoch [1/10] batch [120/1632] time 0.045 (0.060) data 0.000 (0.006) loss 0.8457 (1.7856) lr 1.0000e-05 eta 0:16:14\n",
            "epoch [1/10] batch [140/1632] time 0.042 (0.057) data 0.000 (0.005) loss 1.9795 (1.9470) lr 1.0000e-05 eta 0:15:30\n",
            "epoch [1/10] batch [160/1632] time 0.042 (0.056) data 0.000 (0.005) loss 3.2188 (2.0530) lr 1.0000e-05 eta 0:14:58\n",
            "epoch [1/10] batch [180/1632] time 0.041 (0.054) data 0.000 (0.004) loss 0.0732 (2.1692) lr 1.0000e-05 eta 0:14:33\n",
            "epoch [1/10] batch [200/1632] time 0.039 (0.053) data 0.000 (0.004) loss 0.2976 (2.1004) lr 1.0000e-05 eta 0:14:12\n",
            "epoch [1/10] batch [220/1632] time 0.040 (0.052) data 0.000 (0.003) loss 2.9023 (2.1216) lr 1.0000e-05 eta 0:13:52\n",
            "epoch [1/10] batch [240/1632] time 0.042 (0.051) data 0.000 (0.003) loss 0.1021 (2.0894) lr 1.0000e-05 eta 0:13:36\n",
            "epoch [1/10] batch [260/1632] time 0.040 (0.050) data 0.000 (0.003) loss 3.2969 (2.1367) lr 1.0000e-05 eta 0:13:23\n",
            "epoch [1/10] batch [280/1632] time 0.042 (0.049) data 0.000 (0.003) loss 0.6191 (2.1553) lr 1.0000e-05 eta 0:13:12\n",
            "epoch [1/10] batch [300/1632] time 0.041 (0.049) data 0.000 (0.003) loss 6.3984 (2.1686) lr 1.0000e-05 eta 0:13:04\n",
            "epoch [1/10] batch [320/1632] time 0.043 (0.049) data 0.000 (0.002) loss 0.4836 (2.2038) lr 1.0000e-05 eta 0:12:56\n",
            "epoch [1/10] batch [340/1632] time 0.043 (0.048) data 0.000 (0.002) loss 1.1035 (2.2423) lr 1.0000e-05 eta 0:12:49\n",
            "epoch [1/10] batch [360/1632] time 0.040 (0.048) data 0.000 (0.002) loss 7.3750 (2.2367) lr 1.0000e-05 eta 0:12:42\n",
            "epoch [1/10] batch [380/1632] time 0.039 (0.047) data 0.000 (0.002) loss 4.8203 (2.2254) lr 1.0000e-05 eta 0:12:36\n",
            "epoch [1/10] batch [400/1632] time 0.041 (0.047) data 0.000 (0.002) loss 2.3594 (2.2703) lr 1.0000e-05 eta 0:12:30\n",
            "epoch [1/10] batch [420/1632] time 0.042 (0.047) data 0.000 (0.002) loss 0.1831 (2.2609) lr 1.0000e-05 eta 0:12:24\n",
            "epoch [1/10] batch [440/1632] time 0.043 (0.047) data 0.000 (0.002) loss 0.2991 (2.2737) lr 1.0000e-05 eta 0:12:19\n",
            "epoch [1/10] batch [460/1632] time 0.041 (0.046) data 0.000 (0.002) loss 3.3359 (2.2704) lr 1.0000e-05 eta 0:12:14\n",
            "epoch [1/10] batch [480/1632] time 0.042 (0.046) data 0.000 (0.002) loss 0.1179 (2.2408) lr 1.0000e-05 eta 0:12:09\n",
            "epoch [1/10] batch [500/1632] time 0.043 (0.046) data 0.000 (0.002) loss 0.1533 (2.2165) lr 1.0000e-05 eta 0:12:05\n",
            "epoch [1/10] batch [520/1632] time 0.041 (0.046) data 0.000 (0.002) loss 0.6230 (2.2099) lr 1.0000e-05 eta 0:12:02\n",
            "epoch [1/10] batch [540/1632] time 0.041 (0.046) data 0.000 (0.002) loss 0.0284 (2.2305) lr 1.0000e-05 eta 0:11:58\n",
            "epoch [1/10] batch [560/1632] time 0.041 (0.045) data 0.000 (0.002) loss 2.6562 (2.2279) lr 1.0000e-05 eta 0:11:55\n",
            "epoch [1/10] batch [580/1632] time 0.041 (0.045) data 0.000 (0.001) loss 0.0018 (2.1922) lr 1.0000e-05 eta 0:11:52\n",
            "epoch [1/10] batch [600/1632] time 0.040 (0.045) data 0.000 (0.001) loss 0.0477 (2.1944) lr 1.0000e-05 eta 0:11:48\n",
            "epoch [1/10] batch [620/1632] time 0.043 (0.045) data 0.000 (0.001) loss 0.5386 (2.1928) lr 1.0000e-05 eta 0:11:45\n",
            "epoch [1/10] batch [640/1632] time 0.041 (0.045) data 0.000 (0.001) loss 1.3154 (2.1970) lr 1.0000e-05 eta 0:11:43\n",
            "epoch [1/10] batch [660/1632] time 0.041 (0.045) data 0.000 (0.001) loss 0.0120 (2.1697) lr 1.0000e-05 eta 0:11:41\n",
            "epoch [1/10] batch [680/1632] time 0.046 (0.045) data 0.000 (0.001) loss 11.6172 (2.1568) lr 1.0000e-05 eta 0:11:39\n",
            "epoch [1/10] batch [700/1632] time 0.040 (0.045) data 0.000 (0.001) loss 0.1696 (2.1278) lr 1.0000e-05 eta 0:11:37\n",
            "epoch [1/10] batch [720/1632] time 0.049 (0.045) data 0.000 (0.001) loss 0.1405 (2.1239) lr 1.0000e-05 eta 0:11:35\n",
            "epoch [1/10] batch [740/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.0635 (2.1125) lr 1.0000e-05 eta 0:11:33\n",
            "epoch [1/10] batch [760/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.1129 (2.1165) lr 1.0000e-05 eta 0:11:31\n",
            "epoch [1/10] batch [780/1632] time 0.042 (0.044) data 0.000 (0.001) loss 0.8574 (2.1151) lr 1.0000e-05 eta 0:11:29\n",
            "epoch [1/10] batch [800/1632] time 0.040 (0.044) data 0.000 (0.001) loss 5.6211 (2.1115) lr 1.0000e-05 eta 0:11:26\n",
            "epoch [1/10] batch [820/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.0633 (2.1027) lr 1.0000e-05 eta 0:11:24\n",
            "epoch [1/10] batch [840/1632] time 0.041 (0.044) data 0.000 (0.001) loss 1.0576 (2.0903) lr 1.0000e-05 eta 0:11:22\n",
            "epoch [1/10] batch [860/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.0193 (2.0814) lr 1.0000e-05 eta 0:11:20\n",
            "epoch [1/10] batch [880/1632] time 0.040 (0.044) data 0.000 (0.001) loss 1.6797 (2.0788) lr 1.0000e-05 eta 0:11:18\n",
            "epoch [1/10] batch [900/1632] time 0.041 (0.044) data 0.000 (0.001) loss 3.3770 (2.0692) lr 1.0000e-05 eta 0:11:17\n",
            "epoch [1/10] batch [920/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.0095 (2.0374) lr 1.0000e-05 eta 0:11:15\n",
            "epoch [1/10] batch [940/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.7827 (2.0549) lr 1.0000e-05 eta 0:11:13\n",
            "epoch [1/10] batch [960/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.0413 (2.0559) lr 1.0000e-05 eta 0:11:11\n",
            "epoch [1/10] batch [980/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.3479 (2.0390) lr 1.0000e-05 eta 0:11:11\n",
            "epoch [1/10] batch [1000/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.3469 (2.0264) lr 1.0000e-05 eta 0:11:10\n",
            "epoch [1/10] batch [1020/1632] time 0.045 (0.044) data 0.000 (0.001) loss 5.2891 (2.0116) lr 1.0000e-05 eta 0:11:09\n",
            "epoch [1/10] batch [1040/1632] time 0.039 (0.044) data 0.000 (0.001) loss 7.5352 (1.9974) lr 1.0000e-05 eta 0:11:07\n",
            "epoch [1/10] batch [1060/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.0643 (2.0035) lr 1.0000e-05 eta 0:11:06\n",
            "epoch [1/10] batch [1080/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.2668 (2.0250) lr 1.0000e-05 eta 0:11:04\n",
            "epoch [1/10] batch [1100/1632] time 0.041 (0.044) data 0.000 (0.001) loss 3.3477 (2.0216) lr 1.0000e-05 eta 0:11:03\n",
            "epoch [1/10] batch [1120/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.0412 (2.0047) lr 1.0000e-05 eta 0:11:01\n",
            "epoch [1/10] batch [1140/1632] time 0.041 (0.044) data 0.000 (0.001) loss 2.3867 (2.0085) lr 1.0000e-05 eta 0:11:00\n",
            "epoch [1/10] batch [1160/1632] time 0.043 (0.043) data 0.000 (0.001) loss 1.7920 (2.0039) lr 1.0000e-05 eta 0:10:59\n",
            "epoch [1/10] batch [1180/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0778 (1.9922) lr 1.0000e-05 eta 0:10:57\n",
            "epoch [1/10] batch [1200/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.1047 (1.9868) lr 1.0000e-05 eta 0:10:55\n",
            "epoch [1/10] batch [1220/1632] time 0.040 (0.043) data 0.000 (0.001) loss 7.6172 (2.0098) lr 1.0000e-05 eta 0:10:54\n",
            "epoch [1/10] batch [1240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.6807 (2.0095) lr 1.0000e-05 eta 0:10:53\n",
            "epoch [1/10] batch [1260/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.3647 (2.0056) lr 1.0000e-05 eta 0:10:52\n",
            "epoch [1/10] batch [1280/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1616 (1.9934) lr 1.0000e-05 eta 0:10:50\n",
            "epoch [1/10] batch [1300/1632] time 0.048 (0.043) data 0.000 (0.001) loss 9.8281 (1.9971) lr 1.0000e-05 eta 0:10:49\n",
            "epoch [1/10] batch [1320/1632] time 0.043 (0.044) data 0.000 (0.002) loss 0.0122 (1.9961) lr 1.0000e-05 eta 0:10:59\n",
            "epoch [1/10] batch [1340/1632] time 0.040 (0.044) data 0.000 (0.002) loss 4.6406 (1.9918) lr 1.0000e-05 eta 0:11:01\n",
            "epoch [1/10] batch [1360/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.2505 (1.9855) lr 1.0000e-05 eta 0:10:59\n",
            "epoch [1/10] batch [1380/1632] time 0.043 (0.044) data 0.000 (0.002) loss 0.0090 (1.9888) lr 1.0000e-05 eta 0:10:58\n",
            "epoch [1/10] batch [1400/1632] time 0.044 (0.044) data 0.000 (0.002) loss 2.8652 (1.9814) lr 1.0000e-05 eta 0:10:57\n",
            "epoch [1/10] batch [1420/1632] time 0.044 (0.044) data 0.000 (0.002) loss 0.0498 (1.9702) lr 1.0000e-05 eta 0:10:55\n",
            "epoch [1/10] batch [1440/1632] time 0.039 (0.044) data 0.000 (0.002) loss 2.3867 (1.9739) lr 1.0000e-05 eta 0:10:54\n",
            "epoch [1/10] batch [1460/1632] time 0.039 (0.044) data 0.000 (0.002) loss 1.3633 (1.9759) lr 1.0000e-05 eta 0:10:53\n",
            "epoch [1/10] batch [1480/1632] time 0.039 (0.044) data 0.000 (0.002) loss 0.1219 (1.9709) lr 1.0000e-05 eta 0:10:52\n",
            "epoch [1/10] batch [1500/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0454 (1.9762) lr 1.0000e-05 eta 0:10:51\n",
            "epoch [1/10] batch [1520/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.7559 (1.9818) lr 1.0000e-05 eta 0:10:49\n",
            "epoch [1/10] batch [1540/1632] time 0.040 (0.044) data 0.000 (0.002) loss 4.6836 (1.9769) lr 1.0000e-05 eta 0:10:48\n",
            "epoch [1/10] batch [1560/1632] time 0.040 (0.044) data 0.000 (0.002) loss 1.3975 (1.9734) lr 1.0000e-05 eta 0:10:46\n",
            "epoch [1/10] batch [1580/1632] time 0.039 (0.044) data 0.000 (0.002) loss 1.0059 (1.9773) lr 1.0000e-05 eta 0:10:45\n",
            "epoch [1/10] batch [1600/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.1395 (1.9651) lr 1.0000e-05 eta 0:10:44\n",
            "epoch [1/10] batch [1620/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.1148 (1.9580) lr 1.0000e-05 eta 0:10:45\n",
            "epoch [2/10] batch [20/1632] time 0.042 (0.056) data 0.000 (0.013) loss 1.0029 (2.0155) lr 2.0000e-03 eta 0:13:46\n",
            "epoch [2/10] batch [40/1632] time 0.041 (0.049) data 0.000 (0.007) loss 6.7305 (2.4239) lr 2.0000e-03 eta 0:12:00\n",
            "epoch [2/10] batch [60/1632] time 0.042 (0.047) data 0.000 (0.005) loss 3.3340 (2.0971) lr 2.0000e-03 eta 0:11:20\n",
            "epoch [2/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.0372 (2.0052) lr 2.0000e-03 eta 0:11:00\n",
            "epoch [2/10] batch [100/1632] time 0.042 (0.044) data 0.000 (0.003) loss 0.4570 (2.3273) lr 2.0000e-03 eta 0:10:48\n",
            "epoch [2/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0479 (2.3367) lr 2.0000e-03 eta 0:10:38\n",
            "epoch [2/10] batch [140/1632] time 0.042 (0.043) data 0.000 (0.002) loss 5.6914 (2.1660) lr 2.0000e-03 eta 0:10:32\n",
            "epoch [2/10] batch [160/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.4209 (2.2013) lr 2.0000e-03 eta 0:10:27\n",
            "epoch [2/10] batch [180/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.0320 (2.1618) lr 2.0000e-03 eta 0:10:23\n",
            "epoch [2/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.2852 (2.1162) lr 2.0000e-03 eta 0:10:19\n",
            "epoch [2/10] batch [220/1632] time 0.043 (0.043) data 0.000 (0.001) loss 1.7617 (2.0878) lr 2.0000e-03 eta 0:10:16\n",
            "epoch [2/10] batch [240/1632] time 0.040 (0.043) data 0.000 (0.001) loss 1.4395 (2.0693) lr 2.0000e-03 eta 0:10:14\n",
            "epoch [2/10] batch [260/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.0000 (2.0068) lr 2.0000e-03 eta 0:10:11\n",
            "epoch [2/10] batch [280/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.2061 (1.9246) lr 2.0000e-03 eta 0:10:09\n",
            "epoch [2/10] batch [300/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.6533 (1.9260) lr 2.0000e-03 eta 0:10:07\n",
            "epoch [2/10] batch [320/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4373 (1.9029) lr 2.0000e-03 eta 0:10:06\n",
            "epoch [2/10] batch [340/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0506 (1.8545) lr 2.0000e-03 eta 0:10:06\n",
            "epoch [2/10] batch [360/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0138 (1.8367) lr 2.0000e-03 eta 0:10:05\n",
            "epoch [2/10] batch [380/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.2617 (1.8061) lr 2.0000e-03 eta 0:10:04\n",
            "epoch [2/10] batch [400/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.2705 (1.7805) lr 2.0000e-03 eta 0:10:03\n",
            "epoch [2/10] batch [420/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.3770 (1.7693) lr 2.0000e-03 eta 0:10:02\n",
            "epoch [2/10] batch [440/1632] time 0.042 (0.042) data 0.000 (0.001) loss 7.6094 (1.7831) lr 2.0000e-03 eta 0:10:01\n",
            "epoch [2/10] batch [460/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1335 (1.7737) lr 2.0000e-03 eta 0:10:00\n",
            "epoch [2/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8140 (1.7672) lr 2.0000e-03 eta 0:09:58\n",
            "epoch [2/10] batch [500/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.1523 (1.7659) lr 2.0000e-03 eta 0:09:57\n",
            "epoch [2/10] batch [520/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.0273 (1.7386) lr 2.0000e-03 eta 0:09:56\n",
            "epoch [2/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3811 (1.7385) lr 2.0000e-03 eta 0:09:55\n",
            "epoch [2/10] batch [560/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0321 (1.7219) lr 2.0000e-03 eta 0:09:54\n",
            "epoch [2/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0422 (1.7042) lr 2.0000e-03 eta 0:09:53\n",
            "epoch [2/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2094 (1.6752) lr 2.0000e-03 eta 0:09:51\n",
            "epoch [2/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0214 (1.6813) lr 2.0000e-03 eta 0:09:50\n",
            "epoch [2/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.2812 (1.6759) lr 2.0000e-03 eta 0:09:49\n",
            "epoch [2/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0234 (1.6565) lr 2.0000e-03 eta 0:09:48\n",
            "epoch [2/10] batch [680/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0015 (1.6432) lr 2.0000e-03 eta 0:09:48\n",
            "epoch [2/10] batch [700/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.7109 (1.6596) lr 2.0000e-03 eta 0:09:47\n",
            "epoch [2/10] batch [720/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.6309 (1.6490) lr 2.0000e-03 eta 0:09:46\n",
            "epoch [2/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.7461 (1.6487) lr 2.0000e-03 eta 0:09:45\n",
            "epoch [2/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0944 (1.6393) lr 2.0000e-03 eta 0:09:45\n",
            "epoch [2/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.1641 (1.6555) lr 2.0000e-03 eta 0:09:44\n",
            "epoch [2/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4136 (1.6321) lr 2.0000e-03 eta 0:09:43\n",
            "epoch [2/10] batch [820/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.7168 (1.6187) lr 2.0000e-03 eta 0:09:42\n",
            "epoch [2/10] batch [840/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.7061 (1.6151) lr 2.0000e-03 eta 0:09:41\n",
            "epoch [2/10] batch [860/1632] time 0.042 (0.042) data 0.000 (0.001) loss 4.4922 (1.6220) lr 2.0000e-03 eta 0:09:40\n",
            "epoch [2/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7778 (1.6198) lr 2.0000e-03 eta 0:09:39\n",
            "epoch [2/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.6924 (1.5957) lr 2.0000e-03 eta 0:09:38\n",
            "epoch [2/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0046 (1.5913) lr 2.0000e-03 eta 0:09:37\n",
            "epoch [2/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.9009 (1.5830) lr 2.0000e-03 eta 0:09:36\n",
            "epoch [2/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3518 (1.5798) lr 2.0000e-03 eta 0:09:35\n",
            "epoch [2/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3337 (1.5818) lr 2.0000e-03 eta 0:09:34\n",
            "epoch [2/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3213 (1.5802) lr 2.0000e-03 eta 0:09:33\n",
            "epoch [2/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0018 (1.5711) lr 2.0000e-03 eta 0:09:33\n",
            "epoch [2/10] batch [1040/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.4756 (1.5536) lr 2.0000e-03 eta 0:09:32\n",
            "epoch [2/10] batch [1060/1632] time 0.047 (0.042) data 0.000 (0.001) loss 8.7500 (1.5539) lr 2.0000e-03 eta 0:09:32\n",
            "epoch [2/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5190 (1.5421) lr 2.0000e-03 eta 0:09:31\n",
            "epoch [2/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.6172 (1.5391) lr 2.0000e-03 eta 0:09:30\n",
            "epoch [2/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.1934 (1.5408) lr 2.0000e-03 eta 0:09:29\n",
            "epoch [2/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.3457 (1.5365) lr 2.0000e-03 eta 0:09:28\n",
            "epoch [2/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4744 (1.5316) lr 2.0000e-03 eta 0:09:27\n",
            "epoch [2/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.7393 (1.5216) lr 2.0000e-03 eta 0:09:26\n",
            "epoch [2/10] batch [1200/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.0410 (1.5154) lr 2.0000e-03 eta 0:09:25\n",
            "epoch [2/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1311 (1.5156) lr 2.0000e-03 eta 0:09:24\n",
            "epoch [2/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5488 (1.5221) lr 2.0000e-03 eta 0:09:24\n",
            "epoch [2/10] batch [1260/1632] time 0.042 (0.042) data 0.000 (0.000) loss 7.1406 (1.5185) lr 2.0000e-03 eta 0:09:23\n",
            "epoch [2/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 3.9805 (1.5174) lr 2.0000e-03 eta 0:09:22\n",
            "epoch [2/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5571 (1.5153) lr 2.0000e-03 eta 0:09:21\n",
            "epoch [2/10] batch [1320/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.7793 (1.5172) lr 2.0000e-03 eta 0:09:20\n",
            "epoch [2/10] batch [1340/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0159 (1.5139) lr 2.0000e-03 eta 0:09:19\n",
            "epoch [2/10] batch [1360/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.4902 (1.5069) lr 2.0000e-03 eta 0:09:19\n",
            "epoch [2/10] batch [1380/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.4431 (1.5030) lr 2.0000e-03 eta 0:09:18\n",
            "epoch [2/10] batch [1400/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.2988 (1.5117) lr 2.0000e-03 eta 0:09:17\n",
            "epoch [2/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 6.7188 (1.5132) lr 2.0000e-03 eta 0:09:16\n",
            "epoch [2/10] batch [1440/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1879 (1.5137) lr 2.0000e-03 eta 0:09:15\n",
            "epoch [2/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1327 (1.5097) lr 2.0000e-03 eta 0:09:14\n",
            "epoch [2/10] batch [1480/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3088 (1.5049) lr 2.0000e-03 eta 0:09:13\n",
            "epoch [2/10] batch [1500/1632] time 0.042 (0.042) data 0.000 (0.000) loss 4.1328 (1.5017) lr 2.0000e-03 eta 0:09:12\n",
            "epoch [2/10] batch [1520/1632] time 0.042 (0.042) data 0.000 (0.000) loss 7.7930 (1.5092) lr 2.0000e-03 eta 0:09:12\n",
            "epoch [2/10] batch [1540/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.9126 (1.5061) lr 2.0000e-03 eta 0:09:11\n",
            "epoch [2/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0435 (1.4975) lr 2.0000e-03 eta 0:09:10\n",
            "epoch [2/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.7900 (1.4909) lr 2.0000e-03 eta 0:09:09\n",
            "epoch [2/10] batch [1600/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0691 (1.4895) lr 2.0000e-03 eta 0:09:08\n",
            "epoch [2/10] batch [1620/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2820 (1.4965) lr 2.0000e-03 eta 0:09:07\n",
            "epoch [3/10] batch [20/1632] time 0.044 (0.056) data 0.000 (0.012) loss 3.7246 (1.1507) lr 1.9511e-03 eta 0:12:16\n",
            "epoch [3/10] batch [40/1632] time 0.044 (0.049) data 0.000 (0.006) loss 0.7520 (1.1600) lr 1.9511e-03 eta 0:10:42\n",
            "epoch [3/10] batch [60/1632] time 0.045 (0.047) data 0.000 (0.004) loss 0.0314 (1.0398) lr 1.9511e-03 eta 0:10:13\n",
            "epoch [3/10] batch [80/1632] time 0.042 (0.046) data 0.000 (0.003) loss 0.7529 (1.0836) lr 1.9511e-03 eta 0:09:57\n",
            "epoch [3/10] batch [100/1632] time 0.046 (0.045) data 0.000 (0.003) loss 0.1169 (1.1531) lr 1.9511e-03 eta 0:09:48\n",
            "epoch [3/10] batch [120/1632] time 0.042 (0.045) data 0.000 (0.002) loss 0.6162 (1.1152) lr 1.9511e-03 eta 0:09:40\n",
            "epoch [3/10] batch [140/1632] time 0.045 (0.044) data 0.000 (0.002) loss 0.1320 (1.1432) lr 1.9511e-03 eta 0:09:33\n",
            "epoch [3/10] batch [160/1632] time 0.041 (0.044) data 0.000 (0.002) loss 1.2783 (1.1253) lr 1.9511e-03 eta 0:09:28\n",
            "epoch [3/10] batch [180/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0320 (1.1200) lr 1.9511e-03 eta 0:09:24\n",
            "epoch [3/10] batch [200/1632] time 0.041 (0.044) data 0.000 (0.001) loss 3.1328 (1.0739) lr 1.9511e-03 eta 0:09:20\n",
            "epoch [3/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.9521 (1.0594) lr 1.9511e-03 eta 0:09:16\n",
            "epoch [3/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0818 (1.0635) lr 1.9511e-03 eta 0:09:13\n",
            "epoch [3/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1659 (1.0111) lr 1.9511e-03 eta 0:09:11\n",
            "epoch [3/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0347 (1.0430) lr 1.9511e-03 eta 0:09:09\n",
            "epoch [3/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 6.9141 (1.0886) lr 1.9511e-03 eta 0:09:07\n",
            "epoch [3/10] batch [320/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.1582 (1.0605) lr 1.9511e-03 eta 0:09:05\n",
            "epoch [3/10] batch [340/1632] time 0.042 (0.043) data 0.000 (0.001) loss 2.4297 (1.0778) lr 1.9511e-03 eta 0:09:03\n",
            "epoch [3/10] batch [360/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.2041 (1.0928) lr 1.9511e-03 eta 0:09:02\n",
            "epoch [3/10] batch [380/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1680 (1.0664) lr 1.9511e-03 eta 0:09:01\n",
            "epoch [3/10] batch [400/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6421 (1.0975) lr 1.9511e-03 eta 0:09:00\n",
            "epoch [3/10] batch [420/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.4221 (1.1037) lr 1.9511e-03 eta 0:09:00\n",
            "epoch [3/10] batch [440/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2778 (1.1030) lr 1.9511e-03 eta 0:08:59\n",
            "epoch [3/10] batch [460/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0905 (1.1212) lr 1.9511e-03 eta 0:08:57\n",
            "epoch [3/10] batch [480/1632] time 0.045 (0.043) data 0.000 (0.001) loss 2.2637 (1.1256) lr 1.9511e-03 eta 0:08:56\n",
            "epoch [3/10] batch [500/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.1641 (1.1426) lr 1.9511e-03 eta 0:08:55\n",
            "epoch [3/10] batch [520/1632] time 0.045 (0.043) data 0.000 (0.001) loss 6.0625 (1.1598) lr 1.9511e-03 eta 0:08:53\n",
            "epoch [3/10] batch [540/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.4570 (1.1425) lr 1.9511e-03 eta 0:08:52\n",
            "epoch [3/10] batch [560/1632] time 0.045 (0.043) data 0.000 (0.001) loss 1.3018 (1.1326) lr 1.9511e-03 eta 0:08:51\n",
            "epoch [3/10] batch [580/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0291 (1.1156) lr 1.9511e-03 eta 0:08:51\n",
            "epoch [3/10] batch [600/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1447 (1.1098) lr 1.9511e-03 eta 0:08:50\n",
            "epoch [3/10] batch [620/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.4729 (1.1091) lr 1.9511e-03 eta 0:08:49\n",
            "epoch [3/10] batch [640/1632] time 0.046 (0.043) data 0.000 (0.001) loss 0.4951 (1.1201) lr 1.9511e-03 eta 0:08:48\n",
            "epoch [3/10] batch [660/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.9766 (1.1328) lr 1.9511e-03 eta 0:08:47\n",
            "epoch [3/10] batch [680/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5537 (1.1293) lr 1.9511e-03 eta 0:08:45\n",
            "epoch [3/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.4766 (1.1230) lr 1.9511e-03 eta 0:08:44\n",
            "epoch [3/10] batch [720/1632] time 0.044 (0.042) data 0.000 (0.001) loss 7.8438 (1.1369) lr 1.9511e-03 eta 0:08:43\n",
            "epoch [3/10] batch [740/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.5864 (1.1358) lr 1.9511e-03 eta 0:08:43\n",
            "epoch [3/10] batch [760/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.9648 (1.1509) lr 1.9511e-03 eta 0:08:42\n",
            "epoch [3/10] batch [780/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.4055 (1.1441) lr 1.9511e-03 eta 0:08:42\n",
            "epoch [3/10] batch [800/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.2629 (1.1633) lr 1.9511e-03 eta 0:08:41\n",
            "epoch [3/10] batch [820/1632] time 0.041 (0.043) data 0.000 (0.001) loss 3.3965 (1.1761) lr 1.9511e-03 eta 0:08:40\n",
            "epoch [3/10] batch [840/1632] time 0.045 (0.043) data 0.000 (0.001) loss 2.7324 (1.1760) lr 1.9511e-03 eta 0:08:39\n",
            "epoch [3/10] batch [860/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1953 (1.1828) lr 1.9511e-03 eta 0:08:38\n",
            "epoch [3/10] batch [880/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2520 (1.1789) lr 1.9511e-03 eta 0:08:37\n",
            "epoch [3/10] batch [900/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.7944 (1.1732) lr 1.9511e-03 eta 0:08:36\n",
            "epoch [3/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0401 (1.1813) lr 1.9511e-03 eta 0:08:35\n",
            "epoch [3/10] batch [940/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0064 (1.1851) lr 1.9511e-03 eta 0:08:34\n",
            "epoch [3/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.6489 (1.1844) lr 1.9511e-03 eta 0:08:33\n",
            "epoch [3/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1426 (1.1824) lr 1.9511e-03 eta 0:08:32\n",
            "epoch [3/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3311 (1.1842) lr 1.9511e-03 eta 0:08:31\n",
            "epoch [3/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.8848 (1.1770) lr 1.9511e-03 eta 0:08:31\n",
            "epoch [3/10] batch [1040/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0178 (1.1774) lr 1.9511e-03 eta 0:08:30\n",
            "epoch [3/10] batch [1060/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0049 (1.1703) lr 1.9511e-03 eta 0:08:29\n",
            "epoch [3/10] batch [1080/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.3779 (1.1688) lr 1.9511e-03 eta 0:08:28\n",
            "epoch [3/10] batch [1100/1632] time 0.043 (0.043) data 0.000 (0.000) loss 0.0556 (1.1633) lr 1.9511e-03 eta 0:08:28\n",
            "epoch [3/10] batch [1120/1632] time 0.044 (0.043) data 0.000 (0.000) loss 0.9028 (1.1585) lr 1.9511e-03 eta 0:08:27\n",
            "epoch [3/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2793 (1.1491) lr 1.9511e-03 eta 0:08:26\n",
            "epoch [3/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2830 (1.1448) lr 1.9511e-03 eta 0:08:24\n",
            "epoch [3/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.9678 (1.1479) lr 1.9511e-03 eta 0:08:23\n",
            "epoch [3/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0335 (1.1435) lr 1.9511e-03 eta 0:08:22\n",
            "epoch [3/10] batch [1220/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.3018 (1.1377) lr 1.9511e-03 eta 0:08:21\n",
            "epoch [3/10] batch [1240/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4932 (1.1338) lr 1.9511e-03 eta 0:08:20\n",
            "epoch [3/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2416 (1.1389) lr 1.9511e-03 eta 0:08:19\n",
            "epoch [3/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1151 (1.1325) lr 1.9511e-03 eta 0:08:18\n",
            "epoch [3/10] batch [1300/1632] time 0.043 (0.042) data 0.000 (0.000) loss 3.4219 (1.1358) lr 1.9511e-03 eta 0:08:17\n",
            "epoch [3/10] batch [1320/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.1102 (1.1356) lr 1.9511e-03 eta 0:08:16\n",
            "epoch [3/10] batch [1340/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0906 (1.1286) lr 1.9511e-03 eta 0:08:15\n",
            "epoch [3/10] batch [1360/1632] time 0.043 (0.042) data 0.000 (0.000) loss 3.1016 (1.1295) lr 1.9511e-03 eta 0:08:15\n",
            "epoch [3/10] batch [1380/1632] time 0.043 (0.042) data 0.000 (0.000) loss 2.1211 (1.1454) lr 1.9511e-03 eta 0:08:14\n",
            "epoch [3/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0146 (1.1514) lr 1.9511e-03 eta 0:08:13\n",
            "epoch [3/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1698 (1.1464) lr 1.9511e-03 eta 0:08:12\n",
            "epoch [3/10] batch [1440/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.4854 (1.1488) lr 1.9511e-03 eta 0:08:11\n",
            "epoch [3/10] batch [1460/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.9033 (1.1451) lr 1.9511e-03 eta 0:08:10\n",
            "epoch [3/10] batch [1480/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.8462 (1.1435) lr 1.9511e-03 eta 0:08:09\n",
            "epoch [3/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4683 (1.1361) lr 1.9511e-03 eta 0:08:08\n",
            "epoch [3/10] batch [1520/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0811 (1.1345) lr 1.9511e-03 eta 0:08:07\n",
            "epoch [3/10] batch [1540/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0262 (1.1316) lr 1.9511e-03 eta 0:08:06\n",
            "epoch [3/10] batch [1560/1632] time 0.042 (0.042) data 0.000 (0.000) loss 3.7910 (1.1319) lr 1.9511e-03 eta 0:08:06\n",
            "epoch [3/10] batch [1580/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0142 (1.1318) lr 1.9511e-03 eta 0:08:05\n",
            "epoch [3/10] batch [1600/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0285 (1.1310) lr 1.9511e-03 eta 0:08:04\n",
            "epoch [3/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 2.9258 (1.1247) lr 1.9511e-03 eta 0:08:03\n",
            "epoch [4/10] batch [20/1632] time 0.041 (0.054) data 0.000 (0.012) loss 3.0156 (1.4094) lr 1.8090e-03 eta 0:10:20\n",
            "epoch [4/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.006) loss 0.7754 (1.2141) lr 1.8090e-03 eta 0:09:03\n",
            "epoch [4/10] batch [60/1632] time 0.043 (0.046) data 0.001 (0.004) loss 0.2379 (1.1826) lr 1.8090e-03 eta 0:08:40\n",
            "epoch [4/10] batch [80/1632] time 0.043 (0.045) data 0.000 (0.003) loss 0.2888 (1.0514) lr 1.8090e-03 eta 0:08:30\n",
            "epoch [4/10] batch [100/1632] time 0.042 (0.044) data 0.000 (0.003) loss 0.2871 (0.9852) lr 1.8090e-03 eta 0:08:23\n",
            "epoch [4/10] batch [120/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.2949 (0.9031) lr 1.8090e-03 eta 0:08:20\n",
            "epoch [4/10] batch [140/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.1737 (0.9189) lr 1.8090e-03 eta 0:08:17\n",
            "epoch [4/10] batch [160/1632] time 0.041 (0.044) data 0.000 (0.002) loss 3.7480 (0.8686) lr 1.8090e-03 eta 0:08:13\n",
            "epoch [4/10] batch [180/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.0225 (0.8892) lr 1.8090e-03 eta 0:08:09\n",
            "epoch [4/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2008 (0.9309) lr 1.8090e-03 eta 0:08:06\n",
            "epoch [4/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 3.2695 (0.9756) lr 1.8090e-03 eta 0:08:03\n",
            "epoch [4/10] batch [240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 2.0000 (0.9671) lr 1.8090e-03 eta 0:08:01\n",
            "epoch [4/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1980 (0.9356) lr 1.8090e-03 eta 0:07:58\n",
            "epoch [4/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0434 (0.9361) lr 1.8090e-03 eta 0:07:56\n",
            "epoch [4/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.4229 (0.9567) lr 1.8090e-03 eta 0:07:54\n",
            "epoch [4/10] batch [320/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0007 (0.9588) lr 1.8090e-03 eta 0:07:52\n",
            "epoch [4/10] batch [340/1632] time 0.042 (0.042) data 0.000 (0.001) loss 3.9414 (1.0074) lr 1.8090e-03 eta 0:07:51\n",
            "epoch [4/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0129 (1.0259) lr 1.8090e-03 eta 0:07:49\n",
            "epoch [4/10] batch [380/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.2615 (1.0062) lr 1.8090e-03 eta 0:07:47\n",
            "epoch [4/10] batch [400/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2524 (0.9944) lr 1.8090e-03 eta 0:07:47\n",
            "epoch [4/10] batch [420/1632] time 0.044 (0.042) data 0.000 (0.001) loss 3.3301 (0.9922) lr 1.8090e-03 eta 0:07:46\n",
            "epoch [4/10] batch [440/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.3752 (0.9694) lr 1.8090e-03 eta 0:07:46\n",
            "epoch [4/10] batch [460/1632] time 0.046 (0.043) data 0.000 (0.001) loss 1.3760 (0.9773) lr 1.8090e-03 eta 0:07:46\n",
            "epoch [4/10] batch [480/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6655 (0.9589) lr 1.8090e-03 eta 0:07:45\n",
            "epoch [4/10] batch [500/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.5254 (0.9633) lr 1.8090e-03 eta 0:07:44\n",
            "epoch [4/10] batch [520/1632] time 0.043 (0.042) data 0.000 (0.001) loss 2.4102 (0.9558) lr 1.8090e-03 eta 0:07:43\n",
            "epoch [4/10] batch [540/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.0015 (0.9612) lr 1.8090e-03 eta 0:07:42\n",
            "epoch [4/10] batch [560/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0140 (0.9762) lr 1.8090e-03 eta 0:07:42\n",
            "epoch [4/10] batch [580/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0381 (0.9615) lr 1.8090e-03 eta 0:07:41\n",
            "epoch [4/10] batch [600/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.0418 (0.9812) lr 1.8090e-03 eta 0:07:40\n",
            "epoch [4/10] batch [620/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.3745 (0.9893) lr 1.8090e-03 eta 0:07:39\n",
            "epoch [4/10] batch [640/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.6587 (0.9711) lr 1.8090e-03 eta 0:07:38\n",
            "epoch [4/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.9976 (0.9611) lr 1.8090e-03 eta 0:07:37\n",
            "epoch [4/10] batch [680/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.6392 (0.9623) lr 1.8090e-03 eta 0:07:36\n",
            "epoch [4/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.0371 (0.9790) lr 1.8090e-03 eta 0:07:34\n",
            "epoch [4/10] batch [720/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0121 (0.9891) lr 1.8090e-03 eta 0:07:34\n",
            "epoch [4/10] batch [740/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.1289 (0.9938) lr 1.8090e-03 eta 0:07:33\n",
            "epoch [4/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0731 (0.9845) lr 1.8090e-03 eta 0:07:32\n",
            "epoch [4/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8027 (0.9808) lr 1.8090e-03 eta 0:07:31\n",
            "epoch [4/10] batch [800/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.5312 (0.9970) lr 1.8090e-03 eta 0:07:30\n",
            "epoch [4/10] batch [820/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1685 (0.9943) lr 1.8090e-03 eta 0:07:29\n",
            "epoch [4/10] batch [840/1632] time 0.045 (0.042) data 0.000 (0.001) loss 4.2031 (1.0220) lr 1.8090e-03 eta 0:07:28\n",
            "epoch [4/10] batch [860/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.4875 (1.0203) lr 1.8090e-03 eta 0:07:27\n",
            "epoch [4/10] batch [880/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.3179 (1.0228) lr 1.8090e-03 eta 0:07:26\n",
            "epoch [4/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.4590 (1.0222) lr 1.8090e-03 eta 0:07:25\n",
            "epoch [4/10] batch [920/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.8521 (1.0233) lr 1.8090e-03 eta 0:07:24\n",
            "epoch [4/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3176 (1.0226) lr 1.8090e-03 eta 0:07:23\n",
            "epoch [4/10] batch [960/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.1126 (1.0122) lr 1.8090e-03 eta 0:07:22\n",
            "epoch [4/10] batch [980/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.6348 (1.0153) lr 1.8090e-03 eta 0:07:21\n",
            "epoch [4/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.8389 (1.0081) lr 1.8090e-03 eta 0:07:20\n",
            "epoch [4/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 3.5801 (1.0200) lr 1.8090e-03 eta 0:07:20\n",
            "epoch [4/10] batch [1040/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.2067 (1.0111) lr 1.8090e-03 eta 0:07:19\n",
            "epoch [4/10] batch [1060/1632] time 0.043 (0.042) data 0.000 (0.000) loss 9.1250 (1.0372) lr 1.8090e-03 eta 0:07:18\n",
            "epoch [4/10] batch [1080/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.8501 (1.0253) lr 1.8090e-03 eta 0:07:17\n",
            "epoch [4/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 5.0312 (1.0338) lr 1.8090e-03 eta 0:07:16\n",
            "epoch [4/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0414 (1.0342) lr 1.8090e-03 eta 0:07:15\n",
            "epoch [4/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0088 (1.0280) lr 1.8090e-03 eta 0:07:14\n",
            "epoch [4/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 12.4219 (1.0488) lr 1.8090e-03 eta 0:07:13\n",
            "epoch [4/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 4.7969 (1.0468) lr 1.8090e-03 eta 0:07:12\n",
            "epoch [4/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8701 (1.0388) lr 1.8090e-03 eta 0:07:11\n",
            "epoch [4/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 4.6016 (1.0455) lr 1.8090e-03 eta 0:07:10\n",
            "epoch [4/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1135 (1.0447) lr 1.8090e-03 eta 0:07:09\n",
            "epoch [4/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.2891 (1.0394) lr 1.8090e-03 eta 0:07:08\n",
            "epoch [4/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5142 (1.0436) lr 1.8090e-03 eta 0:07:07\n",
            "epoch [4/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9341 (1.0381) lr 1.8090e-03 eta 0:07:06\n",
            "epoch [4/10] batch [1320/1632] time 0.041 (0.042) data 0.000 (0.000) loss 4.3516 (1.0520) lr 1.8090e-03 eta 0:07:05\n",
            "epoch [4/10] batch [1340/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0193 (1.0570) lr 1.8090e-03 eta 0:07:04\n",
            "epoch [4/10] batch [1360/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.6841 (1.0538) lr 1.8090e-03 eta 0:07:03\n",
            "epoch [4/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 7.1992 (1.0514) lr 1.8090e-03 eta 0:07:02\n",
            "epoch [4/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9238 (1.0583) lr 1.8090e-03 eta 0:07:01\n",
            "epoch [4/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1559 (1.0533) lr 1.8090e-03 eta 0:07:00\n",
            "epoch [4/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3213 (1.0460) lr 1.8090e-03 eta 0:06:59\n",
            "epoch [4/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0025 (1.0423) lr 1.8090e-03 eta 0:06:59\n",
            "epoch [4/10] batch [1480/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1279 (1.0381) lr 1.8090e-03 eta 0:06:58\n",
            "epoch [4/10] batch [1500/1632] time 0.040 (0.042) data 0.000 (0.000) loss 4.5469 (1.0414) lr 1.8090e-03 eta 0:06:57\n",
            "epoch [4/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0074 (1.0374) lr 1.8090e-03 eta 0:06:56\n",
            "epoch [4/10] batch [1540/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0597 (1.0310) lr 1.8090e-03 eta 0:06:55\n",
            "epoch [4/10] batch [1560/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2334 (1.0258) lr 1.8090e-03 eta 0:06:54\n",
            "epoch [4/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.4277 (1.0271) lr 1.8090e-03 eta 0:06:53\n",
            "epoch [4/10] batch [1600/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0106 (1.0244) lr 1.8090e-03 eta 0:06:52\n",
            "epoch [4/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 1.2617 (1.0182) lr 1.8090e-03 eta 0:06:51\n",
            "epoch [5/10] batch [20/1632] time 0.045 (0.056) data 0.000 (0.013) loss 4.0938 (1.5649) lr 1.5878e-03 eta 0:09:05\n",
            "epoch [5/10] batch [40/1632] time 0.043 (0.049) data 0.000 (0.006) loss 0.0482 (1.1284) lr 1.5878e-03 eta 0:07:53\n",
            "epoch [5/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.1260 (0.9378) lr 1.5878e-03 eta 0:07:30\n",
            "epoch [5/10] batch [80/1632] time 0.042 (0.045) data 0.000 (0.003) loss 1.6162 (0.9745) lr 1.5878e-03 eta 0:07:18\n",
            "epoch [5/10] batch [100/1632] time 0.043 (0.045) data 0.000 (0.003) loss 1.0713 (0.9132) lr 1.5878e-03 eta 0:07:11\n",
            "epoch [5/10] batch [120/1632] time 0.043 (0.044) data 0.000 (0.002) loss 0.8477 (0.9393) lr 1.5878e-03 eta 0:07:07\n",
            "epoch [5/10] batch [140/1632] time 0.042 (0.044) data 0.000 (0.002) loss 1.1768 (0.9290) lr 1.5878e-03 eta 0:07:04\n",
            "epoch [5/10] batch [160/1632] time 0.043 (0.044) data 0.000 (0.002) loss 2.9180 (0.9304) lr 1.5878e-03 eta 0:07:01\n",
            "epoch [5/10] batch [180/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.1147 (0.8946) lr 1.5878e-03 eta 0:06:59\n",
            "epoch [5/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.5259 (0.9362) lr 1.5878e-03 eta 0:06:56\n",
            "epoch [5/10] batch [220/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.6177 (0.8938) lr 1.5878e-03 eta 0:06:55\n",
            "epoch [5/10] batch [240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 3.0996 (0.9116) lr 1.5878e-03 eta 0:06:53\n",
            "epoch [5/10] batch [260/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.0186 (0.9555) lr 1.5878e-03 eta 0:06:51\n",
            "epoch [5/10] batch [280/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2896 (0.9374) lr 1.5878e-03 eta 0:06:49\n",
            "epoch [5/10] batch [300/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2213 (0.9250) lr 1.5878e-03 eta 0:06:47\n",
            "epoch [5/10] batch [320/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1583 (0.9256) lr 1.5878e-03 eta 0:06:46\n",
            "epoch [5/10] batch [340/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0094 (0.9300) lr 1.5878e-03 eta 0:06:45\n",
            "epoch [5/10] batch [360/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0011 (0.9736) lr 1.5878e-03 eta 0:06:43\n",
            "epoch [5/10] batch [380/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0137 (0.9603) lr 1.5878e-03 eta 0:06:42\n",
            "epoch [5/10] batch [400/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.5645 (0.9588) lr 1.5878e-03 eta 0:06:41\n",
            "epoch [5/10] batch [420/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0207 (0.9810) lr 1.5878e-03 eta 0:06:39\n",
            "epoch [5/10] batch [440/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6904 (1.0008) lr 1.5878e-03 eta 0:06:38\n",
            "epoch [5/10] batch [460/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0011 (0.9878) lr 1.5878e-03 eta 0:06:37\n",
            "epoch [5/10] batch [480/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0181 (1.0062) lr 1.5878e-03 eta 0:06:36\n",
            "epoch [5/10] batch [500/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.0322 (1.0096) lr 1.5878e-03 eta 0:06:35\n",
            "epoch [5/10] batch [520/1632] time 0.042 (0.043) data 0.000 (0.001) loss 2.4297 (1.0012) lr 1.5878e-03 eta 0:06:34\n",
            "epoch [5/10] batch [540/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.1582 (0.9966) lr 1.5878e-03 eta 0:06:33\n",
            "epoch [5/10] batch [560/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.6436 (0.9824) lr 1.5878e-03 eta 0:06:32\n",
            "epoch [5/10] batch [580/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.2598 (0.9683) lr 1.5878e-03 eta 0:06:31\n",
            "epoch [5/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3782 (0.9730) lr 1.5878e-03 eta 0:06:30\n",
            "epoch [5/10] batch [620/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.6787 (0.9714) lr 1.5878e-03 eta 0:06:29\n",
            "epoch [5/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.6670 (0.9708) lr 1.5878e-03 eta 0:06:27\n",
            "epoch [5/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.1924 (0.9663) lr 1.5878e-03 eta 0:06:26\n",
            "epoch [5/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3357 (0.9509) lr 1.5878e-03 eta 0:06:25\n",
            "epoch [5/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2847 (0.9475) lr 1.5878e-03 eta 0:06:24\n",
            "epoch [5/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0520 (0.9554) lr 1.5878e-03 eta 0:06:22\n",
            "epoch [5/10] batch [740/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.8770 (0.9603) lr 1.5878e-03 eta 0:06:21\n",
            "epoch [5/10] batch [760/1632] time 0.044 (0.042) data 0.000 (0.001) loss 1.1729 (0.9625) lr 1.5878e-03 eta 0:06:20\n",
            "epoch [5/10] batch [780/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3464 (0.9661) lr 1.5878e-03 eta 0:06:19\n",
            "epoch [5/10] batch [800/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5215 (0.9831) lr 1.5878e-03 eta 0:06:19\n",
            "epoch [5/10] batch [820/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3342 (0.9809) lr 1.5878e-03 eta 0:06:18\n",
            "epoch [5/10] batch [840/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5669 (0.9961) lr 1.5878e-03 eta 0:06:17\n",
            "epoch [5/10] batch [860/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.7568 (1.0006) lr 1.5878e-03 eta 0:06:16\n",
            "epoch [5/10] batch [880/1632] time 0.042 (0.042) data 0.000 (0.001) loss 5.2617 (0.9980) lr 1.5878e-03 eta 0:06:15\n",
            "epoch [5/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6611 (0.9918) lr 1.5878e-03 eta 0:06:14\n",
            "epoch [5/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.8516 (0.9930) lr 1.5878e-03 eta 0:06:13\n",
            "epoch [5/10] batch [940/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.4062 (0.9894) lr 1.5878e-03 eta 0:06:12\n",
            "epoch [5/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.2324 (0.9898) lr 1.5878e-03 eta 0:06:12\n",
            "epoch [5/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1798 (0.9869) lr 1.5878e-03 eta 0:06:11\n",
            "epoch [5/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1641 (0.9748) lr 1.5878e-03 eta 0:06:10\n",
            "epoch [5/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0418 (0.9710) lr 1.5878e-03 eta 0:06:09\n",
            "epoch [5/10] batch [1040/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.2542 (0.9739) lr 1.5878e-03 eta 0:06:08\n",
            "epoch [5/10] batch [1060/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.9277 (0.9671) lr 1.5878e-03 eta 0:06:07\n",
            "epoch [5/10] batch [1080/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.1035 (0.9652) lr 1.5878e-03 eta 0:06:06\n",
            "epoch [5/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6421 (0.9743) lr 1.5878e-03 eta 0:06:06\n",
            "epoch [5/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4263 (0.9639) lr 1.5878e-03 eta 0:06:05\n",
            "epoch [5/10] batch [1140/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.2139 (0.9540) lr 1.5878e-03 eta 0:06:04\n",
            "epoch [5/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 3.0723 (0.9513) lr 1.5878e-03 eta 0:06:03\n",
            "epoch [5/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0010 (0.9468) lr 1.5878e-03 eta 0:06:02\n",
            "epoch [5/10] batch [1200/1632] time 0.042 (0.042) data 0.000 (0.000) loss 5.5820 (0.9553) lr 1.5878e-03 eta 0:06:01\n",
            "epoch [5/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.3340 (0.9518) lr 1.5878e-03 eta 0:06:00\n",
            "epoch [5/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1699 (0.9482) lr 1.5878e-03 eta 0:05:59\n",
            "epoch [5/10] batch [1260/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.4766 (0.9505) lr 1.5878e-03 eta 0:05:58\n",
            "epoch [5/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.2812 (0.9469) lr 1.5878e-03 eta 0:05:57\n",
            "epoch [5/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.7568 (0.9534) lr 1.5878e-03 eta 0:05:56\n",
            "epoch [5/10] batch [1320/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.9233 (0.9560) lr 1.5878e-03 eta 0:05:55\n",
            "epoch [5/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.7402 (0.9622) lr 1.5878e-03 eta 0:05:54\n",
            "epoch [5/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0502 (0.9618) lr 1.5878e-03 eta 0:05:53\n",
            "epoch [5/10] batch [1380/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4202 (0.9592) lr 1.5878e-03 eta 0:05:52\n",
            "epoch [5/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5312 (0.9593) lr 1.5878e-03 eta 0:05:52\n",
            "epoch [5/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.4707 (0.9533) lr 1.5878e-03 eta 0:05:51\n",
            "epoch [5/10] batch [1440/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0071 (0.9623) lr 1.5878e-03 eta 0:05:50\n",
            "epoch [5/10] batch [1460/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0398 (0.9592) lr 1.5878e-03 eta 0:05:49\n",
            "epoch [5/10] batch [1480/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0495 (0.9584) lr 1.5878e-03 eta 0:05:48\n",
            "epoch [5/10] batch [1500/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.2852 (0.9553) lr 1.5878e-03 eta 0:05:47\n",
            "epoch [5/10] batch [1520/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0326 (0.9566) lr 1.5878e-03 eta 0:05:46\n",
            "epoch [5/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0641 (0.9580) lr 1.5878e-03 eta 0:05:45\n",
            "epoch [5/10] batch [1560/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.3193 (0.9579) lr 1.5878e-03 eta 0:05:45\n",
            "epoch [5/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0008 (0.9586) lr 1.5878e-03 eta 0:05:44\n",
            "epoch [5/10] batch [1600/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8623 (0.9569) lr 1.5878e-03 eta 0:05:43\n",
            "epoch [5/10] batch [1620/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1892 (0.9514) lr 1.5878e-03 eta 0:05:42\n",
            "epoch [6/10] batch [20/1632] time 0.041 (0.054) data 0.000 (0.012) loss 1.2988 (0.6914) lr 1.3090e-03 eta 0:07:22\n",
            "epoch [6/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.006) loss 0.2194 (0.7014) lr 1.3090e-03 eta 0:06:26\n",
            "epoch [6/10] batch [60/1632] time 0.042 (0.046) data 0.000 (0.004) loss 0.4456 (0.6749) lr 1.3090e-03 eta 0:06:13\n",
            "epoch [6/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 1.5977 (0.7277) lr 1.3090e-03 eta 0:06:02\n",
            "epoch [6/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 0.2112 (0.7406) lr 1.3090e-03 eta 0:05:57\n",
            "epoch [6/10] batch [120/1632] time 0.042 (0.044) data 0.000 (0.002) loss 3.0293 (0.7198) lr 1.3090e-03 eta 0:05:53\n",
            "epoch [6/10] batch [140/1632] time 0.041 (0.044) data 0.000 (0.002) loss 1.2861 (0.7661) lr 1.3090e-03 eta 0:05:49\n",
            "epoch [6/10] batch [160/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.5879 (0.7939) lr 1.3090e-03 eta 0:05:46\n",
            "epoch [6/10] batch [180/1632] time 0.043 (0.043) data 0.000 (0.002) loss 2.1348 (0.8873) lr 1.3090e-03 eta 0:05:45\n",
            "epoch [6/10] batch [200/1632] time 0.042 (0.043) data 0.000 (0.001) loss 4.7773 (0.9152) lr 1.3090e-03 eta 0:05:44\n",
            "epoch [6/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1741 (0.9203) lr 1.3090e-03 eta 0:05:42\n",
            "epoch [6/10] batch [240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1460 (0.9618) lr 1.3090e-03 eta 0:05:41\n",
            "epoch [6/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0500 (0.9235) lr 1.3090e-03 eta 0:05:40\n",
            "epoch [6/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0682 (0.8898) lr 1.3090e-03 eta 0:05:38\n",
            "epoch [6/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0026 (0.8932) lr 1.3090e-03 eta 0:05:37\n",
            "epoch [6/10] batch [320/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0047 (0.8966) lr 1.3090e-03 eta 0:05:35\n",
            "epoch [6/10] batch [340/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0013 (0.9062) lr 1.3090e-03 eta 0:05:34\n",
            "epoch [6/10] batch [360/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.0010 (0.9233) lr 1.3090e-03 eta 0:05:32\n",
            "epoch [6/10] batch [380/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.9404 (0.9176) lr 1.3090e-03 eta 0:05:31\n",
            "epoch [6/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7905 (0.9013) lr 1.3090e-03 eta 0:05:29\n",
            "epoch [6/10] batch [420/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.1533 (0.8883) lr 1.3090e-03 eta 0:05:28\n",
            "epoch [6/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.1914 (0.8748) lr 1.3090e-03 eta 0:05:26\n",
            "epoch [6/10] batch [460/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.2158 (0.8572) lr 1.3090e-03 eta 0:05:25\n",
            "epoch [6/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2140 (0.8744) lr 1.3090e-03 eta 0:05:24\n",
            "epoch [6/10] batch [500/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.1680 (0.8660) lr 1.3090e-03 eta 0:05:23\n",
            "epoch [6/10] batch [520/1632] time 0.044 (0.042) data 0.000 (0.001) loss 1.6689 (0.8580) lr 1.3090e-03 eta 0:05:22\n",
            "epoch [6/10] batch [540/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.4312 (0.8633) lr 1.3090e-03 eta 0:05:22\n",
            "epoch [6/10] batch [560/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.0381 (0.8596) lr 1.3090e-03 eta 0:05:21\n",
            "epoch [6/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0186 (0.8521) lr 1.3090e-03 eta 0:05:20\n",
            "epoch [6/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6216 (0.8528) lr 1.3090e-03 eta 0:05:19\n",
            "epoch [6/10] batch [620/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0529 (0.8659) lr 1.3090e-03 eta 0:05:18\n",
            "epoch [6/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0696 (0.8565) lr 1.3090e-03 eta 0:05:17\n",
            "epoch [6/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3303 (0.8540) lr 1.3090e-03 eta 0:05:16\n",
            "epoch [6/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8413 (0.8486) lr 1.3090e-03 eta 0:05:15\n",
            "epoch [6/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2554 (0.8568) lr 1.3090e-03 eta 0:05:14\n",
            "epoch [6/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0048 (0.8495) lr 1.3090e-03 eta 0:05:13\n",
            "epoch [6/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.3203 (0.8547) lr 1.3090e-03 eta 0:05:12\n",
            "epoch [6/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.4194 (0.8460) lr 1.3090e-03 eta 0:05:11\n",
            "epoch [6/10] batch [780/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0825 (0.8484) lr 1.3090e-03 eta 0:05:11\n",
            "epoch [6/10] batch [800/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1561 (0.8536) lr 1.3090e-03 eta 0:05:10\n",
            "epoch [6/10] batch [820/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.7852 (0.8481) lr 1.3090e-03 eta 0:05:09\n",
            "epoch [6/10] batch [840/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0300 (0.8492) lr 1.3090e-03 eta 0:05:08\n",
            "epoch [6/10] batch [860/1632] time 0.046 (0.042) data 0.000 (0.001) loss 1.7363 (0.8481) lr 1.3090e-03 eta 0:05:07\n",
            "epoch [6/10] batch [880/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0351 (0.8398) lr 1.3090e-03 eta 0:05:07\n",
            "epoch [6/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7778 (0.8355) lr 1.3090e-03 eta 0:05:06\n",
            "epoch [6/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3496 (0.8463) lr 1.3090e-03 eta 0:05:05\n",
            "epoch [6/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5674 (0.8447) lr 1.3090e-03 eta 0:05:04\n",
            "epoch [6/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3096 (0.8379) lr 1.3090e-03 eta 0:05:03\n",
            "epoch [6/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1753 (0.8321) lr 1.3090e-03 eta 0:05:02\n",
            "epoch [6/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1525 (0.8305) lr 1.3090e-03 eta 0:05:01\n",
            "epoch [6/10] batch [1020/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.4727 (0.8373) lr 1.3090e-03 eta 0:05:00\n",
            "epoch [6/10] batch [1040/1632] time 0.041 (0.042) data 0.000 (0.000) loss 5.2734 (0.8360) lr 1.3090e-03 eta 0:04:59\n",
            "epoch [6/10] batch [1060/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.9775 (0.8394) lr 1.3090e-03 eta 0:04:58\n",
            "epoch [6/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0093 (0.8398) lr 1.3090e-03 eta 0:04:57\n",
            "epoch [6/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 7.3398 (0.8412) lr 1.3090e-03 eta 0:04:57\n",
            "epoch [6/10] batch [1120/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.8066 (0.8388) lr 1.3090e-03 eta 0:04:56\n",
            "epoch [6/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2086 (0.8316) lr 1.3090e-03 eta 0:04:55\n",
            "epoch [6/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0796 (0.8300) lr 1.3090e-03 eta 0:04:54\n",
            "epoch [6/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.1904 (0.8207) lr 1.3090e-03 eta 0:04:53\n",
            "epoch [6/10] batch [1200/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0146 (0.8245) lr 1.3090e-03 eta 0:04:52\n",
            "epoch [6/10] batch [1220/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0737 (0.8259) lr 1.3090e-03 eta 0:04:52\n",
            "epoch [6/10] batch [1240/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0238 (0.8309) lr 1.3090e-03 eta 0:04:51\n",
            "epoch [6/10] batch [1260/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.0854 (0.8335) lr 1.3090e-03 eta 0:04:50\n",
            "epoch [6/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3801 (0.8356) lr 1.3090e-03 eta 0:04:50\n",
            "epoch [6/10] batch [1300/1632] time 0.046 (0.042) data 0.000 (0.000) loss 2.2305 (0.8315) lr 1.3090e-03 eta 0:04:49\n",
            "epoch [6/10] batch [1320/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0098 (0.8265) lr 1.3090e-03 eta 0:04:49\n",
            "epoch [6/10] batch [1340/1632] time 0.046 (0.042) data 0.000 (0.000) loss 7.6250 (0.8305) lr 1.3090e-03 eta 0:04:48\n",
            "epoch [6/10] batch [1360/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.1395 (0.8259) lr 1.3090e-03 eta 0:04:48\n",
            "epoch [6/10] batch [1380/1632] time 0.046 (0.042) data 0.000 (0.000) loss 1.7012 (0.8208) lr 1.3090e-03 eta 0:04:47\n",
            "epoch [6/10] batch [1400/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.8633 (0.8233) lr 1.3090e-03 eta 0:04:46\n",
            "epoch [6/10] batch [1420/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.2949 (0.8232) lr 1.3090e-03 eta 0:04:46\n",
            "epoch [6/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0341 (0.8159) lr 1.3090e-03 eta 0:04:45\n",
            "epoch [6/10] batch [1460/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0224 (0.8109) lr 1.3090e-03 eta 0:04:44\n",
            "epoch [6/10] batch [1480/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1124 (0.8149) lr 1.3090e-03 eta 0:04:43\n",
            "epoch [6/10] batch [1500/1632] time 0.041 (0.043) data 0.000 (0.000) loss 0.0322 (0.8201) lr 1.3090e-03 eta 0:04:43\n",
            "epoch [6/10] batch [1520/1632] time 0.041 (0.043) data 0.000 (0.000) loss 0.0091 (0.8224) lr 1.3090e-03 eta 0:04:42\n",
            "epoch [6/10] batch [1540/1632] time 0.043 (0.043) data 0.000 (0.000) loss 1.5752 (0.8270) lr 1.3090e-03 eta 0:04:41\n",
            "epoch [6/10] batch [1560/1632] time 0.042 (0.043) data 0.000 (0.000) loss 0.0112 (0.8284) lr 1.3090e-03 eta 0:04:40\n",
            "epoch [6/10] batch [1580/1632] time 0.043 (0.043) data 0.000 (0.000) loss 0.0038 (0.8243) lr 1.3090e-03 eta 0:04:39\n",
            "epoch [6/10] batch [1600/1632] time 0.043 (0.043) data 0.000 (0.000) loss 1.4961 (0.8239) lr 1.3090e-03 eta 0:04:38\n",
            "epoch [6/10] batch [1620/1632] time 0.039 (0.043) data 0.000 (0.000) loss 0.9561 (0.8238) lr 1.3090e-03 eta 0:04:38\n",
            "epoch [7/10] batch [20/1632] time 0.041 (0.055) data 0.000 (0.013) loss 1.2891 (0.5357) lr 1.0000e-03 eta 0:06:01\n",
            "epoch [7/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.006) loss 0.2186 (0.5577) lr 1.0000e-03 eta 0:05:13\n",
            "epoch [7/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.5039 (0.6290) lr 1.0000e-03 eta 0:04:57\n",
            "epoch [7/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 1.0322 (0.6548) lr 1.0000e-03 eta 0:04:48\n",
            "epoch [7/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 0.8301 (0.6349) lr 1.0000e-03 eta 0:04:43\n",
            "epoch [7/10] batch [120/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.4219 (0.8084) lr 1.0000e-03 eta 0:04:40\n",
            "epoch [7/10] batch [140/1632] time 0.043 (0.044) data 0.000 (0.002) loss 0.0786 (0.8074) lr 1.0000e-03 eta 0:04:38\n",
            "epoch [7/10] batch [160/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.3298 (0.7877) lr 1.0000e-03 eta 0:04:35\n",
            "epoch [7/10] batch [180/1632] time 0.043 (0.043) data 0.000 (0.002) loss 0.2405 (0.7916) lr 1.0000e-03 eta 0:04:34\n",
            "epoch [7/10] batch [200/1632] time 0.044 (0.043) data 0.001 (0.001) loss 5.9414 (0.8046) lr 1.0000e-03 eta 0:04:33\n",
            "epoch [7/10] batch [220/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0991 (0.8019) lr 1.0000e-03 eta 0:04:32\n",
            "epoch [7/10] batch [240/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.8618 (0.7886) lr 1.0000e-03 eta 0:04:30\n",
            "epoch [7/10] batch [260/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.7002 (0.8028) lr 1.0000e-03 eta 0:04:29\n",
            "epoch [7/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0029 (0.7882) lr 1.0000e-03 eta 0:04:28\n",
            "epoch [7/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.3645 (0.7657) lr 1.0000e-03 eta 0:04:27\n",
            "epoch [7/10] batch [320/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0213 (0.7413) lr 1.0000e-03 eta 0:04:26\n",
            "epoch [7/10] batch [340/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0135 (0.7352) lr 1.0000e-03 eta 0:04:25\n",
            "epoch [7/10] batch [360/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0992 (0.7287) lr 1.0000e-03 eta 0:04:24\n",
            "epoch [7/10] batch [380/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2271 (0.7461) lr 1.0000e-03 eta 0:04:22\n",
            "epoch [7/10] batch [400/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1401 (0.7657) lr 1.0000e-03 eta 0:04:21\n",
            "epoch [7/10] batch [420/1632] time 0.042 (0.043) data 0.000 (0.001) loss 3.2031 (0.7709) lr 1.0000e-03 eta 0:04:20\n",
            "epoch [7/10] batch [440/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1215 (0.7735) lr 1.0000e-03 eta 0:04:18\n",
            "epoch [7/10] batch [460/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.9717 (0.7735) lr 1.0000e-03 eta 0:04:17\n",
            "epoch [7/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1801 (0.7755) lr 1.0000e-03 eta 0:04:16\n",
            "epoch [7/10] batch [500/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0150 (0.7970) lr 1.0000e-03 eta 0:04:15\n",
            "epoch [7/10] batch [520/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.6641 (0.8099) lr 1.0000e-03 eta 0:04:14\n",
            "epoch [7/10] batch [540/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3120 (0.8016) lr 1.0000e-03 eta 0:04:13\n",
            "epoch [7/10] batch [560/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.4932 (0.7980) lr 1.0000e-03 eta 0:04:12\n",
            "epoch [7/10] batch [580/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.0976 (0.7851) lr 1.0000e-03 eta 0:04:11\n",
            "epoch [7/10] batch [600/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0399 (0.7805) lr 1.0000e-03 eta 0:04:11\n",
            "epoch [7/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2484 (0.8070) lr 1.0000e-03 eta 0:04:10\n",
            "epoch [7/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0646 (0.8164) lr 1.0000e-03 eta 0:04:09\n",
            "epoch [7/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4700 (0.8324) lr 1.0000e-03 eta 0:04:08\n",
            "epoch [7/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1079 (0.8285) lr 1.0000e-03 eta 0:04:07\n",
            "epoch [7/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 3.6426 (0.8426) lr 1.0000e-03 eta 0:04:06\n",
            "epoch [7/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.4717 (0.8447) lr 1.0000e-03 eta 0:04:04\n",
            "epoch [7/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1313 (0.8437) lr 1.0000e-03 eta 0:04:04\n",
            "epoch [7/10] batch [760/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8608 (0.8472) lr 1.0000e-03 eta 0:04:03\n",
            "epoch [7/10] batch [780/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0346 (0.8497) lr 1.0000e-03 eta 0:04:02\n",
            "epoch [7/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1213 (0.8481) lr 1.0000e-03 eta 0:04:01\n",
            "epoch [7/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 3.3301 (0.8526) lr 1.0000e-03 eta 0:04:00\n",
            "epoch [7/10] batch [840/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.1534 (0.8421) lr 1.0000e-03 eta 0:03:59\n",
            "epoch [7/10] batch [860/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.5068 (0.8354) lr 1.0000e-03 eta 0:03:58\n",
            "epoch [7/10] batch [880/1632] time 0.046 (0.042) data 0.000 (0.001) loss 3.1348 (0.8499) lr 1.0000e-03 eta 0:03:58\n",
            "epoch [7/10] batch [900/1632] time 0.046 (0.042) data 0.000 (0.001) loss 0.9082 (0.8708) lr 1.0000e-03 eta 0:03:57\n",
            "epoch [7/10] batch [920/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.6987 (0.8708) lr 1.0000e-03 eta 0:03:56\n",
            "epoch [7/10] batch [940/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.1161 (0.8716) lr 1.0000e-03 eta 0:03:56\n",
            "epoch [7/10] batch [960/1632] time 0.048 (0.042) data 0.000 (0.001) loss 1.1377 (0.8690) lr 1.0000e-03 eta 0:03:55\n",
            "epoch [7/10] batch [980/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2302 (0.8649) lr 1.0000e-03 eta 0:03:54\n",
            "epoch [7/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0035 (0.8649) lr 1.0000e-03 eta 0:03:53\n",
            "epoch [7/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1458 (0.8621) lr 1.0000e-03 eta 0:03:52\n",
            "epoch [7/10] batch [1040/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0784 (0.8612) lr 1.0000e-03 eta 0:03:52\n",
            "epoch [7/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.2090 (0.8665) lr 1.0000e-03 eta 0:03:51\n",
            "epoch [7/10] batch [1080/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0133 (0.8589) lr 1.0000e-03 eta 0:03:50\n",
            "epoch [7/10] batch [1100/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0471 (0.8660) lr 1.0000e-03 eta 0:03:49\n",
            "epoch [7/10] batch [1120/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0691 (0.8640) lr 1.0000e-03 eta 0:03:48\n",
            "epoch [7/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.2910 (0.8608) lr 1.0000e-03 eta 0:03:47\n",
            "epoch [7/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.8735 (0.8509) lr 1.0000e-03 eta 0:03:46\n",
            "epoch [7/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0185 (0.8544) lr 1.0000e-03 eta 0:03:45\n",
            "epoch [7/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0195 (0.8515) lr 1.0000e-03 eta 0:03:45\n",
            "epoch [7/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3962 (0.8479) lr 1.0000e-03 eta 0:03:44\n",
            "epoch [7/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5786 (0.8426) lr 1.0000e-03 eta 0:03:43\n",
            "epoch [7/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0151 (0.8351) lr 1.0000e-03 eta 0:03:42\n",
            "epoch [7/10] batch [1280/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.1738 (0.8343) lr 1.0000e-03 eta 0:03:41\n",
            "epoch [7/10] batch [1300/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0339 (0.8352) lr 1.0000e-03 eta 0:03:40\n",
            "epoch [7/10] batch [1320/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.6738 (0.8284) lr 1.0000e-03 eta 0:03:40\n",
            "epoch [7/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1903 (0.8307) lr 1.0000e-03 eta 0:03:39\n",
            "epoch [7/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.3428 (0.8284) lr 1.0000e-03 eta 0:03:38\n",
            "epoch [7/10] batch [1380/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.0889 (0.8267) lr 1.0000e-03 eta 0:03:37\n",
            "epoch [7/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0008 (0.8229) lr 1.0000e-03 eta 0:03:36\n",
            "epoch [7/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.7188 (0.8190) lr 1.0000e-03 eta 0:03:35\n",
            "epoch [7/10] batch [1440/1632] time 0.040 (0.042) data 0.000 (0.000) loss 3.3477 (0.8181) lr 1.0000e-03 eta 0:03:34\n",
            "epoch [7/10] batch [1460/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.7852 (0.8165) lr 1.0000e-03 eta 0:03:33\n",
            "epoch [7/10] batch [1480/1632] time 0.044 (0.042) data 0.000 (0.000) loss 1.1953 (0.8142) lr 1.0000e-03 eta 0:03:32\n",
            "epoch [7/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.8584 (0.8122) lr 1.0000e-03 eta 0:03:31\n",
            "epoch [7/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0121 (0.8131) lr 1.0000e-03 eta 0:03:31\n",
            "epoch [7/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0146 (0.8108) lr 1.0000e-03 eta 0:03:30\n",
            "epoch [7/10] batch [1560/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1946 (0.8069) lr 1.0000e-03 eta 0:03:29\n",
            "epoch [7/10] batch [1580/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.2415 (0.8104) lr 1.0000e-03 eta 0:03:28\n",
            "epoch [7/10] batch [1600/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0983 (0.8085) lr 1.0000e-03 eta 0:03:27\n",
            "epoch [7/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.5664 (0.8091) lr 1.0000e-03 eta 0:03:27\n",
            "epoch [8/10] batch [20/1632] time 0.041 (0.055) data 0.000 (0.012) loss 3.5977 (1.0590) lr 6.9098e-04 eta 0:04:29\n",
            "epoch [8/10] batch [40/1632] time 0.041 (0.049) data 0.000 (0.006) loss 0.0380 (0.8792) lr 6.9098e-04 eta 0:03:55\n",
            "epoch [8/10] batch [60/1632] time 0.045 (0.046) data 0.000 (0.004) loss 0.6108 (0.8225) lr 6.9098e-04 eta 0:03:43\n",
            "epoch [8/10] batch [80/1632] time 0.042 (0.045) data 0.000 (0.003) loss 0.0409 (0.8494) lr 6.9098e-04 eta 0:03:37\n",
            "epoch [8/10] batch [100/1632] time 0.045 (0.044) data 0.000 (0.003) loss 0.4561 (0.9191) lr 6.9098e-04 eta 0:03:32\n",
            "epoch [8/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0707 (0.8356) lr 6.9098e-04 eta 0:03:29\n",
            "epoch [8/10] batch [140/1632] time 0.045 (0.044) data 0.000 (0.002) loss 1.9160 (0.8691) lr 6.9098e-04 eta 0:03:27\n",
            "epoch [8/10] batch [160/1632] time 0.040 (0.043) data 0.000 (0.002) loss 0.0801 (0.8386) lr 6.9098e-04 eta 0:03:25\n",
            "epoch [8/10] batch [180/1632] time 0.044 (0.043) data 0.000 (0.002) loss 0.0711 (0.8833) lr 6.9098e-04 eta 0:03:24\n",
            "epoch [8/10] batch [200/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2435 (0.8899) lr 6.9098e-04 eta 0:03:23\n",
            "epoch [8/10] batch [220/1632] time 0.043 (0.043) data 0.000 (0.001) loss 1.0674 (0.8588) lr 6.9098e-04 eta 0:03:22\n",
            "epoch [8/10] batch [240/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.3738 (0.8545) lr 6.9098e-04 eta 0:03:21\n",
            "epoch [8/10] batch [260/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.0108 (0.8313) lr 6.9098e-04 eta 0:03:20\n",
            "epoch [8/10] batch [280/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.3640 (0.8135) lr 6.9098e-04 eta 0:03:19\n",
            "epoch [8/10] batch [300/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.1716 (0.8053) lr 6.9098e-04 eta 0:03:18\n",
            "epoch [8/10] batch [320/1632] time 0.044 (0.043) data 0.000 (0.001) loss 6.6875 (0.8319) lr 6.9098e-04 eta 0:03:17\n",
            "epoch [8/10] batch [340/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6162 (0.8016) lr 6.9098e-04 eta 0:03:16\n",
            "epoch [8/10] batch [360/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.2031 (0.7875) lr 6.9098e-04 eta 0:03:15\n",
            "epoch [8/10] batch [380/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0626 (0.7683) lr 6.9098e-04 eta 0:03:13\n",
            "epoch [8/10] batch [400/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2852 (0.7881) lr 6.9098e-04 eta 0:03:12\n",
            "epoch [8/10] batch [420/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.2354 (0.7758) lr 6.9098e-04 eta 0:03:11\n",
            "epoch [8/10] batch [440/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0492 (0.7736) lr 6.9098e-04 eta 0:03:10\n",
            "epoch [8/10] batch [460/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2632 (0.7802) lr 6.9098e-04 eta 0:03:09\n",
            "epoch [8/10] batch [480/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0199 (0.7897) lr 6.9098e-04 eta 0:03:07\n",
            "epoch [8/10] batch [500/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0036 (0.7715) lr 6.9098e-04 eta 0:03:06\n",
            "epoch [8/10] batch [520/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2021 (0.7915) lr 6.9098e-04 eta 0:03:05\n",
            "epoch [8/10] batch [540/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0392 (0.7927) lr 6.9098e-04 eta 0:03:04\n",
            "epoch [8/10] batch [560/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.6372 (0.7892) lr 6.9098e-04 eta 0:03:03\n",
            "epoch [8/10] batch [580/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.4055 (0.7883) lr 6.9098e-04 eta 0:03:03\n",
            "epoch [8/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1212 (0.7843) lr 6.9098e-04 eta 0:03:02\n",
            "epoch [8/10] batch [620/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0772 (0.7827) lr 6.9098e-04 eta 0:03:01\n",
            "epoch [8/10] batch [640/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.6230 (0.7907) lr 6.9098e-04 eta 0:03:00\n",
            "epoch [8/10] batch [660/1632] time 0.042 (0.042) data 0.000 (0.001) loss 6.1094 (0.8135) lr 6.9098e-04 eta 0:02:59\n",
            "epoch [8/10] batch [680/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0220 (0.8035) lr 6.9098e-04 eta 0:02:59\n",
            "epoch [8/10] batch [700/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.2778 (0.7896) lr 6.9098e-04 eta 0:02:58\n",
            "epoch [8/10] batch [720/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.3884 (0.7797) lr 6.9098e-04 eta 0:02:57\n",
            "epoch [8/10] batch [740/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0725 (0.7885) lr 6.9098e-04 eta 0:02:56\n",
            "epoch [8/10] batch [760/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2224 (0.7943) lr 6.9098e-04 eta 0:02:55\n",
            "epoch [8/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0798 (0.7889) lr 6.9098e-04 eta 0:02:54\n",
            "epoch [8/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1354 (0.7943) lr 6.9098e-04 eta 0:02:53\n",
            "epoch [8/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.4434 (0.8101) lr 6.9098e-04 eta 0:02:52\n",
            "epoch [8/10] batch [840/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0026 (0.8018) lr 6.9098e-04 eta 0:02:51\n",
            "epoch [8/10] batch [860/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0690 (0.7927) lr 6.9098e-04 eta 0:02:51\n",
            "epoch [8/10] batch [880/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.2793 (0.8049) lr 6.9098e-04 eta 0:02:50\n",
            "epoch [8/10] batch [900/1632] time 0.046 (0.042) data 0.000 (0.001) loss 0.6235 (0.8070) lr 6.9098e-04 eta 0:02:49\n",
            "epoch [8/10] batch [920/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1735 (0.7967) lr 6.9098e-04 eta 0:02:48\n",
            "epoch [8/10] batch [940/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.2028 (0.7949) lr 6.9098e-04 eta 0:02:47\n",
            "epoch [8/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1350 (0.7970) lr 6.9098e-04 eta 0:02:47\n",
            "epoch [8/10] batch [980/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.0898 (0.8041) lr 6.9098e-04 eta 0:02:46\n",
            "epoch [8/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0125 (0.8100) lr 6.9098e-04 eta 0:02:45\n",
            "epoch [8/10] batch [1020/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0424 (0.8123) lr 6.9098e-04 eta 0:02:44\n",
            "epoch [8/10] batch [1040/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0382 (0.8014) lr 6.9098e-04 eta 0:02:43\n",
            "epoch [8/10] batch [1060/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0402 (0.7966) lr 6.9098e-04 eta 0:02:42\n",
            "epoch [8/10] batch [1080/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3247 (0.8042) lr 6.9098e-04 eta 0:02:42\n",
            "epoch [8/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.7334 (0.8003) lr 6.9098e-04 eta 0:02:41\n",
            "epoch [8/10] batch [1120/1632] time 0.089 (0.042) data 0.000 (0.000) loss 0.4392 (0.7943) lr 6.9098e-04 eta 0:02:40\n",
            "epoch [8/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0991 (0.7911) lr 6.9098e-04 eta 0:02:39\n",
            "epoch [8/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8462 (0.7904) lr 6.9098e-04 eta 0:02:38\n",
            "epoch [8/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1157 (0.7927) lr 6.9098e-04 eta 0:02:37\n",
            "epoch [8/10] batch [1200/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.7979 (0.7937) lr 6.9098e-04 eta 0:02:36\n",
            "epoch [8/10] batch [1220/1632] time 0.046 (0.042) data 0.000 (0.000) loss 2.6035 (0.7913) lr 6.9098e-04 eta 0:02:36\n",
            "epoch [8/10] batch [1240/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0006 (0.7936) lr 6.9098e-04 eta 0:02:35\n",
            "epoch [8/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.1504 (0.7954) lr 6.9098e-04 eta 0:02:34\n",
            "epoch [8/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0008 (0.7992) lr 6.9098e-04 eta 0:02:33\n",
            "epoch [8/10] batch [1300/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0347 (0.7966) lr 6.9098e-04 eta 0:02:32\n",
            "epoch [8/10] batch [1320/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2783 (0.7973) lr 6.9098e-04 eta 0:02:31\n",
            "epoch [8/10] batch [1340/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4082 (0.7930) lr 6.9098e-04 eta 0:02:30\n",
            "epoch [8/10] batch [1360/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.6763 (0.7954) lr 6.9098e-04 eta 0:02:29\n",
            "epoch [8/10] batch [1380/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0894 (0.7971) lr 6.9098e-04 eta 0:02:29\n",
            "epoch [8/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1312 (0.7952) lr 6.9098e-04 eta 0:02:28\n",
            "epoch [8/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.9663 (0.7979) lr 6.9098e-04 eta 0:02:27\n",
            "epoch [8/10] batch [1440/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1216 (0.7951) lr 6.9098e-04 eta 0:02:26\n",
            "epoch [8/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0939 (0.7951) lr 6.9098e-04 eta 0:02:25\n",
            "epoch [8/10] batch [1480/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0173 (0.7962) lr 6.9098e-04 eta 0:02:24\n",
            "epoch [8/10] batch [1500/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3086 (0.7959) lr 6.9098e-04 eta 0:02:23\n",
            "epoch [8/10] batch [1520/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0609 (0.7997) lr 6.9098e-04 eta 0:02:23\n",
            "epoch [8/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3354 (0.7978) lr 6.9098e-04 eta 0:02:22\n",
            "epoch [8/10] batch [1560/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0966 (0.7936) lr 6.9098e-04 eta 0:02:21\n",
            "epoch [8/10] batch [1580/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0235 (0.7938) lr 6.9098e-04 eta 0:02:20\n",
            "epoch [8/10] batch [1600/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0305 (0.7940) lr 6.9098e-04 eta 0:02:19\n",
            "epoch [8/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 1.3271 (0.7949) lr 6.9098e-04 eta 0:02:18\n",
            "epoch [9/10] batch [20/1632] time 0.041 (0.055) data 0.000 (0.012) loss 0.8457 (0.4372) lr 4.1221e-04 eta 0:02:59\n",
            "epoch [9/10] batch [40/1632] time 0.041 (0.049) data 0.000 (0.006) loss 0.3328 (0.6420) lr 4.1221e-04 eta 0:02:37\n",
            "epoch [9/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.0160 (0.7468) lr 4.1221e-04 eta 0:02:28\n",
            "epoch [9/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.0153 (0.7511) lr 4.1221e-04 eta 0:02:24\n",
            "epoch [9/10] batch [100/1632] time 0.040 (0.045) data 0.000 (0.003) loss 0.1287 (0.7120) lr 4.1221e-04 eta 0:02:21\n",
            "epoch [9/10] batch [120/1632] time 0.045 (0.044) data 0.000 (0.002) loss 1.1934 (0.6801) lr 4.1221e-04 eta 0:02:19\n",
            "epoch [9/10] batch [140/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.1122 (0.6801) lr 4.1221e-04 eta 0:02:18\n",
            "epoch [9/10] batch [160/1632] time 0.046 (0.044) data 0.000 (0.002) loss 0.1083 (0.7151) lr 4.1221e-04 eta 0:02:16\n",
            "epoch [9/10] batch [180/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.3289 (0.6919) lr 4.1221e-04 eta 0:02:15\n",
            "epoch [9/10] batch [200/1632] time 0.045 (0.044) data 0.000 (0.001) loss 0.4341 (0.7412) lr 4.1221e-04 eta 0:02:14\n",
            "epoch [9/10] batch [220/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.4028 (0.7584) lr 4.1221e-04 eta 0:02:13\n",
            "epoch [9/10] batch [240/1632] time 0.046 (0.044) data 0.000 (0.001) loss 0.5093 (0.7407) lr 4.1221e-04 eta 0:02:12\n",
            "epoch [9/10] batch [260/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.0051 (0.7105) lr 4.1221e-04 eta 0:02:10\n",
            "epoch [9/10] batch [280/1632] time 0.045 (0.044) data 0.000 (0.001) loss 0.8071 (0.7194) lr 4.1221e-04 eta 0:02:09\n",
            "epoch [9/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2969 (0.7499) lr 4.1221e-04 eta 0:02:08\n",
            "epoch [9/10] batch [320/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.1062 (0.7432) lr 4.1221e-04 eta 0:02:07\n",
            "epoch [9/10] batch [340/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0304 (0.7266) lr 4.1221e-04 eta 0:02:06\n",
            "epoch [9/10] batch [360/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0005 (0.7113) lr 4.1221e-04 eta 0:02:05\n",
            "epoch [9/10] batch [380/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0692 (0.7374) lr 4.1221e-04 eta 0:02:04\n",
            "epoch [9/10] batch [400/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2054 (0.7546) lr 4.1221e-04 eta 0:02:03\n",
            "epoch [9/10] batch [420/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.5132 (0.7366) lr 4.1221e-04 eta 0:02:02\n",
            "epoch [9/10] batch [440/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2544 (0.7374) lr 4.1221e-04 eta 0:02:01\n",
            "epoch [9/10] batch [460/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2764 (0.7318) lr 4.1221e-04 eta 0:02:00\n",
            "epoch [9/10] batch [480/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2374 (0.7301) lr 4.1221e-04 eta 0:01:59\n",
            "epoch [9/10] batch [500/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0500 (0.7327) lr 4.1221e-04 eta 0:01:58\n",
            "epoch [9/10] batch [520/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.3916 (0.7436) lr 4.1221e-04 eta 0:01:57\n",
            "epoch [9/10] batch [540/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0986 (0.7501) lr 4.1221e-04 eta 0:01:56\n",
            "epoch [9/10] batch [560/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0882 (0.7361) lr 4.1221e-04 eta 0:01:55\n",
            "epoch [9/10] batch [580/1632] time 0.042 (0.043) data 0.001 (0.001) loss 0.0005 (0.7315) lr 4.1221e-04 eta 0:01:54\n",
            "epoch [9/10] batch [600/1632] time 0.040 (0.043) data 0.000 (0.001) loss 1.3457 (0.7358) lr 4.1221e-04 eta 0:01:53\n",
            "epoch [9/10] batch [620/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.5410 (0.7350) lr 4.1221e-04 eta 0:01:52\n",
            "epoch [9/10] batch [640/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.0221 (0.7338) lr 4.1221e-04 eta 0:01:51\n",
            "epoch [9/10] batch [660/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.0391 (0.7367) lr 4.1221e-04 eta 0:01:50\n",
            "epoch [9/10] batch [680/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.8403 (0.7333) lr 4.1221e-04 eta 0:01:50\n",
            "epoch [9/10] batch [700/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.3767 (0.7291) lr 4.1221e-04 eta 0:01:49\n",
            "epoch [9/10] batch [720/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.0823 (0.7337) lr 4.1221e-04 eta 0:01:48\n",
            "epoch [9/10] batch [740/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.1236 (0.7411) lr 4.1221e-04 eta 0:01:47\n",
            "epoch [9/10] batch [760/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.1638 (0.7473) lr 4.1221e-04 eta 0:01:46\n",
            "epoch [9/10] batch [780/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.0005 (0.7634) lr 4.1221e-04 eta 0:01:45\n",
            "epoch [9/10] batch [800/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0812 (0.7582) lr 4.1221e-04 eta 0:01:45\n",
            "epoch [9/10] batch [820/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.4106 (0.7665) lr 4.1221e-04 eta 0:01:44\n",
            "epoch [9/10] batch [840/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0820 (0.7542) lr 4.1221e-04 eta 0:01:43\n",
            "epoch [9/10] batch [860/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.0928 (0.7537) lr 4.1221e-04 eta 0:01:42\n",
            "epoch [9/10] batch [880/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.4138 (0.7576) lr 4.1221e-04 eta 0:01:41\n",
            "epoch [9/10] batch [900/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.4187 (0.7518) lr 4.1221e-04 eta 0:01:40\n",
            "epoch [9/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0083 (0.7506) lr 4.1221e-04 eta 0:01:39\n",
            "epoch [9/10] batch [940/1632] time 0.042 (0.043) data 0.000 (0.000) loss 0.8784 (0.7442) lr 4.1221e-04 eta 0:01:38\n",
            "epoch [9/10] batch [960/1632] time 0.045 (0.043) data 0.000 (0.000) loss 0.0307 (0.7426) lr 4.1221e-04 eta 0:01:37\n",
            "epoch [9/10] batch [980/1632] time 0.041 (0.043) data 0.000 (0.000) loss 0.4556 (0.7399) lr 4.1221e-04 eta 0:01:37\n",
            "epoch [9/10] batch [1000/1632] time 0.045 (0.043) data 0.000 (0.000) loss 1.6260 (0.7389) lr 4.1221e-04 eta 0:01:36\n",
            "epoch [9/10] batch [1020/1632] time 0.041 (0.043) data 0.000 (0.000) loss 1.5391 (0.7398) lr 4.1221e-04 eta 0:01:35\n",
            "epoch [9/10] batch [1040/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.5127 (0.7564) lr 4.1221e-04 eta 0:01:34\n",
            "epoch [9/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4631 (0.7607) lr 4.1221e-04 eta 0:01:33\n",
            "epoch [9/10] batch [1080/1632] time 0.045 (0.042) data 0.000 (0.000) loss 1.2266 (0.7559) lr 4.1221e-04 eta 0:01:32\n",
            "epoch [9/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0682 (0.7639) lr 4.1221e-04 eta 0:01:31\n",
            "epoch [9/10] batch [1120/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.5132 (0.7579) lr 4.1221e-04 eta 0:01:30\n",
            "epoch [9/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0720 (0.7525) lr 4.1221e-04 eta 0:01:30\n",
            "epoch [9/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.7734 (0.7589) lr 4.1221e-04 eta 0:01:29\n",
            "epoch [9/10] batch [1180/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.3538 (0.7537) lr 4.1221e-04 eta 0:01:28\n",
            "epoch [9/10] batch [1200/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.1091 (0.7513) lr 4.1221e-04 eta 0:01:27\n",
            "epoch [9/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3171 (0.7471) lr 4.1221e-04 eta 0:01:26\n",
            "epoch [9/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0096 (0.7413) lr 4.1221e-04 eta 0:01:25\n",
            "epoch [9/10] batch [1260/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0064 (0.7384) lr 4.1221e-04 eta 0:01:24\n",
            "epoch [9/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0715 (0.7328) lr 4.1221e-04 eta 0:01:23\n",
            "epoch [9/10] batch [1300/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.3579 (0.7335) lr 4.1221e-04 eta 0:01:23\n",
            "epoch [9/10] batch [1320/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4097 (0.7346) lr 4.1221e-04 eta 0:01:22\n",
            "epoch [9/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0920 (0.7336) lr 4.1221e-04 eta 0:01:21\n",
            "epoch [9/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.5684 (0.7371) lr 4.1221e-04 eta 0:01:20\n",
            "epoch [9/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4136 (0.7325) lr 4.1221e-04 eta 0:01:19\n",
            "epoch [9/10] batch [1400/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.1371 (0.7298) lr 4.1221e-04 eta 0:01:18\n",
            "epoch [9/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0046 (0.7304) lr 4.1221e-04 eta 0:01:17\n",
            "epoch [9/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1437 (0.7258) lr 4.1221e-04 eta 0:01:17\n",
            "epoch [9/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.3984 (0.7246) lr 4.1221e-04 eta 0:01:16\n",
            "epoch [9/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.5039 (0.7300) lr 4.1221e-04 eta 0:01:15\n",
            "epoch [9/10] batch [1500/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2375 (0.7304) lr 4.1221e-04 eta 0:01:14\n",
            "epoch [9/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0037 (0.7251) lr 4.1221e-04 eta 0:01:13\n",
            "epoch [9/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.4521 (0.7268) lr 4.1221e-04 eta 0:01:12\n",
            "epoch [9/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6304 (0.7229) lr 4.1221e-04 eta 0:01:11\n",
            "epoch [9/10] batch [1580/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3396 (0.7272) lr 4.1221e-04 eta 0:01:11\n",
            "epoch [9/10] batch [1600/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0217 (0.7322) lr 4.1221e-04 eta 0:01:10\n",
            "epoch [9/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.4136 (0.7345) lr 4.1221e-04 eta 0:01:09\n",
            "epoch [10/10] batch [20/1632] time 0.042 (0.058) data 0.000 (0.014) loss 0.2305 (0.3801) lr 1.9098e-04 eta 0:01:33\n",
            "epoch [10/10] batch [40/1632] time 0.043 (0.050) data 0.000 (0.007) loss 0.1632 (0.4657) lr 1.9098e-04 eta 0:01:20\n",
            "epoch [10/10] batch [60/1632] time 0.043 (0.048) data 0.000 (0.005) loss 0.0155 (0.5677) lr 1.9098e-04 eta 0:01:15\n",
            "epoch [10/10] batch [80/1632] time 0.043 (0.047) data 0.000 (0.004) loss 0.0615 (0.5519) lr 1.9098e-04 eta 0:01:12\n",
            "epoch [10/10] batch [100/1632] time 0.043 (0.046) data 0.000 (0.003) loss 0.0331 (0.4922) lr 1.9098e-04 eta 0:01:10\n",
            "epoch [10/10] batch [120/1632] time 0.043 (0.046) data 0.000 (0.003) loss 0.0711 (0.5647) lr 1.9098e-04 eta 0:01:09\n",
            "epoch [10/10] batch [140/1632] time 0.041 (0.045) data 0.000 (0.002) loss 0.5425 (0.6132) lr 1.9098e-04 eta 0:01:07\n",
            "epoch [10/10] batch [160/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0793 (0.5829) lr 1.9098e-04 eta 0:01:05\n",
            "epoch [10/10] batch [180/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.2057 (0.5904) lr 1.9098e-04 eta 0:01:04\n",
            "epoch [10/10] batch [200/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.3608 (0.5577) lr 1.9098e-04 eta 0:01:02\n",
            "epoch [10/10] batch [220/1632] time 0.042 (0.044) data 0.000 (0.001) loss 0.2590 (0.5800) lr 1.9098e-04 eta 0:01:01\n",
            "epoch [10/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1582 (0.5919) lr 1.9098e-04 eta 0:01:00\n",
            "epoch [10/10] batch [260/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.0840 (0.5759) lr 1.9098e-04 eta 0:00:59\n",
            "epoch [10/10] batch [280/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0246 (0.5763) lr 1.9098e-04 eta 0:00:58\n",
            "epoch [10/10] batch [300/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1707 (0.5682) lr 1.9098e-04 eta 0:00:57\n",
            "epoch [10/10] batch [320/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.9404 (0.5707) lr 1.9098e-04 eta 0:00:56\n",
            "epoch [10/10] batch [340/1632] time 0.045 (0.043) data 0.000 (0.001) loss 1.9121 (0.5774) lr 1.9098e-04 eta 0:00:55\n",
            "epoch [10/10] batch [360/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0308 (0.5949) lr 1.9098e-04 eta 0:00:54\n",
            "epoch [10/10] batch [380/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.1626 (0.5830) lr 1.9098e-04 eta 0:00:53\n",
            "epoch [10/10] batch [400/1632] time 0.043 (0.043) data 0.000 (0.001) loss 4.4219 (0.5858) lr 1.9098e-04 eta 0:00:52\n",
            "epoch [10/10] batch [420/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.1757 (0.5978) lr 1.9098e-04 eta 0:00:52\n",
            "epoch [10/10] batch [440/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.1136 (0.6065) lr 1.9098e-04 eta 0:00:51\n",
            "epoch [10/10] batch [460/1632] time 0.042 (0.043) data 0.000 (0.001) loss 3.5371 (0.6171) lr 1.9098e-04 eta 0:00:50\n",
            "epoch [10/10] batch [480/1632] time 0.043 (0.043) data 0.000 (0.001) loss 2.0977 (0.6320) lr 1.9098e-04 eta 0:00:49\n",
            "epoch [10/10] batch [500/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.2167 (0.6391) lr 1.9098e-04 eta 0:00:48\n",
            "epoch [10/10] batch [520/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.1082 (0.6337) lr 1.9098e-04 eta 0:00:47\n",
            "epoch [10/10] batch [540/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.3840 (0.6289) lr 1.9098e-04 eta 0:00:46\n",
            "epoch [10/10] batch [560/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.3232 (0.6156) lr 1.9098e-04 eta 0:00:45\n",
            "epoch [10/10] batch [580/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0169 (0.6094) lr 1.9098e-04 eta 0:00:45\n",
            "epoch [10/10] batch [600/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2338 (0.6116) lr 1.9098e-04 eta 0:00:44\n",
            "epoch [10/10] batch [620/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2583 (0.6323) lr 1.9098e-04 eta 0:00:43\n",
            "epoch [10/10] batch [640/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1980 (0.6315) lr 1.9098e-04 eta 0:00:42\n",
            "epoch [10/10] batch [660/1632] time 0.043 (0.043) data 0.000 (0.001) loss 2.2812 (0.6442) lr 1.9098e-04 eta 0:00:41\n",
            "epoch [10/10] batch [680/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1790 (0.6508) lr 1.9098e-04 eta 0:00:40\n",
            "epoch [10/10] batch [700/1632] time 0.041 (0.043) data 0.000 (0.001) loss 5.4727 (0.6736) lr 1.9098e-04 eta 0:00:39\n",
            "epoch [10/10] batch [720/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2448 (0.6826) lr 1.9098e-04 eta 0:00:38\n",
            "epoch [10/10] batch [740/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1044 (0.6759) lr 1.9098e-04 eta 0:00:38\n",
            "epoch [10/10] batch [760/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.4570 (0.6720) lr 1.9098e-04 eta 0:00:37\n",
            "epoch [10/10] batch [780/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1980 (0.6728) lr 1.9098e-04 eta 0:00:36\n",
            "epoch [10/10] batch [800/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6470 (0.6837) lr 1.9098e-04 eta 0:00:35\n",
            "epoch [10/10] batch [820/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0029 (0.6826) lr 1.9098e-04 eta 0:00:34\n",
            "epoch [10/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.2148 (0.6966) lr 1.9098e-04 eta 0:00:33\n",
            "epoch [10/10] batch [860/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0043 (0.6953) lr 1.9098e-04 eta 0:00:32\n",
            "epoch [10/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8657 (0.7025) lr 1.9098e-04 eta 0:00:31\n",
            "epoch [10/10] batch [900/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.8867 (0.7039) lr 1.9098e-04 eta 0:00:31\n",
            "epoch [10/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2976 (0.7019) lr 1.9098e-04 eta 0:00:30\n",
            "epoch [10/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6680 (0.7002) lr 1.9098e-04 eta 0:00:29\n",
            "epoch [10/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3098 (0.6992) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 3.6641 (0.7025) lr 1.9098e-04 eta 0:00:27\n",
            "epoch [10/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.6079 (0.7012) lr 1.9098e-04 eta 0:00:26\n",
            "epoch [10/10] batch [1020/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.0033 (0.6922) lr 1.9098e-04 eta 0:00:25\n",
            "epoch [10/10] batch [1040/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0109 (0.6904) lr 1.9098e-04 eta 0:00:25\n",
            "epoch [10/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4365 (0.6856) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [10/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6694 (0.6890) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5864 (0.6973) lr 1.9098e-04 eta 0:00:22\n",
            "epoch [10/10] batch [1120/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5059 (0.7049) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [10/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5332 (0.7020) lr 1.9098e-04 eta 0:00:20\n",
            "epoch [10/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0836 (0.7005) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0058 (0.7014) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3074 (0.7027) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1033 (0.7048) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [10/10] batch [1240/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0081 (0.7028) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [1260/1632] time 0.044 (0.042) data 0.000 (0.000) loss 3.4473 (0.7107) lr 1.9098e-04 eta 0:00:15\n",
            "epoch [10/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.4668 (0.7191) lr 1.9098e-04 eta 0:00:14\n",
            "epoch [10/10] batch [1300/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0277 (0.7153) lr 1.9098e-04 eta 0:00:14\n",
            "epoch [10/10] batch [1320/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.1820 (0.7155) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [1340/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.7256 (0.7181) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.6309 (0.7186) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [10/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3818 (0.7144) lr 1.9098e-04 eta 0:00:10\n",
            "epoch [10/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0615 (0.7172) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2839 (0.7182) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.7388 (0.7216) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0195 (0.7183) lr 1.9098e-04 eta 0:00:07\n",
            "epoch [10/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0009 (0.7225) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 6.0703 (0.7246) lr 1.9098e-04 eta 0:00:05\n",
            "epoch [10/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.8105 (0.7213) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [10/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0279 (0.7172) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4019 (0.7149) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [1580/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.5557 (0.7151) lr 1.9098e-04 eta 0:00:02\n",
            "epoch [10/10] batch [1600/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.7065 (0.7129) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [1620/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0018 (0.7117) lr 1.9098e-04 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-10\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 25/25 [00:30<00:00,  1.23s/it]\n",
            "=> result\n",
            "* total: 2,463\n",
            "* correct: 2,213\n",
            "* accuracy: 89.8%\n",
            "* error: 10.2%\n",
            "* macro_f1: 88.8%\n",
            "Elapsed: 0:12:05\n",
            "2024-12-10 11:32:26.054318: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-10 11:32:26.071440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-10 11:32:26.092415: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-10 11:32:26.098795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-10 11:32:26.113704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-10 11:32:27.159707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/MIP/vit_b16_c4_ep10_batch1.yaml\n",
            "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: 0\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2\n",
            "resume: \n",
            "root: data/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: MIP\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordFlowers\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: data/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: MIP\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               12\n",
            "On-line CPU(s) list:                  0-11\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   6\n",
            "Socket(s):                            1\n",
            "Stepping:                             7\n",
            "BogoMIPS:                             4400.44\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            192 KiB (6 instances)\n",
            "L1i cache:                            192 KiB (6 instances)\n",
            "L2 cache:                             6 MiB (6 instances)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-11\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Not affected\n",
            "Vulnerability Mds:                    Not affected\n",
            "Vulnerability Meltdown:               Not affected\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: MIP\n",
            "Loading dataset: OxfordFlowers\n",
            "Reading split from /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_flowers/split_zhou_OxfordFlowers.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_flowers/split_fewshot/shot_16-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------------\n",
            "Dataset    OxfordFlowers\n",
            "# classes  102\n",
            "# train_x  1,632\n",
            "# val      408\n",
            "# test     2,463\n",
            "---------  -------------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.ctx', 'prompt_learner.meta_net.linear1.weight'}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2/tensorboard)\n",
            "epoch [1/10] batch [20/1632] time 0.041 (0.144) data 0.000 (0.028) loss 0.0558 (3.0087) lr 1.0000e-05 eta 0:38:59\n",
            "epoch [1/10] batch [40/1632] time 0.040 (0.093) data 0.000 (0.014) loss 0.1139 (2.4033) lr 1.0000e-05 eta 0:25:08\n",
            "epoch [1/10] batch [60/1632] time 0.040 (0.076) data 0.000 (0.009) loss 3.7031 (2.3855) lr 1.0000e-05 eta 0:20:31\n",
            "epoch [1/10] batch [80/1632] time 0.040 (0.067) data 0.000 (0.007) loss 1.3701 (2.2371) lr 1.0000e-05 eta 0:18:11\n",
            "epoch [1/10] batch [100/1632] time 0.045 (0.062) data 0.000 (0.006) loss 8.5781 (2.6658) lr 1.0000e-05 eta 0:16:51\n",
            "epoch [1/10] batch [120/1632] time 0.040 (0.059) data 0.000 (0.005) loss 0.2009 (2.5446) lr 1.0000e-05 eta 0:15:55\n",
            "epoch [1/10] batch [140/1632] time 0.042 (0.057) data 0.000 (0.004) loss 9.3281 (2.6358) lr 1.0000e-05 eta 0:15:14\n",
            "epoch [1/10] batch [160/1632] time 0.041 (0.055) data 0.000 (0.004) loss 1.2334 (2.5432) lr 1.0000e-05 eta 0:14:44\n",
            "epoch [1/10] batch [180/1632] time 0.041 (0.053) data 0.000 (0.003) loss 1.2627 (2.4758) lr 1.0000e-05 eta 0:14:20\n",
            "epoch [1/10] batch [200/1632] time 0.040 (0.052) data 0.000 (0.003) loss 1.2432 (2.4169) lr 1.0000e-05 eta 0:13:59\n",
            "epoch [1/10] batch [220/1632] time 0.042 (0.051) data 0.000 (0.003) loss 0.1014 (2.3780) lr 1.0000e-05 eta 0:13:41\n",
            "epoch [1/10] batch [240/1632] time 0.042 (0.050) data 0.000 (0.003) loss 1.9727 (2.3945) lr 1.0000e-05 eta 0:13:27\n",
            "epoch [1/10] batch [260/1632] time 0.039 (0.049) data 0.001 (0.002) loss 1.0488 (2.4083) lr 1.0000e-05 eta 0:13:14\n",
            "epoch [1/10] batch [280/1632] time 0.042 (0.049) data 0.000 (0.002) loss 2.5508 (2.3625) lr 1.0000e-05 eta 0:13:03\n",
            "epoch [1/10] batch [300/1632] time 0.041 (0.048) data 0.000 (0.002) loss 6.0859 (2.3697) lr 1.0000e-05 eta 0:12:53\n",
            "epoch [1/10] batch [320/1632] time 0.039 (0.048) data 0.000 (0.002) loss 2.4023 (2.4005) lr 1.0000e-05 eta 0:12:45\n",
            "epoch [1/10] batch [340/1632] time 0.041 (0.048) data 0.000 (0.002) loss 4.9336 (2.3666) lr 1.0000e-05 eta 0:12:43\n",
            "epoch [1/10] batch [360/1632] time 0.042 (0.047) data 0.000 (0.002) loss 0.0952 (2.3927) lr 1.0000e-05 eta 0:12:36\n",
            "epoch [1/10] batch [380/1632] time 0.042 (0.047) data 0.000 (0.002) loss 1.9961 (2.3473) lr 1.0000e-05 eta 0:12:30\n",
            "epoch [1/10] batch [400/1632] time 0.040 (0.047) data 0.000 (0.002) loss 2.8945 (2.3682) lr 1.0000e-05 eta 0:12:24\n",
            "epoch [1/10] batch [420/1632] time 0.045 (0.047) data 0.000 (0.002) loss 3.3418 (2.3959) lr 1.0000e-05 eta 0:12:19\n",
            "epoch [1/10] batch [440/1632] time 0.041 (0.046) data 0.000 (0.002) loss 1.5996 (2.4066) lr 1.0000e-05 eta 0:12:15\n",
            "epoch [1/10] batch [460/1632] time 0.044 (0.046) data 0.000 (0.002) loss 0.9590 (2.4018) lr 1.0000e-05 eta 0:12:11\n",
            "epoch [1/10] batch [480/1632] time 0.041 (0.046) data 0.000 (0.002) loss 5.4453 (2.4334) lr 1.0000e-05 eta 0:12:08\n",
            "epoch [1/10] batch [500/1632] time 0.042 (0.046) data 0.000 (0.002) loss 2.8516 (2.4223) lr 1.0000e-05 eta 0:12:03\n",
            "epoch [1/10] batch [520/1632] time 0.047 (0.046) data 0.000 (0.002) loss 0.3511 (2.4172) lr 1.0000e-05 eta 0:12:00\n",
            "epoch [1/10] batch [540/1632] time 0.042 (0.045) data 0.000 (0.001) loss 0.9526 (2.4052) lr 1.0000e-05 eta 0:11:56\n",
            "epoch [1/10] batch [560/1632] time 0.039 (0.045) data 0.000 (0.001) loss 0.5044 (2.4049) lr 1.0000e-05 eta 0:11:53\n",
            "epoch [1/10] batch [580/1632] time 0.046 (0.045) data 0.000 (0.001) loss 0.1132 (2.3751) lr 1.0000e-05 eta 0:11:50\n",
            "epoch [1/10] batch [600/1632] time 0.042 (0.045) data 0.000 (0.001) loss 2.9062 (2.3559) lr 1.0000e-05 eta 0:11:47\n",
            "epoch [1/10] batch [620/1632] time 0.039 (0.045) data 0.000 (0.001) loss 0.8984 (2.3393) lr 1.0000e-05 eta 0:11:45\n",
            "epoch [1/10] batch [640/1632] time 0.040 (0.045) data 0.000 (0.001) loss 0.3125 (2.3116) lr 1.0000e-05 eta 0:11:42\n",
            "epoch [1/10] batch [660/1632] time 0.046 (0.045) data 0.000 (0.001) loss 5.0820 (2.3191) lr 1.0000e-05 eta 0:11:40\n",
            "epoch [1/10] batch [680/1632] time 0.042 (0.045) data 0.000 (0.001) loss 6.2500 (2.2865) lr 1.0000e-05 eta 0:11:37\n",
            "epoch [1/10] batch [700/1632] time 0.046 (0.045) data 0.000 (0.001) loss 2.5957 (2.2627) lr 1.0000e-05 eta 0:11:35\n",
            "epoch [1/10] batch [720/1632] time 0.039 (0.044) data 0.000 (0.001) loss 7.3438 (2.2423) lr 1.0000e-05 eta 0:11:33\n",
            "epoch [1/10] batch [740/1632] time 0.041 (0.044) data 0.000 (0.001) loss 2.0254 (2.2474) lr 1.0000e-05 eta 0:11:31\n",
            "epoch [1/10] batch [760/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.4607 (2.2424) lr 1.0000e-05 eta 0:11:29\n",
            "epoch [1/10] batch [780/1632] time 0.040 (0.044) data 0.000 (0.001) loss 6.6094 (2.2451) lr 1.0000e-05 eta 0:11:27\n",
            "epoch [1/10] batch [800/1632] time 0.045 (0.044) data 0.000 (0.001) loss 11.0859 (2.2374) lr 1.0000e-05 eta 0:11:25\n",
            "epoch [1/10] batch [820/1632] time 0.042 (0.044) data 0.000 (0.001) loss 3.5742 (2.2341) lr 1.0000e-05 eta 0:11:23\n",
            "epoch [1/10] batch [840/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.9644 (2.2403) lr 1.0000e-05 eta 0:11:21\n",
            "epoch [1/10] batch [860/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.0698 (2.2399) lr 1.0000e-05 eta 0:11:20\n",
            "epoch [1/10] batch [880/1632] time 0.040 (0.044) data 0.000 (0.001) loss 5.2031 (2.2345) lr 1.0000e-05 eta 0:11:17\n",
            "epoch [1/10] batch [900/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.0192 (2.2435) lr 1.0000e-05 eta 0:11:16\n",
            "epoch [1/10] batch [920/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.0447 (2.2461) lr 1.0000e-05 eta 0:11:16\n",
            "epoch [1/10] batch [940/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.8076 (2.2410) lr 1.0000e-05 eta 0:11:15\n",
            "epoch [1/10] batch [960/1632] time 0.043 (0.044) data 0.000 (0.001) loss 6.8164 (2.2401) lr 1.0000e-05 eta 0:11:13\n",
            "epoch [1/10] batch [980/1632] time 0.042 (0.044) data 0.000 (0.001) loss 0.4480 (2.2335) lr 1.0000e-05 eta 0:11:11\n",
            "epoch [1/10] batch [1000/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.5117 (2.2532) lr 1.0000e-05 eta 0:11:10\n",
            "epoch [1/10] batch [1020/1632] time 0.042 (0.044) data 0.000 (0.001) loss 1.9551 (2.2625) lr 1.0000e-05 eta 0:11:08\n",
            "epoch [1/10] batch [1040/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.4314 (2.2611) lr 1.0000e-05 eta 0:11:06\n",
            "epoch [1/10] batch [1060/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.4497 (2.2578) lr 1.0000e-05 eta 0:11:05\n",
            "epoch [1/10] batch [1080/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.3389 (2.2494) lr 1.0000e-05 eta 0:11:03\n",
            "epoch [1/10] batch [1100/1632] time 0.042 (0.044) data 0.000 (0.001) loss 1.3311 (2.2480) lr 1.0000e-05 eta 0:11:02\n",
            "epoch [1/10] batch [1120/1632] time 0.045 (0.044) data 0.000 (0.001) loss 0.8169 (2.2541) lr 1.0000e-05 eta 0:11:01\n",
            "epoch [1/10] batch [1140/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.4189 (2.2457) lr 1.0000e-05 eta 0:11:00\n",
            "epoch [1/10] batch [1160/1632] time 0.040 (0.043) data 0.000 (0.001) loss 2.7852 (2.2297) lr 1.0000e-05 eta 0:10:58\n",
            "epoch [1/10] batch [1180/1632] time 0.041 (0.043) data 0.000 (0.001) loss 5.4219 (2.2423) lr 1.0000e-05 eta 0:10:57\n",
            "epoch [1/10] batch [1200/1632] time 0.042 (0.043) data 0.000 (0.001) loss 3.9863 (2.2299) lr 1.0000e-05 eta 0:10:56\n",
            "epoch [1/10] batch [1220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.4458 (2.2233) lr 1.0000e-05 eta 0:10:55\n",
            "epoch [1/10] batch [1240/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.5933 (2.2082) lr 1.0000e-05 eta 0:10:53\n",
            "epoch [1/10] batch [1260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 9.8281 (2.2144) lr 1.0000e-05 eta 0:10:52\n",
            "epoch [1/10] batch [1280/1632] time 0.039 (0.043) data 0.000 (0.001) loss 0.3120 (2.2302) lr 1.0000e-05 eta 0:10:50\n",
            "epoch [1/10] batch [1300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1816 (2.2121) lr 1.0000e-05 eta 0:10:49\n",
            "epoch [1/10] batch [1320/1632] time 0.041 (0.043) data 0.000 (0.001) loss 5.2969 (2.2086) lr 1.0000e-05 eta 0:10:47\n",
            "epoch [1/10] batch [1340/1632] time 0.044 (0.043) data 0.000 (0.001) loss 1.6533 (2.2021) lr 1.0000e-05 eta 0:10:46\n",
            "epoch [1/10] batch [1360/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.5220 (2.1920) lr 1.0000e-05 eta 0:10:45\n",
            "epoch [1/10] batch [1380/1632] time 0.040 (0.043) data 0.000 (0.001) loss 1.5859 (2.1908) lr 1.0000e-05 eta 0:10:43\n",
            "epoch [1/10] batch [1400/1632] time 0.040 (0.043) data 0.000 (0.001) loss 5.8242 (2.1860) lr 1.0000e-05 eta 0:10:42\n",
            "epoch [1/10] batch [1420/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.2307 (2.1830) lr 1.0000e-05 eta 0:10:41\n",
            "epoch [1/10] batch [1440/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.0561 (2.1749) lr 1.0000e-05 eta 0:10:40\n",
            "epoch [1/10] batch [1460/1632] time 0.045 (0.043) data 0.000 (0.001) loss 6.6445 (2.1723) lr 1.0000e-05 eta 0:10:39\n",
            "epoch [1/10] batch [1480/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.6304 (2.1634) lr 1.0000e-05 eta 0:10:37\n",
            "epoch [1/10] batch [1500/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.7964 (2.1536) lr 1.0000e-05 eta 0:10:36\n",
            "epoch [1/10] batch [1520/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0481 (2.1386) lr 1.0000e-05 eta 0:10:35\n",
            "epoch [1/10] batch [1540/1632] time 0.045 (0.043) data 0.001 (0.001) loss 0.1273 (2.1398) lr 1.0000e-05 eta 0:10:34\n",
            "epoch [1/10] batch [1560/1632] time 0.040 (0.043) data 0.000 (0.001) loss 1.1562 (2.1435) lr 1.0000e-05 eta 0:10:33\n",
            "epoch [1/10] batch [1580/1632] time 0.040 (0.043) data 0.000 (0.001) loss 2.6660 (2.1399) lr 1.0000e-05 eta 0:10:32\n",
            "epoch [1/10] batch [1600/1632] time 0.039 (0.043) data 0.000 (0.001) loss 9.0938 (2.1482) lr 1.0000e-05 eta 0:10:30\n",
            "epoch [1/10] batch [1620/1632] time 0.039 (0.043) data 0.000 (0.001) loss 0.0639 (2.1469) lr 1.0000e-05 eta 0:10:29\n",
            "epoch [2/10] batch [20/1632] time 0.042 (0.055) data 0.000 (0.012) loss 7.4375 (3.0269) lr 2.0000e-03 eta 0:13:25\n",
            "epoch [2/10] batch [40/1632] time 0.042 (0.048) data 0.000 (0.006) loss 0.4597 (2.8035) lr 2.0000e-03 eta 0:11:47\n",
            "epoch [2/10] batch [60/1632] time 0.043 (0.046) data 0.000 (0.004) loss 6.6523 (2.4259) lr 2.0000e-03 eta 0:11:15\n",
            "epoch [2/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.3518 (2.2096) lr 2.0000e-03 eta 0:10:56\n",
            "epoch [2/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 0.0124 (2.1082) lr 2.0000e-03 eta 0:10:46\n",
            "epoch [2/10] batch [120/1632] time 0.042 (0.044) data 0.000 (0.002) loss 1.5332 (2.1145) lr 2.0000e-03 eta 0:10:37\n",
            "epoch [2/10] batch [140/1632] time 0.041 (0.043) data 0.000 (0.002) loss 2.0840 (2.0498) lr 2.0000e-03 eta 0:10:31\n",
            "epoch [2/10] batch [160/1632] time 0.043 (0.043) data 0.000 (0.002) loss 1.4229 (1.9195) lr 2.0000e-03 eta 0:10:28\n",
            "epoch [2/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.4414 (1.8442) lr 2.0000e-03 eta 0:10:26\n",
            "epoch [2/10] batch [200/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.8813 (1.8013) lr 2.0000e-03 eta 0:10:23\n",
            "epoch [2/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6118 (1.7840) lr 2.0000e-03 eta 0:10:20\n",
            "epoch [2/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0442 (1.7602) lr 2.0000e-03 eta 0:10:19\n",
            "epoch [2/10] batch [260/1632] time 0.040 (0.043) data 0.000 (0.001) loss 2.5918 (1.7576) lr 2.0000e-03 eta 0:10:16\n",
            "epoch [2/10] batch [280/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.4136 (1.7397) lr 2.0000e-03 eta 0:10:13\n",
            "epoch [2/10] batch [300/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.6396 (1.6899) lr 2.0000e-03 eta 0:10:11\n",
            "epoch [2/10] batch [320/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0096 (1.6454) lr 2.0000e-03 eta 0:10:10\n",
            "epoch [2/10] batch [340/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.0918 (1.6473) lr 2.0000e-03 eta 0:10:07\n",
            "epoch [2/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6548 (1.6553) lr 2.0000e-03 eta 0:10:05\n",
            "epoch [2/10] batch [380/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0936 (1.6185) lr 2.0000e-03 eta 0:10:04\n",
            "epoch [2/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0134 (1.6102) lr 2.0000e-03 eta 0:10:02\n",
            "epoch [2/10] batch [420/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0598 (1.6399) lr 2.0000e-03 eta 0:10:00\n",
            "epoch [2/10] batch [440/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.8271 (1.6287) lr 2.0000e-03 eta 0:09:58\n",
            "epoch [2/10] batch [460/1632] time 0.040 (0.042) data 0.000 (0.001) loss 4.1719 (1.6194) lr 2.0000e-03 eta 0:09:57\n",
            "epoch [2/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6147 (1.6080) lr 2.0000e-03 eta 0:09:55\n",
            "epoch [2/10] batch [500/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0219 (1.6016) lr 2.0000e-03 eta 0:09:54\n",
            "epoch [2/10] batch [520/1632] time 0.042 (0.042) data 0.000 (0.001) loss 8.3906 (1.6127) lr 2.0000e-03 eta 0:09:53\n",
            "epoch [2/10] batch [540/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.4546 (1.6029) lr 2.0000e-03 eta 0:09:53\n",
            "epoch [2/10] batch [560/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.2499 (1.6050) lr 2.0000e-03 eta 0:09:52\n",
            "epoch [2/10] batch [580/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.9297 (1.5982) lr 2.0000e-03 eta 0:09:51\n",
            "epoch [2/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4758 (1.6001) lr 2.0000e-03 eta 0:09:50\n",
            "epoch [2/10] batch [620/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.4785 (1.6196) lr 2.0000e-03 eta 0:09:48\n",
            "epoch [2/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3499 (1.6213) lr 2.0000e-03 eta 0:09:47\n",
            "epoch [2/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.5762 (1.6201) lr 2.0000e-03 eta 0:09:46\n",
            "epoch [2/10] batch [680/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0329 (1.6171) lr 2.0000e-03 eta 0:09:45\n",
            "epoch [2/10] batch [700/1632] time 0.040 (0.042) data 0.000 (0.001) loss 5.4609 (1.6212) lr 2.0000e-03 eta 0:09:44\n",
            "epoch [2/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.6719 (1.6159) lr 2.0000e-03 eta 0:09:43\n",
            "epoch [2/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.4375 (1.6125) lr 2.0000e-03 eta 0:09:42\n",
            "epoch [2/10] batch [760/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0677 (1.5824) lr 2.0000e-03 eta 0:09:41\n",
            "epoch [2/10] batch [780/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.8242 (1.5711) lr 2.0000e-03 eta 0:09:40\n",
            "epoch [2/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 5.2031 (1.5666) lr 2.0000e-03 eta 0:09:39\n",
            "epoch [2/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5171 (1.5606) lr 2.0000e-03 eta 0:09:38\n",
            "epoch [2/10] batch [840/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0763 (1.5518) lr 2.0000e-03 eta 0:09:37\n",
            "epoch [2/10] batch [860/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.1942 (1.5540) lr 2.0000e-03 eta 0:09:37\n",
            "epoch [2/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1702 (1.5463) lr 2.0000e-03 eta 0:09:36\n",
            "epoch [2/10] batch [900/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.3589 (1.5494) lr 2.0000e-03 eta 0:09:36\n",
            "epoch [2/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5752 (1.5455) lr 2.0000e-03 eta 0:09:36\n",
            "epoch [2/10] batch [940/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5171 (1.5475) lr 2.0000e-03 eta 0:09:35\n",
            "epoch [2/10] batch [960/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1082 (1.5338) lr 2.0000e-03 eta 0:09:34\n",
            "epoch [2/10] batch [980/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.9624 (1.5260) lr 2.0000e-03 eta 0:09:33\n",
            "epoch [2/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.001) loss 9.1953 (1.5332) lr 2.0000e-03 eta 0:09:32\n",
            "epoch [2/10] batch [1020/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.2832 (1.5229) lr 2.0000e-03 eta 0:09:31\n",
            "epoch [2/10] batch [1040/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.7266 (1.5337) lr 2.0000e-03 eta 0:09:30\n",
            "epoch [2/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8359 (1.5274) lr 2.0000e-03 eta 0:09:29\n",
            "epoch [2/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.5840 (1.5171) lr 2.0000e-03 eta 0:09:28\n",
            "epoch [2/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.2852 (1.5129) lr 2.0000e-03 eta 0:09:27\n",
            "epoch [2/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2502 (1.5165) lr 2.0000e-03 eta 0:09:26\n",
            "epoch [2/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9023 (1.5227) lr 2.0000e-03 eta 0:09:25\n",
            "epoch [2/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1260 (1.5220) lr 2.0000e-03 eta 0:09:24\n",
            "epoch [2/10] batch [1180/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.5249 (1.5237) lr 2.0000e-03 eta 0:09:23\n",
            "epoch [2/10] batch [1200/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0715 (1.5198) lr 2.0000e-03 eta 0:09:23\n",
            "epoch [2/10] batch [1220/1632] time 0.044 (0.042) data 0.000 (0.000) loss 2.6211 (1.5111) lr 2.0000e-03 eta 0:09:23\n",
            "epoch [2/10] batch [1240/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.3379 (1.5045) lr 2.0000e-03 eta 0:09:22\n",
            "epoch [2/10] batch [1260/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0015 (1.5045) lr 2.0000e-03 eta 0:09:21\n",
            "epoch [2/10] batch [1280/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.4810 (1.4984) lr 2.0000e-03 eta 0:09:21\n",
            "epoch [2/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.4473 (1.5001) lr 2.0000e-03 eta 0:09:20\n",
            "epoch [2/10] batch [1320/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5024 (1.4947) lr 2.0000e-03 eta 0:09:19\n",
            "epoch [2/10] batch [1340/1632] time 0.053 (0.042) data 0.000 (0.000) loss 1.3936 (1.4922) lr 2.0000e-03 eta 0:09:18\n",
            "epoch [2/10] batch [1360/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.4531 (1.5063) lr 2.0000e-03 eta 0:09:18\n",
            "epoch [2/10] batch [1380/1632] time 0.040 (0.042) data 0.000 (0.000) loss 3.4160 (1.5008) lr 2.0000e-03 eta 0:09:16\n",
            "epoch [2/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.2197 (1.5018) lr 2.0000e-03 eta 0:09:15\n",
            "epoch [2/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.8867 (1.5020) lr 2.0000e-03 eta 0:09:14\n",
            "epoch [2/10] batch [1440/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0546 (1.4990) lr 2.0000e-03 eta 0:09:13\n",
            "epoch [2/10] batch [1460/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.3025 (1.4912) lr 2.0000e-03 eta 0:09:12\n",
            "epoch [2/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3992 (1.4908) lr 2.0000e-03 eta 0:09:11\n",
            "epoch [2/10] batch [1500/1632] time 0.046 (0.042) data 0.000 (0.000) loss 3.4355 (1.4863) lr 2.0000e-03 eta 0:09:10\n",
            "epoch [2/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1719 (1.4802) lr 2.0000e-03 eta 0:09:09\n",
            "epoch [2/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0197 (1.4874) lr 2.0000e-03 eta 0:09:09\n",
            "epoch [2/10] batch [1560/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0547 (1.4791) lr 2.0000e-03 eta 0:09:08\n",
            "epoch [2/10] batch [1580/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.5664 (1.4903) lr 2.0000e-03 eta 0:09:07\n",
            "epoch [2/10] batch [1600/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.6650 (1.4828) lr 2.0000e-03 eta 0:09:06\n",
            "epoch [2/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 4.5117 (1.4830) lr 2.0000e-03 eta 0:09:05\n",
            "epoch [3/10] batch [20/1632] time 0.041 (0.055) data 0.000 (0.012) loss 0.4875 (1.3917) lr 1.9511e-03 eta 0:11:54\n",
            "epoch [3/10] batch [40/1632] time 0.040 (0.048) data 0.000 (0.006) loss 4.4414 (1.3620) lr 1.9511e-03 eta 0:10:24\n",
            "epoch [3/10] batch [60/1632] time 0.043 (0.046) data 0.000 (0.004) loss 0.3044 (1.0454) lr 1.9511e-03 eta 0:09:57\n",
            "epoch [3/10] batch [80/1632] time 0.040 (0.045) data 0.000 (0.003) loss 3.0000 (1.0402) lr 1.9511e-03 eta 0:09:38\n",
            "epoch [3/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 1.0117 (1.0444) lr 1.9511e-03 eta 0:09:27\n",
            "epoch [3/10] batch [120/1632] time 0.041 (0.043) data 0.000 (0.002) loss 1.6377 (1.0424) lr 1.9511e-03 eta 0:09:20\n",
            "epoch [3/10] batch [140/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.5127 (1.2132) lr 1.9511e-03 eta 0:09:16\n",
            "epoch [3/10] batch [160/1632] time 0.040 (0.043) data 0.000 (0.002) loss 0.6055 (1.1771) lr 1.9511e-03 eta 0:09:13\n",
            "epoch [3/10] batch [180/1632] time 0.045 (0.043) data 0.000 (0.002) loss 3.2578 (1.2284) lr 1.9511e-03 eta 0:09:11\n",
            "epoch [3/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.7603 (1.2360) lr 1.9511e-03 eta 0:09:10\n",
            "epoch [3/10] batch [220/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2930 (1.1755) lr 1.9511e-03 eta 0:09:09\n",
            "epoch [3/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2450 (1.1947) lr 1.9511e-03 eta 0:09:07\n",
            "epoch [3/10] batch [260/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2406 (1.1978) lr 1.9511e-03 eta 0:09:05\n",
            "epoch [3/10] batch [280/1632] time 0.046 (0.043) data 0.000 (0.001) loss 0.1214 (1.1786) lr 1.9511e-03 eta 0:09:05\n",
            "epoch [3/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.8140 (1.1552) lr 1.9511e-03 eta 0:09:04\n",
            "epoch [3/10] batch [320/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.2120 (1.1786) lr 1.9511e-03 eta 0:09:03\n",
            "epoch [3/10] batch [340/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1573 (1.1504) lr 1.9511e-03 eta 0:09:01\n",
            "epoch [3/10] batch [360/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.5410 (1.1742) lr 1.9511e-03 eta 0:08:59\n",
            "epoch [3/10] batch [380/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1686 (1.1621) lr 1.9511e-03 eta 0:08:58\n",
            "epoch [3/10] batch [400/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3167 (1.1453) lr 1.9511e-03 eta 0:08:56\n",
            "epoch [3/10] batch [420/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4163 (1.1581) lr 1.9511e-03 eta 0:08:55\n",
            "epoch [3/10] batch [440/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0612 (1.1675) lr 1.9511e-03 eta 0:08:53\n",
            "epoch [3/10] batch [460/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.1152 (1.1821) lr 1.9511e-03 eta 0:08:53\n",
            "epoch [3/10] batch [480/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1703 (1.1701) lr 1.9511e-03 eta 0:08:52\n",
            "epoch [3/10] batch [500/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1069 (1.1849) lr 1.9511e-03 eta 0:08:51\n",
            "epoch [3/10] batch [520/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.9824 (1.1828) lr 1.9511e-03 eta 0:08:50\n",
            "epoch [3/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.0205 (1.1761) lr 1.9511e-03 eta 0:08:49\n",
            "epoch [3/10] batch [560/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.3538 (1.1900) lr 1.9511e-03 eta 0:08:47\n",
            "epoch [3/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0002 (1.1891) lr 1.9511e-03 eta 0:08:47\n",
            "epoch [3/10] batch [600/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.2754 (1.1821) lr 1.9511e-03 eta 0:08:46\n",
            "epoch [3/10] batch [620/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1001 (1.1718) lr 1.9511e-03 eta 0:08:45\n",
            "epoch [3/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 4.5742 (1.1862) lr 1.9511e-03 eta 0:08:44\n",
            "epoch [3/10] batch [660/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0553 (1.1831) lr 1.9511e-03 eta 0:08:42\n",
            "epoch [3/10] batch [680/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1646 (1.1826) lr 1.9511e-03 eta 0:08:41\n",
            "epoch [3/10] batch [700/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0723 (1.1912) lr 1.9511e-03 eta 0:08:40\n",
            "epoch [3/10] batch [720/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0550 (1.1861) lr 1.9511e-03 eta 0:08:38\n",
            "epoch [3/10] batch [740/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0070 (1.1740) lr 1.9511e-03 eta 0:08:38\n",
            "epoch [3/10] batch [760/1632] time 0.043 (0.042) data 0.000 (0.001) loss 6.8086 (1.1925) lr 1.9511e-03 eta 0:08:37\n",
            "epoch [3/10] batch [780/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.1338 (1.2031) lr 1.9511e-03 eta 0:08:37\n",
            "epoch [3/10] batch [800/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0894 (1.2030) lr 1.9511e-03 eta 0:08:36\n",
            "epoch [3/10] batch [820/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0781 (1.2159) lr 1.9511e-03 eta 0:08:35\n",
            "epoch [3/10] batch [840/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.4536 (1.2119) lr 1.9511e-03 eta 0:08:34\n",
            "epoch [3/10] batch [860/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0578 (1.2052) lr 1.9511e-03 eta 0:08:33\n",
            "epoch [3/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.9976 (1.1959) lr 1.9511e-03 eta 0:08:31\n",
            "epoch [3/10] batch [900/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0259 (1.1875) lr 1.9511e-03 eta 0:08:30\n",
            "epoch [3/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0137 (1.1895) lr 1.9511e-03 eta 0:08:29\n",
            "epoch [3/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0002 (1.1846) lr 1.9511e-03 eta 0:08:29\n",
            "epoch [3/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5527 (1.1768) lr 1.9511e-03 eta 0:08:28\n",
            "epoch [3/10] batch [980/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2429 (1.1726) lr 1.9511e-03 eta 0:08:27\n",
            "epoch [3/10] batch [1000/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.3945 (1.1774) lr 1.9511e-03 eta 0:08:25\n",
            "epoch [3/10] batch [1020/1632] time 0.041 (0.042) data 0.000 (0.000) loss 4.3828 (1.1860) lr 1.9511e-03 eta 0:08:24\n",
            "epoch [3/10] batch [1040/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.3379 (1.1895) lr 1.9511e-03 eta 0:08:23\n",
            "epoch [3/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.7739 (1.1942) lr 1.9511e-03 eta 0:08:22\n",
            "epoch [3/10] batch [1080/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.8281 (1.2032) lr 1.9511e-03 eta 0:08:21\n",
            "epoch [3/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.4756 (1.2030) lr 1.9511e-03 eta 0:08:20\n",
            "epoch [3/10] batch [1120/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.9253 (1.2036) lr 1.9511e-03 eta 0:08:18\n",
            "epoch [3/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8574 (1.1969) lr 1.9511e-03 eta 0:08:17\n",
            "epoch [3/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2810 (1.2007) lr 1.9511e-03 eta 0:08:16\n",
            "epoch [3/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.0723 (1.1980) lr 1.9511e-03 eta 0:08:15\n",
            "epoch [3/10] batch [1200/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0865 (1.2097) lr 1.9511e-03 eta 0:08:14\n",
            "epoch [3/10] batch [1220/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.4607 (1.2121) lr 1.9511e-03 eta 0:08:13\n",
            "epoch [3/10] batch [1240/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.1484 (1.2117) lr 1.9511e-03 eta 0:08:13\n",
            "epoch [3/10] batch [1260/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.8164 (1.2097) lr 1.9511e-03 eta 0:08:12\n",
            "epoch [3/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0925 (1.2085) lr 1.9511e-03 eta 0:08:11\n",
            "epoch [3/10] batch [1300/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2817 (1.2070) lr 1.9511e-03 eta 0:08:10\n",
            "epoch [3/10] batch [1320/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0814 (1.2143) lr 1.9511e-03 eta 0:08:09\n",
            "epoch [3/10] batch [1340/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3186 (1.2145) lr 1.9511e-03 eta 0:08:09\n",
            "epoch [3/10] batch [1360/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.0352 (1.2200) lr 1.9511e-03 eta 0:08:08\n",
            "epoch [3/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.9648 (1.2231) lr 1.9511e-03 eta 0:08:07\n",
            "epoch [3/10] batch [1400/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2598 (1.2154) lr 1.9511e-03 eta 0:08:06\n",
            "epoch [3/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 6.1641 (1.2146) lr 1.9511e-03 eta 0:08:05\n",
            "epoch [3/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4180 (1.2154) lr 1.9511e-03 eta 0:08:04\n",
            "epoch [3/10] batch [1460/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.3027 (1.2150) lr 1.9511e-03 eta 0:08:03\n",
            "epoch [3/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9868 (1.2168) lr 1.9511e-03 eta 0:08:02\n",
            "epoch [3/10] batch [1500/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1170 (1.2107) lr 1.9511e-03 eta 0:08:01\n",
            "epoch [3/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.4668 (1.2182) lr 1.9511e-03 eta 0:08:01\n",
            "epoch [3/10] batch [1540/1632] time 0.045 (0.042) data 0.000 (0.000) loss 1.9346 (1.2137) lr 1.9511e-03 eta 0:08:00\n",
            "epoch [3/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6753 (1.2126) lr 1.9511e-03 eta 0:07:59\n",
            "epoch [3/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8320 (1.2105) lr 1.9511e-03 eta 0:07:59\n",
            "epoch [3/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.3953 (1.2063) lr 1.9511e-03 eta 0:07:58\n",
            "epoch [3/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.1344 (1.2120) lr 1.9511e-03 eta 0:07:57\n",
            "epoch [4/10] batch [20/1632] time 0.041 (0.056) data 0.000 (0.013) loss 0.9160 (0.9590) lr 1.8090e-03 eta 0:10:33\n",
            "epoch [4/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.007) loss 0.8872 (1.1622) lr 1.8090e-03 eta 0:09:11\n",
            "epoch [4/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.005) loss 1.5762 (1.1274) lr 1.8090e-03 eta 0:08:43\n",
            "epoch [4/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 1.1914 (1.1814) lr 1.8090e-03 eta 0:08:28\n",
            "epoch [4/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 0.5864 (1.0699) lr 1.8090e-03 eta 0:08:19\n",
            "epoch [4/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0026 (1.0589) lr 1.8090e-03 eta 0:08:12\n",
            "epoch [4/10] batch [140/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.3203 (1.0876) lr 1.8090e-03 eta 0:08:08\n",
            "epoch [4/10] batch [160/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.2289 (1.0725) lr 1.8090e-03 eta 0:08:04\n",
            "epoch [4/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.0744 (1.0130) lr 1.8090e-03 eta 0:08:01\n",
            "epoch [4/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.002) loss 4.1289 (1.0572) lr 1.8090e-03 eta 0:07:58\n",
            "epoch [4/10] batch [220/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.0234 (1.0290) lr 1.8090e-03 eta 0:07:56\n",
            "epoch [4/10] batch [240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1555 (1.0401) lr 1.8090e-03 eta 0:07:55\n",
            "epoch [4/10] batch [260/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0742 (1.0230) lr 1.8090e-03 eta 0:07:54\n",
            "epoch [4/10] batch [280/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.2681 (1.0547) lr 1.8090e-03 eta 0:07:53\n",
            "epoch [4/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.8013 (1.0850) lr 1.8090e-03 eta 0:07:52\n",
            "epoch [4/10] batch [320/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.5449 (1.0840) lr 1.8090e-03 eta 0:07:52\n",
            "epoch [4/10] batch [340/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0329 (1.0896) lr 1.8090e-03 eta 0:07:51\n",
            "epoch [4/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1505 (1.0815) lr 1.8090e-03 eta 0:07:50\n",
            "epoch [4/10] batch [380/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1170 (1.0746) lr 1.8090e-03 eta 0:07:48\n",
            "epoch [4/10] batch [400/1632] time 0.046 (0.042) data 0.001 (0.001) loss 0.0508 (1.1198) lr 1.8090e-03 eta 0:07:48\n",
            "epoch [4/10] batch [420/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6880 (1.1116) lr 1.8090e-03 eta 0:07:48\n",
            "epoch [4/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0148 (1.0985) lr 1.8090e-03 eta 0:07:46\n",
            "epoch [4/10] batch [460/1632] time 0.042 (0.042) data 0.000 (0.001) loss 3.5234 (1.0973) lr 1.8090e-03 eta 0:07:45\n",
            "epoch [4/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1519 (1.0771) lr 1.8090e-03 eta 0:07:44\n",
            "epoch [4/10] batch [500/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0811 (1.0974) lr 1.8090e-03 eta 0:07:43\n",
            "epoch [4/10] batch [520/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1532 (1.0989) lr 1.8090e-03 eta 0:07:42\n",
            "epoch [4/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0028 (1.0785) lr 1.8090e-03 eta 0:07:41\n",
            "epoch [4/10] batch [560/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8540 (1.0659) lr 1.8090e-03 eta 0:07:39\n",
            "epoch [4/10] batch [580/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.3044 (1.0710) lr 1.8090e-03 eta 0:07:38\n",
            "epoch [4/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1106 (1.0515) lr 1.8090e-03 eta 0:07:38\n",
            "epoch [4/10] batch [620/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.0000 (1.0403) lr 1.8090e-03 eta 0:07:37\n",
            "epoch [4/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0283 (1.0516) lr 1.8090e-03 eta 0:07:36\n",
            "epoch [4/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.5127 (1.0511) lr 1.8090e-03 eta 0:07:36\n",
            "epoch [4/10] batch [680/1632] time 0.042 (0.042) data 0.000 (0.001) loss 5.7031 (1.0786) lr 1.8090e-03 eta 0:07:35\n",
            "epoch [4/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.7021 (1.0733) lr 1.8090e-03 eta 0:07:34\n",
            "epoch [4/10] batch [720/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.7324 (1.0617) lr 1.8090e-03 eta 0:07:33\n",
            "epoch [4/10] batch [740/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.5742 (1.0567) lr 1.8090e-03 eta 0:07:32\n",
            "epoch [4/10] batch [760/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.4863 (1.0628) lr 1.8090e-03 eta 0:07:30\n",
            "epoch [4/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.0508 (1.0682) lr 1.8090e-03 eta 0:07:29\n",
            "epoch [4/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 5.2852 (1.0708) lr 1.8090e-03 eta 0:07:28\n",
            "epoch [4/10] batch [820/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.7754 (1.0730) lr 1.8090e-03 eta 0:07:27\n",
            "epoch [4/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1731 (1.0741) lr 1.8090e-03 eta 0:07:26\n",
            "epoch [4/10] batch [860/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.4421 (1.0739) lr 1.8090e-03 eta 0:07:25\n",
            "epoch [4/10] batch [880/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5649 (1.0615) lr 1.8090e-03 eta 0:07:24\n",
            "epoch [4/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2427 (1.0603) lr 1.8090e-03 eta 0:07:23\n",
            "epoch [4/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0342 (1.0510) lr 1.8090e-03 eta 0:07:22\n",
            "epoch [4/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0444 (1.0511) lr 1.8090e-03 eta 0:07:21\n",
            "epoch [4/10] batch [960/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0364 (1.0512) lr 1.8090e-03 eta 0:07:20\n",
            "epoch [4/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0885 (1.0572) lr 1.8090e-03 eta 0:07:19\n",
            "epoch [4/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.6211 (1.0665) lr 1.8090e-03 eta 0:07:18\n",
            "epoch [4/10] batch [1020/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.0829 (1.0653) lr 1.8090e-03 eta 0:07:17\n",
            "epoch [4/10] batch [1040/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3875 (1.0649) lr 1.8090e-03 eta 0:07:16\n",
            "epoch [4/10] batch [1060/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.6328 (1.0533) lr 1.8090e-03 eta 0:07:15\n",
            "epoch [4/10] batch [1080/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0533 (1.0462) lr 1.8090e-03 eta 0:07:14\n",
            "epoch [4/10] batch [1100/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0046 (1.0489) lr 1.8090e-03 eta 0:07:13\n",
            "epoch [4/10] batch [1120/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.4058 (1.0500) lr 1.8090e-03 eta 0:07:12\n",
            "epoch [4/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6152 (1.0432) lr 1.8090e-03 eta 0:07:11\n",
            "epoch [4/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.5010 (1.0358) lr 1.8090e-03 eta 0:07:10\n",
            "epoch [4/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8086 (1.0422) lr 1.8090e-03 eta 0:07:09\n",
            "epoch [4/10] batch [1200/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0427 (1.0562) lr 1.8090e-03 eta 0:07:08\n",
            "epoch [4/10] batch [1220/1632] time 0.042 (0.042) data 0.000 (0.000) loss 3.6523 (1.0602) lr 1.8090e-03 eta 0:07:07\n",
            "epoch [4/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2218 (1.0575) lr 1.8090e-03 eta 0:07:06\n",
            "epoch [4/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9736 (1.0550) lr 1.8090e-03 eta 0:07:05\n",
            "epoch [4/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0223 (1.0507) lr 1.8090e-03 eta 0:07:05\n",
            "epoch [4/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.4531 (1.0585) lr 1.8090e-03 eta 0:07:04\n",
            "epoch [4/10] batch [1320/1632] time 0.040 (0.042) data 0.000 (0.000) loss 3.3965 (1.0538) lr 1.8090e-03 eta 0:07:03\n",
            "epoch [4/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0084 (1.0598) lr 1.8090e-03 eta 0:07:02\n",
            "epoch [4/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6489 (1.0615) lr 1.8090e-03 eta 0:07:01\n",
            "epoch [4/10] batch [1380/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.3477 (1.0622) lr 1.8090e-03 eta 0:07:00\n",
            "epoch [4/10] batch [1400/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0076 (1.0569) lr 1.8090e-03 eta 0:06:59\n",
            "epoch [4/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1428 (1.0562) lr 1.8090e-03 eta 0:06:58\n",
            "epoch [4/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5337 (1.0522) lr 1.8090e-03 eta 0:06:57\n",
            "epoch [4/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8560 (1.0583) lr 1.8090e-03 eta 0:06:56\n",
            "epoch [4/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.4160 (1.0572) lr 1.8090e-03 eta 0:06:55\n",
            "epoch [4/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0982 (1.0567) lr 1.8090e-03 eta 0:06:54\n",
            "epoch [4/10] batch [1520/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0609 (1.0575) lr 1.8090e-03 eta 0:06:53\n",
            "epoch [4/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1581 (1.0612) lr 1.8090e-03 eta 0:06:52\n",
            "epoch [4/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.9795 (1.0537) lr 1.8090e-03 eta 0:06:51\n",
            "epoch [4/10] batch [1580/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.3767 (1.0521) lr 1.8090e-03 eta 0:06:50\n",
            "epoch [4/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.6143 (1.0528) lr 1.8090e-03 eta 0:06:49\n",
            "epoch [4/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.0054 (1.0481) lr 1.8090e-03 eta 0:06:48\n",
            "epoch [5/10] batch [20/1632] time 0.042 (0.056) data 0.000 (0.013) loss 1.4727 (1.0379) lr 1.5878e-03 eta 0:09:09\n",
            "epoch [5/10] batch [40/1632] time 0.042 (0.049) data 0.000 (0.007) loss 4.1914 (1.3852) lr 1.5878e-03 eta 0:07:57\n",
            "epoch [5/10] batch [60/1632] time 0.041 (0.047) data 0.000 (0.005) loss 0.1545 (1.3119) lr 1.5878e-03 eta 0:07:35\n",
            "epoch [5/10] batch [80/1632] time 0.042 (0.046) data 0.000 (0.003) loss 0.3154 (1.3585) lr 1.5878e-03 eta 0:07:24\n",
            "epoch [5/10] batch [100/1632] time 0.042 (0.045) data 0.000 (0.003) loss 0.4609 (1.2582) lr 1.5878e-03 eta 0:07:16\n",
            "epoch [5/10] batch [120/1632] time 0.042 (0.045) data 0.000 (0.002) loss 0.8999 (1.1512) lr 1.5878e-03 eta 0:07:10\n",
            "epoch [5/10] batch [140/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.2223 (1.1095) lr 1.5878e-03 eta 0:07:05\n",
            "epoch [5/10] batch [160/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.0750 (1.1317) lr 1.5878e-03 eta 0:07:01\n",
            "epoch [5/10] batch [180/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.1829 (1.1196) lr 1.5878e-03 eta 0:06:58\n",
            "epoch [5/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.2708 (1.0771) lr 1.5878e-03 eta 0:06:56\n",
            "epoch [5/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0754 (1.0580) lr 1.5878e-03 eta 0:06:54\n",
            "epoch [5/10] batch [240/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.5063 (1.0488) lr 1.5878e-03 eta 0:06:54\n",
            "epoch [5/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.7754 (1.0477) lr 1.5878e-03 eta 0:06:53\n",
            "epoch [5/10] batch [280/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.6641 (1.0632) lr 1.5878e-03 eta 0:06:51\n",
            "epoch [5/10] batch [300/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1948 (1.0273) lr 1.5878e-03 eta 0:06:50\n",
            "epoch [5/10] batch [320/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.6333 (1.0154) lr 1.5878e-03 eta 0:06:49\n",
            "epoch [5/10] batch [340/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2937 (1.0057) lr 1.5878e-03 eta 0:06:48\n",
            "epoch [5/10] batch [360/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.6543 (0.9993) lr 1.5878e-03 eta 0:06:47\n",
            "epoch [5/10] batch [380/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1742 (1.0040) lr 1.5878e-03 eta 0:06:46\n",
            "epoch [5/10] batch [400/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0153 (0.9948) lr 1.5878e-03 eta 0:06:45\n",
            "epoch [5/10] batch [420/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.0449 (0.9748) lr 1.5878e-03 eta 0:06:44\n",
            "epoch [5/10] batch [440/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0889 (1.0068) lr 1.5878e-03 eta 0:06:42\n",
            "epoch [5/10] batch [460/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0654 (1.0024) lr 1.5878e-03 eta 0:06:41\n",
            "epoch [5/10] batch [480/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.4158 (0.9963) lr 1.5878e-03 eta 0:06:39\n",
            "epoch [5/10] batch [500/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1964 (0.9910) lr 1.5878e-03 eta 0:06:38\n",
            "epoch [5/10] batch [520/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.6865 (0.9777) lr 1.5878e-03 eta 0:06:37\n",
            "epoch [5/10] batch [540/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0003 (0.9744) lr 1.5878e-03 eta 0:06:36\n",
            "epoch [5/10] batch [560/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.8589 (0.9634) lr 1.5878e-03 eta 0:06:35\n",
            "epoch [5/10] batch [580/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.7188 (0.9828) lr 1.5878e-03 eta 0:06:33\n",
            "epoch [5/10] batch [600/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.5225 (0.9912) lr 1.5878e-03 eta 0:06:32\n",
            "epoch [5/10] batch [620/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.1571 (0.9958) lr 1.5878e-03 eta 0:06:31\n",
            "epoch [5/10] batch [640/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0595 (1.0084) lr 1.5878e-03 eta 0:06:29\n",
            "epoch [5/10] batch [660/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.8652 (1.0022) lr 1.5878e-03 eta 0:06:28\n",
            "epoch [5/10] batch [680/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.1086 (0.9996) lr 1.5878e-03 eta 0:06:27\n",
            "epoch [5/10] batch [700/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0814 (0.9878) lr 1.5878e-03 eta 0:06:26\n",
            "epoch [5/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0168 (0.9886) lr 1.5878e-03 eta 0:06:25\n",
            "epoch [5/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.1592 (0.9877) lr 1.5878e-03 eta 0:06:24\n",
            "epoch [5/10] batch [760/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0941 (0.9885) lr 1.5878e-03 eta 0:06:23\n",
            "epoch [5/10] batch [780/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.6128 (1.0074) lr 1.5878e-03 eta 0:06:21\n",
            "epoch [5/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.1016 (1.0098) lr 1.5878e-03 eta 0:06:20\n",
            "epoch [5/10] batch [820/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.4956 (1.0154) lr 1.5878e-03 eta 0:06:19\n",
            "epoch [5/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3225 (1.0156) lr 1.5878e-03 eta 0:06:18\n",
            "epoch [5/10] batch [860/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.8594 (1.0143) lr 1.5878e-03 eta 0:06:17\n",
            "epoch [5/10] batch [880/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.7603 (1.0047) lr 1.5878e-03 eta 0:06:15\n",
            "epoch [5/10] batch [900/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.1466 (0.9893) lr 1.5878e-03 eta 0:06:14\n",
            "epoch [5/10] batch [920/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0352 (0.9838) lr 1.5878e-03 eta 0:06:13\n",
            "epoch [5/10] batch [940/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.2949 (0.9785) lr 1.5878e-03 eta 0:06:12\n",
            "epoch [5/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1748 (0.9888) lr 1.5878e-03 eta 0:06:11\n",
            "epoch [5/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.9922 (0.9937) lr 1.5878e-03 eta 0:06:10\n",
            "epoch [5/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5884 (0.9989) lr 1.5878e-03 eta 0:06:10\n",
            "epoch [5/10] batch [1020/1632] time 0.046 (0.042) data 0.000 (0.001) loss 0.0099 (0.9937) lr 1.5878e-03 eta 0:06:09\n",
            "epoch [5/10] batch [1040/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.4019 (0.9873) lr 1.5878e-03 eta 0:06:08\n",
            "epoch [5/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.2578 (0.9961) lr 1.5878e-03 eta 0:06:07\n",
            "epoch [5/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0072 (1.0102) lr 1.5878e-03 eta 0:06:07\n",
            "epoch [5/10] batch [1100/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0096 (1.0110) lr 1.5878e-03 eta 0:06:06\n",
            "epoch [5/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8022 (1.0141) lr 1.5878e-03 eta 0:06:05\n",
            "epoch [5/10] batch [1140/1632] time 0.040 (0.042) data 0.000 (0.000) loss 3.2012 (1.0163) lr 1.5878e-03 eta 0:06:04\n",
            "epoch [5/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0015 (1.0118) lr 1.5878e-03 eta 0:06:03\n",
            "epoch [5/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3625 (1.0071) lr 1.5878e-03 eta 0:06:02\n",
            "epoch [5/10] batch [1200/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.2455 (1.0023) lr 1.5878e-03 eta 0:06:01\n",
            "epoch [5/10] batch [1220/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0501 (1.0013) lr 1.5878e-03 eta 0:06:00\n",
            "epoch [5/10] batch [1240/1632] time 0.040 (0.042) data 0.000 (0.000) loss 3.9902 (1.0140) lr 1.5878e-03 eta 0:05:59\n",
            "epoch [5/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0160 (1.0149) lr 1.5878e-03 eta 0:05:58\n",
            "epoch [5/10] batch [1280/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4055 (1.0055) lr 1.5878e-03 eta 0:05:57\n",
            "epoch [5/10] batch [1300/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0361 (1.0102) lr 1.5878e-03 eta 0:05:56\n",
            "epoch [5/10] batch [1320/1632] time 0.043 (0.042) data 0.000 (0.000) loss 2.2012 (1.0020) lr 1.5878e-03 eta 0:05:55\n",
            "epoch [5/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0012 (0.9981) lr 1.5878e-03 eta 0:05:54\n",
            "epoch [5/10] batch [1360/1632] time 0.042 (0.042) data 0.000 (0.000) loss 4.0391 (1.0013) lr 1.5878e-03 eta 0:05:53\n",
            "epoch [5/10] batch [1380/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.0996 (1.0056) lr 1.5878e-03 eta 0:05:52\n",
            "epoch [5/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1667 (0.9970) lr 1.5878e-03 eta 0:05:51\n",
            "epoch [5/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0204 (0.9933) lr 1.5878e-03 eta 0:05:50\n",
            "epoch [5/10] batch [1440/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1381 (0.9931) lr 1.5878e-03 eta 0:05:49\n",
            "epoch [5/10] batch [1460/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.5200 (0.9860) lr 1.5878e-03 eta 0:05:48\n",
            "epoch [5/10] batch [1480/1632] time 0.040 (0.042) data 0.000 (0.000) loss 3.1094 (0.9896) lr 1.5878e-03 eta 0:05:48\n",
            "epoch [5/10] batch [1500/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0765 (0.9850) lr 1.5878e-03 eta 0:05:47\n",
            "epoch [5/10] batch [1520/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1385 (0.9838) lr 1.5878e-03 eta 0:05:46\n",
            "epoch [5/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0565 (0.9836) lr 1.5878e-03 eta 0:05:45\n",
            "epoch [5/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.7656 (0.9772) lr 1.5878e-03 eta 0:05:44\n",
            "epoch [5/10] batch [1580/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1992 (0.9782) lr 1.5878e-03 eta 0:05:43\n",
            "epoch [5/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4172 (0.9766) lr 1.5878e-03 eta 0:05:42\n",
            "epoch [5/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 2.0098 (0.9729) lr 1.5878e-03 eta 0:05:41\n",
            "epoch [6/10] batch [20/1632] time 0.040 (0.055) data 0.000 (0.013) loss 0.5405 (0.5094) lr 1.3090e-03 eta 0:07:24\n",
            "epoch [6/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.006) loss 0.5127 (0.6331) lr 1.3090e-03 eta 0:06:31\n",
            "epoch [6/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.6167 (0.7075) lr 1.3090e-03 eta 0:06:15\n",
            "epoch [6/10] batch [80/1632] time 0.042 (0.046) data 0.000 (0.003) loss 1.5293 (0.7482) lr 1.3090e-03 eta 0:06:14\n",
            "epoch [6/10] batch [100/1632] time 0.043 (0.046) data 0.000 (0.003) loss 0.2932 (0.7914) lr 1.3090e-03 eta 0:06:07\n",
            "epoch [6/10] batch [120/1632] time 0.041 (0.045) data 0.000 (0.002) loss 0.1304 (0.8938) lr 1.3090e-03 eta 0:06:01\n",
            "epoch [6/10] batch [140/1632] time 0.040 (0.044) data 0.000 (0.002) loss 1.1699 (0.9494) lr 1.3090e-03 eta 0:05:56\n",
            "epoch [6/10] batch [160/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0662 (0.9297) lr 1.3090e-03 eta 0:05:53\n",
            "epoch [6/10] batch [180/1632] time 0.040 (0.044) data 0.000 (0.002) loss 2.6621 (0.9036) lr 1.3090e-03 eta 0:05:50\n",
            "epoch [6/10] batch [200/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.1711 (0.8807) lr 1.3090e-03 eta 0:05:47\n",
            "epoch [6/10] batch [220/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.1097 (0.8523) lr 1.3090e-03 eta 0:05:45\n",
            "epoch [6/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0482 (0.8605) lr 1.3090e-03 eta 0:05:43\n",
            "epoch [6/10] batch [260/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.5645 (0.8484) lr 1.3090e-03 eta 0:05:40\n",
            "epoch [6/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6367 (0.8437) lr 1.3090e-03 eta 0:05:39\n",
            "epoch [6/10] batch [300/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0193 (0.8569) lr 1.3090e-03 eta 0:05:37\n",
            "epoch [6/10] batch [320/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0421 (0.8569) lr 1.3090e-03 eta 0:05:36\n",
            "epoch [6/10] batch [340/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6104 (0.8416) lr 1.3090e-03 eta 0:05:34\n",
            "epoch [6/10] batch [360/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.1466 (0.8195) lr 1.3090e-03 eta 0:05:32\n",
            "epoch [6/10] batch [380/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0338 (0.8227) lr 1.3090e-03 eta 0:05:31\n",
            "epoch [6/10] batch [400/1632] time 0.040 (0.042) data 0.000 (0.001) loss 3.6016 (0.8371) lr 1.3090e-03 eta 0:05:29\n",
            "epoch [6/10] batch [420/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.2886 (0.8707) lr 1.3090e-03 eta 0:05:28\n",
            "epoch [6/10] batch [440/1632] time 0.049 (0.043) data 0.000 (0.001) loss 0.5635 (0.9029) lr 1.3090e-03 eta 0:05:28\n",
            "epoch [6/10] batch [460/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.4045 (0.8920) lr 1.3090e-03 eta 0:05:27\n",
            "epoch [6/10] batch [480/1632] time 0.040 (0.043) data 0.000 (0.001) loss 3.8867 (0.9091) lr 1.3090e-03 eta 0:05:26\n",
            "epoch [6/10] batch [500/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.3052 (0.9122) lr 1.3090e-03 eta 0:05:25\n",
            "epoch [6/10] batch [520/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.2666 (0.9015) lr 1.3090e-03 eta 0:05:24\n",
            "epoch [6/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.7109 (0.9428) lr 1.3090e-03 eta 0:05:23\n",
            "epoch [6/10] batch [560/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.8511 (0.9276) lr 1.3090e-03 eta 0:05:22\n",
            "epoch [6/10] batch [580/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0840 (0.9425) lr 1.3090e-03 eta 0:05:21\n",
            "epoch [6/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2166 (0.9475) lr 1.3090e-03 eta 0:05:20\n",
            "epoch [6/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2208 (0.9450) lr 1.3090e-03 eta 0:05:18\n",
            "epoch [6/10] batch [640/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.5488 (0.9437) lr 1.3090e-03 eta 0:05:17\n",
            "epoch [6/10] batch [660/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3293 (0.9560) lr 1.3090e-03 eta 0:05:16\n",
            "epoch [6/10] batch [680/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5366 (0.9624) lr 1.3090e-03 eta 0:05:15\n",
            "epoch [6/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3418 (0.9447) lr 1.3090e-03 eta 0:05:14\n",
            "epoch [6/10] batch [720/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.9976 (0.9437) lr 1.3090e-03 eta 0:05:13\n",
            "epoch [6/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0037 (0.9348) lr 1.3090e-03 eta 0:05:12\n",
            "epoch [6/10] batch [760/1632] time 0.045 (0.042) data 0.000 (0.001) loss 2.0059 (0.9367) lr 1.3090e-03 eta 0:05:11\n",
            "epoch [6/10] batch [780/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.3425 (0.9234) lr 1.3090e-03 eta 0:05:11\n",
            "epoch [6/10] batch [800/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.5386 (0.9190) lr 1.3090e-03 eta 0:05:10\n",
            "epoch [6/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5239 (0.9307) lr 1.3090e-03 eta 0:05:09\n",
            "epoch [6/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0167 (0.9274) lr 1.3090e-03 eta 0:05:08\n",
            "epoch [6/10] batch [860/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.2467 (0.9299) lr 1.3090e-03 eta 0:05:07\n",
            "epoch [6/10] batch [880/1632] time 0.045 (0.042) data 0.000 (0.001) loss 1.6484 (0.9230) lr 1.3090e-03 eta 0:05:06\n",
            "epoch [6/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2334 (0.9179) lr 1.3090e-03 eta 0:05:06\n",
            "epoch [6/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.5693 (0.9169) lr 1.3090e-03 eta 0:05:05\n",
            "epoch [6/10] batch [940/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.9209 (0.9063) lr 1.3090e-03 eta 0:05:04\n",
            "epoch [6/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0078 (0.8947) lr 1.3090e-03 eta 0:05:03\n",
            "epoch [6/10] batch [980/1632] time 0.043 (0.042) data 0.001 (0.001) loss 1.1104 (0.8927) lr 1.3090e-03 eta 0:05:02\n",
            "epoch [6/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1473 (0.8878) lr 1.3090e-03 eta 0:05:01\n",
            "epoch [6/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0141 (0.8883) lr 1.3090e-03 eta 0:05:00\n",
            "epoch [6/10] batch [1040/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.8457 (0.8865) lr 1.3090e-03 eta 0:05:00\n",
            "epoch [6/10] batch [1060/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.1318 (0.8865) lr 1.3090e-03 eta 0:04:59\n",
            "epoch [6/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0574 (0.8874) lr 1.3090e-03 eta 0:04:58\n",
            "epoch [6/10] batch [1100/1632] time 0.042 (0.042) data 0.000 (0.000) loss 3.4629 (0.8842) lr 1.3090e-03 eta 0:04:57\n",
            "epoch [6/10] batch [1120/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.6182 (0.8842) lr 1.3090e-03 eta 0:04:56\n",
            "epoch [6/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.1543 (0.8890) lr 1.3090e-03 eta 0:04:56\n",
            "epoch [6/10] batch [1160/1632] time 0.047 (0.042) data 0.000 (0.000) loss 1.2861 (0.8872) lr 1.3090e-03 eta 0:04:55\n",
            "epoch [6/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0508 (0.8865) lr 1.3090e-03 eta 0:04:54\n",
            "epoch [6/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3435 (0.8807) lr 1.3090e-03 eta 0:04:53\n",
            "epoch [6/10] batch [1220/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.6792 (0.8814) lr 1.3090e-03 eta 0:04:52\n",
            "epoch [6/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.2363 (0.8884) lr 1.3090e-03 eta 0:04:51\n",
            "epoch [6/10] batch [1260/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.9365 (0.8889) lr 1.3090e-03 eta 0:04:50\n",
            "epoch [6/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2499 (0.8912) lr 1.3090e-03 eta 0:04:50\n",
            "epoch [6/10] batch [1300/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1832 (0.8941) lr 1.3090e-03 eta 0:04:49\n",
            "epoch [6/10] batch [1320/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0263 (0.8955) lr 1.3090e-03 eta 0:04:48\n",
            "epoch [6/10] batch [1340/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0059 (0.8925) lr 1.3090e-03 eta 0:04:47\n",
            "epoch [6/10] batch [1360/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.5332 (0.8929) lr 1.3090e-03 eta 0:04:46\n",
            "epoch [6/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1469 (0.8831) lr 1.3090e-03 eta 0:04:45\n",
            "epoch [6/10] batch [1400/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.0059 (0.8816) lr 1.3090e-03 eta 0:04:44\n",
            "epoch [6/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0570 (0.8779) lr 1.3090e-03 eta 0:04:43\n",
            "epoch [6/10] batch [1440/1632] time 0.049 (0.042) data 0.000 (0.000) loss 0.1116 (0.8770) lr 1.3090e-03 eta 0:04:42\n",
            "epoch [6/10] batch [1460/1632] time 0.050 (0.042) data 0.000 (0.000) loss 0.3770 (0.8749) lr 1.3090e-03 eta 0:04:42\n",
            "epoch [6/10] batch [1480/1632] time 0.048 (0.042) data 0.000 (0.000) loss 0.1875 (0.8709) lr 1.3090e-03 eta 0:04:41\n",
            "epoch [6/10] batch [1500/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.3682 (0.8719) lr 1.3090e-03 eta 0:04:40\n",
            "epoch [6/10] batch [1520/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.9072 (0.8762) lr 1.3090e-03 eta 0:04:39\n",
            "epoch [6/10] batch [1540/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0453 (0.8750) lr 1.3090e-03 eta 0:04:38\n",
            "epoch [6/10] batch [1560/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0361 (0.8761) lr 1.3090e-03 eta 0:04:37\n",
            "epoch [6/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3337 (0.8699) lr 1.3090e-03 eta 0:04:37\n",
            "epoch [6/10] batch [1600/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.2615 (0.8706) lr 1.3090e-03 eta 0:04:36\n",
            "epoch [6/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 3.3926 (0.8747) lr 1.3090e-03 eta 0:04:35\n",
            "epoch [7/10] batch [20/1632] time 0.041 (0.055) data 0.000 (0.012) loss 0.0022 (0.3426) lr 1.0000e-03 eta 0:05:57\n",
            "epoch [7/10] batch [40/1632] time 0.040 (0.048) data 0.000 (0.006) loss 0.7271 (0.3285) lr 1.0000e-03 eta 0:05:11\n",
            "epoch [7/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.0202 (0.5150) lr 1.0000e-03 eta 0:04:55\n",
            "epoch [7/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.4619 (0.5975) lr 1.0000e-03 eta 0:04:47\n",
            "epoch [7/10] batch [100/1632] time 0.042 (0.044) data 0.000 (0.003) loss 0.0051 (0.5911) lr 1.0000e-03 eta 0:04:43\n",
            "epoch [7/10] batch [120/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.0663 (0.6580) lr 1.0000e-03 eta 0:04:40\n",
            "epoch [7/10] batch [140/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.3701 (0.6837) lr 1.0000e-03 eta 0:04:38\n",
            "epoch [7/10] batch [160/1632] time 0.044 (0.044) data 0.000 (0.002) loss 0.1016 (0.7069) lr 1.0000e-03 eta 0:04:37\n",
            "epoch [7/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.3984 (0.7261) lr 1.0000e-03 eta 0:04:35\n",
            "epoch [7/10] batch [200/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.1016 (0.6927) lr 1.0000e-03 eta 0:04:33\n",
            "epoch [7/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0536 (0.7313) lr 1.0000e-03 eta 0:04:31\n",
            "epoch [7/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.5918 (0.7685) lr 1.0000e-03 eta 0:04:29\n",
            "epoch [7/10] batch [260/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.3589 (0.7735) lr 1.0000e-03 eta 0:04:28\n",
            "epoch [7/10] batch [280/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.5786 (0.7590) lr 1.0000e-03 eta 0:04:26\n",
            "epoch [7/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.6919 (0.7483) lr 1.0000e-03 eta 0:04:25\n",
            "epoch [7/10] batch [320/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.9722 (0.7470) lr 1.0000e-03 eta 0:04:23\n",
            "epoch [7/10] batch [340/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0955 (0.7429) lr 1.0000e-03 eta 0:04:22\n",
            "epoch [7/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1390 (0.7444) lr 1.0000e-03 eta 0:04:21\n",
            "epoch [7/10] batch [380/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.5859 (0.7730) lr 1.0000e-03 eta 0:04:20\n",
            "epoch [7/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3813 (0.7724) lr 1.0000e-03 eta 0:04:19\n",
            "epoch [7/10] batch [420/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5562 (0.7518) lr 1.0000e-03 eta 0:04:18\n",
            "epoch [7/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.1660 (0.7445) lr 1.0000e-03 eta 0:04:17\n",
            "epoch [7/10] batch [460/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5449 (0.7472) lr 1.0000e-03 eta 0:04:16\n",
            "epoch [7/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2161 (0.7345) lr 1.0000e-03 eta 0:04:15\n",
            "epoch [7/10] batch [500/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4873 (0.7622) lr 1.0000e-03 eta 0:04:14\n",
            "epoch [7/10] batch [520/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0052 (0.7635) lr 1.0000e-03 eta 0:04:13\n",
            "epoch [7/10] batch [540/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0209 (0.7622) lr 1.0000e-03 eta 0:04:12\n",
            "epoch [7/10] batch [560/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.1270 (0.7628) lr 1.0000e-03 eta 0:04:11\n",
            "epoch [7/10] batch [580/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.8447 (0.7615) lr 1.0000e-03 eta 0:04:10\n",
            "epoch [7/10] batch [600/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0194 (0.7600) lr 1.0000e-03 eta 0:04:10\n",
            "epoch [7/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1487 (0.7722) lr 1.0000e-03 eta 0:04:09\n",
            "epoch [7/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0787 (0.7626) lr 1.0000e-03 eta 0:04:08\n",
            "epoch [7/10] batch [660/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.1832 (0.7497) lr 1.0000e-03 eta 0:04:07\n",
            "epoch [7/10] batch [680/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.8086 (0.7705) lr 1.0000e-03 eta 0:04:06\n",
            "epoch [7/10] batch [700/1632] time 0.045 (0.042) data 0.000 (0.001) loss 2.2891 (0.7707) lr 1.0000e-03 eta 0:04:05\n",
            "epoch [7/10] batch [720/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0317 (0.7748) lr 1.0000e-03 eta 0:04:04\n",
            "epoch [7/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.0303 (0.7823) lr 1.0000e-03 eta 0:04:03\n",
            "epoch [7/10] batch [760/1632] time 0.046 (0.042) data 0.000 (0.001) loss 0.4448 (0.7888) lr 1.0000e-03 eta 0:04:02\n",
            "epoch [7/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1722 (0.7831) lr 1.0000e-03 eta 0:04:02\n",
            "epoch [7/10] batch [800/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0698 (0.7867) lr 1.0000e-03 eta 0:04:01\n",
            "epoch [7/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1075 (0.7770) lr 1.0000e-03 eta 0:04:00\n",
            "epoch [7/10] batch [840/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2944 (0.7745) lr 1.0000e-03 eta 0:03:59\n",
            "epoch [7/10] batch [860/1632] time 0.046 (0.042) data 0.000 (0.001) loss 0.1105 (0.7830) lr 1.0000e-03 eta 0:03:59\n",
            "epoch [7/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0871 (0.7779) lr 1.0000e-03 eta 0:03:58\n",
            "epoch [7/10] batch [900/1632] time 0.045 (0.042) data 0.000 (0.001) loss 1.0508 (0.7745) lr 1.0000e-03 eta 0:03:57\n",
            "epoch [7/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5454 (0.7726) lr 1.0000e-03 eta 0:03:56\n",
            "epoch [7/10] batch [940/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.0225 (0.7774) lr 1.0000e-03 eta 0:03:55\n",
            "epoch [7/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.0684 (0.7797) lr 1.0000e-03 eta 0:03:54\n",
            "epoch [7/10] batch [980/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.0176 (0.7854) lr 1.0000e-03 eta 0:03:53\n",
            "epoch [7/10] batch [1000/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2297 (0.7916) lr 1.0000e-03 eta 0:03:53\n",
            "epoch [7/10] batch [1020/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0270 (0.7899) lr 1.0000e-03 eta 0:03:52\n",
            "epoch [7/10] batch [1040/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0507 (0.7897) lr 1.0000e-03 eta 0:03:51\n",
            "epoch [7/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.1406 (0.7888) lr 1.0000e-03 eta 0:03:50\n",
            "epoch [7/10] batch [1080/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4263 (0.7910) lr 1.0000e-03 eta 0:03:49\n",
            "epoch [7/10] batch [1100/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.9355 (0.7844) lr 1.0000e-03 eta 0:03:48\n",
            "epoch [7/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2336 (0.7842) lr 1.0000e-03 eta 0:03:47\n",
            "epoch [7/10] batch [1140/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0600 (0.7824) lr 1.0000e-03 eta 0:03:47\n",
            "epoch [7/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0515 (0.7791) lr 1.0000e-03 eta 0:03:46\n",
            "epoch [7/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 4.0586 (0.7805) lr 1.0000e-03 eta 0:03:45\n",
            "epoch [7/10] batch [1200/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4092 (0.7800) lr 1.0000e-03 eta 0:03:44\n",
            "epoch [7/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.0605 (0.7781) lr 1.0000e-03 eta 0:03:43\n",
            "epoch [7/10] batch [1240/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4302 (0.7909) lr 1.0000e-03 eta 0:03:42\n",
            "epoch [7/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.1826 (0.7881) lr 1.0000e-03 eta 0:03:41\n",
            "epoch [7/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1755 (0.7882) lr 1.0000e-03 eta 0:03:40\n",
            "epoch [7/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0958 (0.7909) lr 1.0000e-03 eta 0:03:39\n",
            "epoch [7/10] batch [1320/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8496 (0.7897) lr 1.0000e-03 eta 0:03:38\n",
            "epoch [7/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0086 (0.7836) lr 1.0000e-03 eta 0:03:38\n",
            "epoch [7/10] batch [1360/1632] time 0.090 (0.042) data 0.000 (0.000) loss 0.5112 (0.7887) lr 1.0000e-03 eta 0:03:37\n",
            "epoch [7/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.7949 (0.7849) lr 1.0000e-03 eta 0:03:36\n",
            "epoch [7/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1703 (0.7878) lr 1.0000e-03 eta 0:03:35\n",
            "epoch [7/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0479 (0.7897) lr 1.0000e-03 eta 0:03:34\n",
            "epoch [7/10] batch [1440/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.3623 (0.7849) lr 1.0000e-03 eta 0:03:33\n",
            "epoch [7/10] batch [1460/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1061 (0.7803) lr 1.0000e-03 eta 0:03:33\n",
            "epoch [7/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0524 (0.7772) lr 1.0000e-03 eta 0:03:32\n",
            "epoch [7/10] batch [1500/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.4121 (0.7768) lr 1.0000e-03 eta 0:03:31\n",
            "epoch [7/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0225 (0.7747) lr 1.0000e-03 eta 0:03:30\n",
            "epoch [7/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9331 (0.7745) lr 1.0000e-03 eta 0:03:29\n",
            "epoch [7/10] batch [1560/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0473 (0.7719) lr 1.0000e-03 eta 0:03:28\n",
            "epoch [7/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1564 (0.7778) lr 1.0000e-03 eta 0:03:27\n",
            "epoch [7/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.3184 (0.7758) lr 1.0000e-03 eta 0:03:27\n",
            "epoch [7/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.6274 (0.7713) lr 1.0000e-03 eta 0:03:26\n",
            "epoch [8/10] batch [20/1632] time 0.042 (0.056) data 0.000 (0.013) loss 0.4666 (0.7203) lr 6.9098e-04 eta 0:04:34\n",
            "epoch [8/10] batch [40/1632] time 0.041 (0.049) data 0.000 (0.007) loss 1.6006 (0.5743) lr 6.9098e-04 eta 0:03:58\n",
            "epoch [8/10] batch [60/1632] time 0.041 (0.047) data 0.000 (0.005) loss 0.2285 (0.5170) lr 6.9098e-04 eta 0:03:45\n",
            "epoch [8/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.2268 (0.5568) lr 6.9098e-04 eta 0:03:38\n",
            "epoch [8/10] batch [100/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.0695 (0.5925) lr 6.9098e-04 eta 0:03:33\n",
            "epoch [8/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 2.3066 (0.6588) lr 6.9098e-04 eta 0:03:30\n",
            "epoch [8/10] batch [140/1632] time 0.042 (0.044) data 0.000 (0.002) loss 7.0859 (0.7972) lr 6.9098e-04 eta 0:03:28\n",
            "epoch [8/10] batch [160/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.0255 (0.8038) lr 6.9098e-04 eta 0:03:25\n",
            "epoch [8/10] batch [180/1632] time 0.043 (0.043) data 0.000 (0.002) loss 0.5303 (0.8089) lr 6.9098e-04 eta 0:03:24\n",
            "epoch [8/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.0190 (0.7934) lr 6.9098e-04 eta 0:03:22\n",
            "epoch [8/10] batch [220/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0267 (0.7878) lr 6.9098e-04 eta 0:03:20\n",
            "epoch [8/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2097 (0.7653) lr 6.9098e-04 eta 0:03:19\n",
            "epoch [8/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.8174 (0.7767) lr 6.9098e-04 eta 0:03:17\n",
            "epoch [8/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0065 (0.7945) lr 6.9098e-04 eta 0:03:16\n",
            "epoch [8/10] batch [300/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0052 (0.7723) lr 6.9098e-04 eta 0:03:15\n",
            "epoch [8/10] batch [320/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.0469 (0.7697) lr 6.9098e-04 eta 0:03:13\n",
            "epoch [8/10] batch [340/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.9058 (0.7496) lr 6.9098e-04 eta 0:03:12\n",
            "epoch [8/10] batch [360/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.3464 (0.7200) lr 6.9098e-04 eta 0:03:11\n",
            "epoch [8/10] batch [380/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2864 (0.7139) lr 6.9098e-04 eta 0:03:10\n",
            "epoch [8/10] batch [400/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1884 (0.7241) lr 6.9098e-04 eta 0:03:08\n",
            "epoch [8/10] batch [420/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.4609 (0.7328) lr 6.9098e-04 eta 0:03:07\n",
            "epoch [8/10] batch [440/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0690 (0.7373) lr 6.9098e-04 eta 0:03:06\n",
            "epoch [8/10] batch [460/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.8062 (0.7229) lr 6.9098e-04 eta 0:03:05\n",
            "epoch [8/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1375 (0.7153) lr 6.9098e-04 eta 0:03:04\n",
            "epoch [8/10] batch [500/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0129 (0.7067) lr 6.9098e-04 eta 0:03:03\n",
            "epoch [8/10] batch [520/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1453 (0.7018) lr 6.9098e-04 eta 0:03:03\n",
            "epoch [8/10] batch [540/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0591 (0.6956) lr 6.9098e-04 eta 0:03:02\n",
            "epoch [8/10] batch [560/1632] time 0.050 (0.042) data 0.000 (0.001) loss 0.0429 (0.7079) lr 6.9098e-04 eta 0:03:01\n",
            "epoch [8/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4019 (0.7139) lr 6.9098e-04 eta 0:03:00\n",
            "epoch [8/10] batch [600/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0119 (0.7067) lr 6.9098e-04 eta 0:03:00\n",
            "epoch [8/10] batch [620/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0714 (0.7126) lr 6.9098e-04 eta 0:02:59\n",
            "epoch [8/10] batch [640/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1860 (0.7141) lr 6.9098e-04 eta 0:02:58\n",
            "epoch [8/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0969 (0.7075) lr 6.9098e-04 eta 0:02:57\n",
            "epoch [8/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1647 (0.7189) lr 6.9098e-04 eta 0:02:56\n",
            "epoch [8/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5239 (0.7152) lr 6.9098e-04 eta 0:02:55\n",
            "epoch [8/10] batch [720/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0022 (0.7077) lr 6.9098e-04 eta 0:02:54\n",
            "epoch [8/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7476 (0.7100) lr 6.9098e-04 eta 0:02:53\n",
            "epoch [8/10] batch [760/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0865 (0.7197) lr 6.9098e-04 eta 0:02:53\n",
            "epoch [8/10] batch [780/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.0977 (0.7178) lr 6.9098e-04 eta 0:02:52\n",
            "epoch [8/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0078 (0.7149) lr 6.9098e-04 eta 0:02:51\n",
            "epoch [8/10] batch [820/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.2251 (0.7142) lr 6.9098e-04 eta 0:02:50\n",
            "epoch [8/10] batch [840/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0206 (0.7163) lr 6.9098e-04 eta 0:02:49\n",
            "epoch [8/10] batch [860/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3711 (0.7351) lr 6.9098e-04 eta 0:02:48\n",
            "epoch [8/10] batch [880/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0011 (0.7328) lr 6.9098e-04 eta 0:02:47\n",
            "epoch [8/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1190 (0.7300) lr 6.9098e-04 eta 0:02:47\n",
            "epoch [8/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 3.3398 (0.7358) lr 6.9098e-04 eta 0:02:46\n",
            "epoch [8/10] batch [940/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.5869 (0.7377) lr 6.9098e-04 eta 0:02:45\n",
            "epoch [8/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0293 (0.7326) lr 6.9098e-04 eta 0:02:44\n",
            "epoch [8/10] batch [980/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0088 (0.7405) lr 6.9098e-04 eta 0:02:43\n",
            "epoch [8/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0250 (0.7403) lr 6.9098e-04 eta 0:02:42\n",
            "epoch [8/10] batch [1020/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2766 (0.7335) lr 6.9098e-04 eta 0:02:41\n",
            "epoch [8/10] batch [1040/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0804 (0.7345) lr 6.9098e-04 eta 0:02:40\n",
            "epoch [8/10] batch [1060/1632] time 0.042 (0.042) data 0.000 (0.000) loss 3.7363 (0.7411) lr 6.9098e-04 eta 0:02:39\n",
            "epoch [8/10] batch [1080/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.5620 (0.7424) lr 6.9098e-04 eta 0:02:39\n",
            "epoch [8/10] batch [1100/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0090 (0.7515) lr 6.9098e-04 eta 0:02:38\n",
            "epoch [8/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6729 (0.7468) lr 6.9098e-04 eta 0:02:37\n",
            "epoch [8/10] batch [1140/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0152 (0.7466) lr 6.9098e-04 eta 0:02:36\n",
            "epoch [8/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0107 (0.7505) lr 6.9098e-04 eta 0:02:35\n",
            "epoch [8/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2681 (0.7625) lr 6.9098e-04 eta 0:02:34\n",
            "epoch [8/10] batch [1200/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.9180 (0.7627) lr 6.9098e-04 eta 0:02:34\n",
            "epoch [8/10] batch [1220/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1076 (0.7601) lr 6.9098e-04 eta 0:02:33\n",
            "epoch [8/10] batch [1240/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.2089 (0.7642) lr 6.9098e-04 eta 0:02:32\n",
            "epoch [8/10] batch [1260/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0203 (0.7632) lr 6.9098e-04 eta 0:02:32\n",
            "epoch [8/10] batch [1280/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0203 (0.7604) lr 6.9098e-04 eta 0:02:31\n",
            "epoch [8/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1744 (0.7576) lr 6.9098e-04 eta 0:02:30\n",
            "epoch [8/10] batch [1320/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0289 (0.7541) lr 6.9098e-04 eta 0:02:29\n",
            "epoch [8/10] batch [1340/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1633 (0.7519) lr 6.9098e-04 eta 0:02:28\n",
            "epoch [8/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6221 (0.7524) lr 6.9098e-04 eta 0:02:27\n",
            "epoch [8/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4216 (0.7501) lr 6.9098e-04 eta 0:02:26\n",
            "epoch [8/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.3789 (0.7479) lr 6.9098e-04 eta 0:02:25\n",
            "epoch [8/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.3828 (0.7432) lr 6.9098e-04 eta 0:02:24\n",
            "epoch [8/10] batch [1440/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0216 (0.7470) lr 6.9098e-04 eta 0:02:24\n",
            "epoch [8/10] batch [1460/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.3311 (0.7474) lr 6.9098e-04 eta 0:02:23\n",
            "epoch [8/10] batch [1480/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1147 (0.7458) lr 6.9098e-04 eta 0:02:22\n",
            "epoch [8/10] batch [1500/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.2617 (0.7397) lr 6.9098e-04 eta 0:02:21\n",
            "epoch [8/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1592 (0.7404) lr 6.9098e-04 eta 0:02:20\n",
            "epoch [8/10] batch [1540/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.5566 (0.7482) lr 6.9098e-04 eta 0:02:19\n",
            "epoch [8/10] batch [1560/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2343 (0.7442) lr 6.9098e-04 eta 0:02:19\n",
            "epoch [8/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1041 (0.7410) lr 6.9098e-04 eta 0:02:18\n",
            "epoch [8/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0473 (0.7437) lr 6.9098e-04 eta 0:02:17\n",
            "epoch [8/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.0029 (0.7444) lr 6.9098e-04 eta 0:02:16\n",
            "epoch [9/10] batch [20/1632] time 0.041 (0.055) data 0.000 (0.013) loss 0.4817 (0.4726) lr 4.1221e-04 eta 0:02:58\n",
            "epoch [9/10] batch [40/1632] time 0.046 (0.048) data 0.000 (0.006) loss 0.3059 (0.4386) lr 4.1221e-04 eta 0:02:35\n",
            "epoch [9/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.0239 (0.4888) lr 4.1221e-04 eta 0:02:27\n",
            "epoch [9/10] batch [80/1632] time 0.040 (0.045) data 0.000 (0.003) loss 0.2666 (0.5306) lr 4.1221e-04 eta 0:02:22\n",
            "epoch [9/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 0.2003 (0.5041) lr 4.1221e-04 eta 0:02:19\n",
            "epoch [9/10] batch [120/1632] time 0.045 (0.044) data 0.000 (0.002) loss 0.0578 (0.5181) lr 4.1221e-04 eta 0:02:17\n",
            "epoch [9/10] batch [140/1632] time 0.040 (0.043) data 0.000 (0.002) loss 0.0052 (0.4806) lr 4.1221e-04 eta 0:02:15\n",
            "epoch [9/10] batch [160/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.0084 (0.5108) lr 4.1221e-04 eta 0:02:13\n",
            "epoch [9/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.0401 (0.5004) lr 4.1221e-04 eta 0:02:12\n",
            "epoch [9/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.4514 (0.5500) lr 4.1221e-04 eta 0:02:10\n",
            "epoch [9/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0497 (0.5424) lr 4.1221e-04 eta 0:02:09\n",
            "epoch [9/10] batch [240/1632] time 0.045 (0.043) data 0.000 (0.001) loss 0.1045 (0.5525) lr 4.1221e-04 eta 0:02:08\n",
            "epoch [9/10] batch [260/1632] time 0.047 (0.043) data 0.000 (0.001) loss 0.0243 (0.5964) lr 4.1221e-04 eta 0:02:07\n",
            "epoch [9/10] batch [280/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.6558 (0.6116) lr 4.1221e-04 eta 0:02:06\n",
            "epoch [9/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1870 (0.6295) lr 4.1221e-04 eta 0:02:06\n",
            "epoch [9/10] batch [320/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2378 (0.6461) lr 4.1221e-04 eta 0:02:05\n",
            "epoch [9/10] batch [340/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.4385 (0.6531) lr 4.1221e-04 eta 0:02:04\n",
            "epoch [9/10] batch [360/1632] time 0.043 (0.042) data 0.000 (0.001) loss 7.2070 (0.6601) lr 4.1221e-04 eta 0:02:03\n",
            "epoch [9/10] batch [380/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.3428 (0.6494) lr 4.1221e-04 eta 0:02:02\n",
            "epoch [9/10] batch [400/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.4006 (0.6584) lr 4.1221e-04 eta 0:02:01\n",
            "epoch [9/10] batch [420/1632] time 0.042 (0.042) data 0.000 (0.001) loss 5.2773 (0.6690) lr 4.1221e-04 eta 0:02:00\n",
            "epoch [9/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6265 (0.6797) lr 4.1221e-04 eta 0:01:59\n",
            "epoch [9/10] batch [460/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2012 (0.6858) lr 4.1221e-04 eta 0:01:58\n",
            "epoch [9/10] batch [480/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0067 (0.6924) lr 4.1221e-04 eta 0:01:57\n",
            "epoch [9/10] batch [500/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2039 (0.6788) lr 4.1221e-04 eta 0:01:56\n",
            "epoch [9/10] batch [520/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2014 (0.6855) lr 4.1221e-04 eta 0:01:56\n",
            "epoch [9/10] batch [540/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0032 (0.6768) lr 4.1221e-04 eta 0:01:55\n",
            "epoch [9/10] batch [560/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.8652 (0.6812) lr 4.1221e-04 eta 0:01:54\n",
            "epoch [9/10] batch [580/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0327 (0.6864) lr 4.1221e-04 eta 0:01:53\n",
            "epoch [9/10] batch [600/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1481 (0.6898) lr 4.1221e-04 eta 0:01:52\n",
            "epoch [9/10] batch [620/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.0602 (0.6854) lr 4.1221e-04 eta 0:01:51\n",
            "epoch [9/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.5371 (0.6764) lr 4.1221e-04 eta 0:01:50\n",
            "epoch [9/10] batch [660/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0054 (0.6665) lr 4.1221e-04 eta 0:01:49\n",
            "epoch [9/10] batch [680/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2283 (0.6682) lr 4.1221e-04 eta 0:01:48\n",
            "epoch [9/10] batch [700/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.3555 (0.6794) lr 4.1221e-04 eta 0:01:47\n",
            "epoch [9/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.2861 (0.6884) lr 4.1221e-04 eta 0:01:47\n",
            "epoch [9/10] batch [740/1632] time 0.040 (0.042) data 0.000 (0.001) loss 3.6270 (0.6861) lr 4.1221e-04 eta 0:01:46\n",
            "epoch [9/10] batch [760/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7153 (0.6930) lr 4.1221e-04 eta 0:01:45\n",
            "epoch [9/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7637 (0.6919) lr 4.1221e-04 eta 0:01:44\n",
            "epoch [9/10] batch [800/1632] time 0.040 (0.042) data 0.000 (0.001) loss 3.7402 (0.6892) lr 4.1221e-04 eta 0:01:43\n",
            "epoch [9/10] batch [820/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0412 (0.6833) lr 4.1221e-04 eta 0:01:42\n",
            "epoch [9/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0168 (0.6787) lr 4.1221e-04 eta 0:01:41\n",
            "epoch [9/10] batch [860/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8789 (0.6803) lr 4.1221e-04 eta 0:01:40\n",
            "epoch [9/10] batch [880/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3909 (0.6739) lr 4.1221e-04 eta 0:01:39\n",
            "epoch [9/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0141 (0.6689) lr 4.1221e-04 eta 0:01:39\n",
            "epoch [9/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0331 (0.6636) lr 4.1221e-04 eta 0:01:38\n",
            "epoch [9/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.0078 (0.6749) lr 4.1221e-04 eta 0:01:37\n",
            "epoch [9/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0934 (0.6791) lr 4.1221e-04 eta 0:01:36\n",
            "epoch [9/10] batch [980/1632] time 0.045 (0.042) data 0.000 (0.000) loss 5.5820 (0.6821) lr 4.1221e-04 eta 0:01:35\n",
            "epoch [9/10] batch [1000/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0719 (0.6840) lr 4.1221e-04 eta 0:01:34\n",
            "epoch [9/10] batch [1020/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.7466 (0.6829) lr 4.1221e-04 eta 0:01:33\n",
            "epoch [9/10] batch [1040/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0074 (0.6835) lr 4.1221e-04 eta 0:01:33\n",
            "epoch [9/10] batch [1060/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.1356 (0.6765) lr 4.1221e-04 eta 0:01:32\n",
            "epoch [9/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4153 (0.6745) lr 4.1221e-04 eta 0:01:31\n",
            "epoch [9/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.5391 (0.6723) lr 4.1221e-04 eta 0:01:30\n",
            "epoch [9/10] batch [1120/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2659 (0.6738) lr 4.1221e-04 eta 0:01:29\n",
            "epoch [9/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0453 (0.6793) lr 4.1221e-04 eta 0:01:28\n",
            "epoch [9/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0202 (0.6750) lr 4.1221e-04 eta 0:01:28\n",
            "epoch [9/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0272 (0.6726) lr 4.1221e-04 eta 0:01:27\n",
            "epoch [9/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0452 (0.6696) lr 4.1221e-04 eta 0:01:26\n",
            "epoch [9/10] batch [1220/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0516 (0.6742) lr 4.1221e-04 eta 0:01:25\n",
            "epoch [9/10] batch [1240/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.6021 (0.6786) lr 4.1221e-04 eta 0:01:24\n",
            "epoch [9/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0841 (0.6822) lr 4.1221e-04 eta 0:01:23\n",
            "epoch [9/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.1240 (0.6819) lr 4.1221e-04 eta 0:01:22\n",
            "epoch [9/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.3008 (0.6863) lr 4.1221e-04 eta 0:01:22\n",
            "epoch [9/10] batch [1320/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.4180 (0.6870) lr 4.1221e-04 eta 0:01:21\n",
            "epoch [9/10] batch [1340/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0114 (0.6914) lr 4.1221e-04 eta 0:01:20\n",
            "epoch [9/10] batch [1360/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.8828 (0.6928) lr 4.1221e-04 eta 0:01:19\n",
            "epoch [9/10] batch [1380/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0837 (0.7022) lr 4.1221e-04 eta 0:01:18\n",
            "epoch [9/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.7559 (0.7029) lr 4.1221e-04 eta 0:01:17\n",
            "epoch [9/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0130 (0.7017) lr 4.1221e-04 eta 0:01:16\n",
            "epoch [9/10] batch [1440/1632] time 0.040 (0.042) data 0.000 (0.000) loss 4.6367 (0.7026) lr 4.1221e-04 eta 0:01:16\n",
            "epoch [9/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2695 (0.6998) lr 4.1221e-04 eta 0:01:15\n",
            "epoch [9/10] batch [1480/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0484 (0.7006) lr 4.1221e-04 eta 0:01:14\n",
            "epoch [9/10] batch [1500/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0481 (0.6968) lr 4.1221e-04 eta 0:01:13\n",
            "epoch [9/10] batch [1520/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.8970 (0.7058) lr 4.1221e-04 eta 0:01:12\n",
            "epoch [9/10] batch [1540/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.9551 (0.7108) lr 4.1221e-04 eta 0:01:11\n",
            "epoch [9/10] batch [1560/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.7109 (0.7131) lr 4.1221e-04 eta 0:01:10\n",
            "epoch [9/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0248 (0.7141) lr 4.1221e-04 eta 0:01:10\n",
            "epoch [9/10] batch [1600/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1260 (0.7156) lr 4.1221e-04 eta 0:01:09\n",
            "epoch [9/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.0232 (0.7166) lr 4.1221e-04 eta 0:01:08\n",
            "epoch [10/10] batch [20/1632] time 0.041 (0.056) data 0.000 (0.013) loss 0.1876 (0.9268) lr 1.9098e-04 eta 0:01:29\n",
            "epoch [10/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.006) loss 0.0702 (0.8679) lr 1.9098e-04 eta 0:01:17\n",
            "epoch [10/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.4275 (0.7424) lr 1.9098e-04 eta 0:01:12\n",
            "epoch [10/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.3918 (0.7120) lr 1.9098e-04 eta 0:01:09\n",
            "epoch [10/10] batch [100/1632] time 0.040 (0.044) data 0.000 (0.003) loss 2.1953 (0.6435) lr 1.9098e-04 eta 0:01:07\n",
            "epoch [10/10] batch [120/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.1638 (0.5976) lr 1.9098e-04 eta 0:01:05\n",
            "epoch [10/10] batch [140/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.1306 (0.6255) lr 1.9098e-04 eta 0:01:04\n",
            "epoch [10/10] batch [160/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.2368 (0.6830) lr 1.9098e-04 eta 0:01:03\n",
            "epoch [10/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.1184 (0.6649) lr 1.9098e-04 eta 0:01:02\n",
            "epoch [10/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0237 (0.7079) lr 1.9098e-04 eta 0:01:01\n",
            "epoch [10/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0451 (0.6895) lr 1.9098e-04 eta 0:01:00\n",
            "epoch [10/10] batch [240/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1063 (0.6897) lr 1.9098e-04 eta 0:00:59\n",
            "epoch [10/10] batch [260/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1122 (0.6885) lr 1.9098e-04 eta 0:00:58\n",
            "epoch [10/10] batch [280/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0948 (0.6682) lr 1.9098e-04 eta 0:00:57\n",
            "epoch [10/10] batch [300/1632] time 0.044 (0.043) data 0.000 (0.001) loss 0.3115 (0.6643) lr 1.9098e-04 eta 0:00:56\n",
            "epoch [10/10] batch [320/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1908 (0.6924) lr 1.9098e-04 eta 0:00:55\n",
            "epoch [10/10] batch [340/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0568 (0.7372) lr 1.9098e-04 eta 0:00:54\n",
            "epoch [10/10] batch [360/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.0062 (0.7545) lr 1.9098e-04 eta 0:00:54\n",
            "epoch [10/10] batch [380/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.3076 (0.7446) lr 1.9098e-04 eta 0:00:53\n",
            "epoch [10/10] batch [400/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.1116 (0.7356) lr 1.9098e-04 eta 0:00:52\n",
            "epoch [10/10] batch [420/1632] time 0.043 (0.043) data 0.000 (0.001) loss 1.3740 (0.7314) lr 1.9098e-04 eta 0:00:51\n",
            "epoch [10/10] batch [440/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0715 (0.7251) lr 1.9098e-04 eta 0:00:50\n",
            "epoch [10/10] batch [460/1632] time 0.043 (0.043) data 0.000 (0.001) loss 1.2471 (0.7245) lr 1.9098e-04 eta 0:00:49\n",
            "epoch [10/10] batch [480/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.1947 (0.7148) lr 1.9098e-04 eta 0:00:49\n",
            "epoch [10/10] batch [500/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0206 (0.7002) lr 1.9098e-04 eta 0:00:48\n",
            "epoch [10/10] batch [520/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0262 (0.7132) lr 1.9098e-04 eta 0:00:47\n",
            "epoch [10/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.1494 (0.7172) lr 1.9098e-04 eta 0:00:46\n",
            "epoch [10/10] batch [560/1632] time 0.042 (0.042) data 0.000 (0.001) loss 5.6016 (0.7285) lr 1.9098e-04 eta 0:00:45\n",
            "epoch [10/10] batch [580/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2489 (0.7182) lr 1.9098e-04 eta 0:00:44\n",
            "epoch [10/10] batch [600/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0310 (0.7156) lr 1.9098e-04 eta 0:00:43\n",
            "epoch [10/10] batch [620/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.4219 (0.7079) lr 1.9098e-04 eta 0:00:42\n",
            "epoch [10/10] batch [640/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.8936 (0.7159) lr 1.9098e-04 eta 0:00:41\n",
            "epoch [10/10] batch [660/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.4875 (0.7251) lr 1.9098e-04 eta 0:00:41\n",
            "epoch [10/10] batch [680/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.0156 (0.7186) lr 1.9098e-04 eta 0:00:40\n",
            "epoch [10/10] batch [700/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0688 (0.7138) lr 1.9098e-04 eta 0:00:39\n",
            "epoch [10/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.3926 (0.7063) lr 1.9098e-04 eta 0:00:38\n",
            "epoch [10/10] batch [740/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.4375 (0.7137) lr 1.9098e-04 eta 0:00:37\n",
            "epoch [10/10] batch [760/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.6133 (0.7116) lr 1.9098e-04 eta 0:00:36\n",
            "epoch [10/10] batch [780/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.9023 (0.7103) lr 1.9098e-04 eta 0:00:35\n",
            "epoch [10/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2988 (0.7191) lr 1.9098e-04 eta 0:00:34\n",
            "epoch [10/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5059 (0.7121) lr 1.9098e-04 eta 0:00:34\n",
            "epoch [10/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6294 (0.7164) lr 1.9098e-04 eta 0:00:33\n",
            "epoch [10/10] batch [860/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0784 (0.7075) lr 1.9098e-04 eta 0:00:32\n",
            "epoch [10/10] batch [880/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0372 (0.7146) lr 1.9098e-04 eta 0:00:31\n",
            "epoch [10/10] batch [900/1632] time 0.045 (0.042) data 0.000 (0.001) loss 1.2490 (0.7079) lr 1.9098e-04 eta 0:00:30\n",
            "epoch [10/10] batch [920/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1083 (0.7000) lr 1.9098e-04 eta 0:00:29\n",
            "epoch [10/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7363 (0.7080) lr 1.9098e-04 eta 0:00:29\n",
            "epoch [10/10] batch [960/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.6689 (0.7025) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [980/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.3633 (0.7021) lr 1.9098e-04 eta 0:00:27\n",
            "epoch [10/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0895 (0.6985) lr 1.9098e-04 eta 0:00:26\n",
            "epoch [10/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0926 (0.6907) lr 1.9098e-04 eta 0:00:25\n",
            "epoch [10/10] batch [1040/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1146 (0.6865) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [10/10] batch [1060/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0291 (0.6876) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [10/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4355 (0.6855) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [1100/1632] time 0.040 (0.042) data 0.000 (0.000) loss 5.1445 (0.6830) lr 1.9098e-04 eta 0:00:22\n",
            "epoch [10/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0111 (0.6835) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [10/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5767 (0.6843) lr 1.9098e-04 eta 0:00:20\n",
            "epoch [10/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.6953 (0.6817) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1652 (0.6827) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [1200/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0872 (0.6769) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [1220/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.0020 (0.6844) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [10/10] batch [1240/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2109 (0.6868) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0508 (0.6903) lr 1.9098e-04 eta 0:00:15\n",
            "epoch [10/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.7480 (0.6887) lr 1.9098e-04 eta 0:00:14\n",
            "epoch [10/10] batch [1300/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1201 (0.6988) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [1320/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1197 (0.7011) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [1340/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.8989 (0.7002) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1730 (0.7020) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [10/10] batch [1380/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4124 (0.7013) lr 1.9098e-04 eta 0:00:10\n",
            "epoch [10/10] batch [1400/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0802 (0.6992) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [1420/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0516 (0.6976) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0067 (0.7015) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [1460/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2297 (0.6994) lr 1.9098e-04 eta 0:00:07\n",
            "epoch [10/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.5977 (0.6979) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0218 (0.6986) lr 1.9098e-04 eta 0:00:05\n",
            "epoch [10/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0758 (0.7020) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [10/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0524 (0.7054) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [1560/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.7544 (0.7109) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [1580/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0457 (0.7086) lr 1.9098e-04 eta 0:00:02\n",
            "epoch [10/10] batch [1600/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.2871 (0.7127) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.3811 (0.7097) lr 1.9098e-04 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2/prompt_learner/model.pth.tar-10\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 25/25 [00:30<00:00,  1.24s/it]\n",
            "=> result\n",
            "* total: 2,463\n",
            "* correct: 2,206\n",
            "* accuracy: 89.6%\n",
            "* error: 10.4%\n",
            "* macro_f1: 88.1%\n",
            "Elapsed: 0:11:57\n",
            "2024-12-10 11:44:38.376267: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-10 11:44:38.393565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-10 11:44:38.414662: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-10 11:44:38.421165: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-10 11:44:38.436089: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-10 11:44:39.494401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/MIP/vit_b16_c4_ep10_batch1.yaml\n",
            "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: 0\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3\n",
            "resume: \n",
            "root: data/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: MIP\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordFlowers\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: data/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: MIP\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               12\n",
            "On-line CPU(s) list:                  0-11\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   6\n",
            "Socket(s):                            1\n",
            "Stepping:                             7\n",
            "BogoMIPS:                             4400.44\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            192 KiB (6 instances)\n",
            "L1i cache:                            192 KiB (6 instances)\n",
            "L2 cache:                             6 MiB (6 instances)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-11\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Not affected\n",
            "Vulnerability Mds:                    Not affected\n",
            "Vulnerability Meltdown:               Not affected\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: MIP\n",
            "Loading dataset: OxfordFlowers\n",
            "Reading split from /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_flowers/split_zhou_OxfordFlowers.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_flowers/split_fewshot/shot_16-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------------\n",
            "Dataset    OxfordFlowers\n",
            "# classes  102\n",
            "# train_x  1,632\n",
            "# val      408\n",
            "# test     2,463\n",
            "---------  -------------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear1.weight', 'prompt_learner.ctx', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear1.bias'}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3/tensorboard)\n",
            "epoch [1/10] batch [20/1632] time 0.042 (0.132) data 0.000 (0.016) loss 3.6621 (4.7679) lr 1.0000e-05 eta 0:35:59\n",
            "epoch [1/10] batch [40/1632] time 0.042 (0.087) data 0.000 (0.008) loss 1.2920 (3.9988) lr 1.0000e-05 eta 0:23:40\n",
            "epoch [1/10] batch [60/1632] time 0.040 (0.072) data 0.000 (0.006) loss 0.1094 (3.5561) lr 1.0000e-05 eta 0:19:25\n",
            "epoch [1/10] batch [80/1632] time 0.040 (0.064) data 0.000 (0.004) loss 0.5234 (3.4791) lr 1.0000e-05 eta 0:17:20\n",
            "epoch [1/10] batch [100/1632] time 0.040 (0.059) data 0.000 (0.003) loss 9.4844 (3.6895) lr 1.0000e-05 eta 0:16:03\n",
            "epoch [1/10] batch [120/1632] time 0.044 (0.056) data 0.000 (0.003) loss 5.3711 (3.4000) lr 1.0000e-05 eta 0:15:12\n",
            "epoch [1/10] batch [140/1632] time 0.040 (0.054) data 0.000 (0.003) loss 0.7495 (3.3240) lr 1.0000e-05 eta 0:14:35\n",
            "epoch [1/10] batch [160/1632] time 0.040 (0.053) data 0.000 (0.002) loss 7.5469 (3.2590) lr 1.0000e-05 eta 0:14:10\n",
            "epoch [1/10] batch [180/1632] time 0.041 (0.051) data 0.000 (0.002) loss 1.9561 (3.1109) lr 1.0000e-05 eta 0:13:50\n",
            "epoch [1/10] batch [200/1632] time 0.047 (0.051) data 0.000 (0.002) loss 2.2969 (3.0407) lr 1.0000e-05 eta 0:13:34\n",
            "epoch [1/10] batch [220/1632] time 0.041 (0.050) data 0.000 (0.002) loss 3.9805 (3.0246) lr 1.0000e-05 eta 0:13:21\n",
            "epoch [1/10] batch [240/1632] time 0.041 (0.049) data 0.000 (0.002) loss 1.2109 (3.0060) lr 1.0000e-05 eta 0:13:09\n",
            "epoch [1/10] batch [260/1632] time 0.040 (0.048) data 0.000 (0.001) loss 1.2822 (2.9959) lr 1.0000e-05 eta 0:12:58\n",
            "epoch [1/10] batch [280/1632] time 0.040 (0.048) data 0.000 (0.001) loss 0.0598 (2.9172) lr 1.0000e-05 eta 0:12:49\n",
            "epoch [1/10] batch [300/1632] time 0.041 (0.048) data 0.000 (0.001) loss 1.8262 (2.8722) lr 1.0000e-05 eta 0:12:42\n",
            "epoch [1/10] batch [320/1632] time 0.040 (0.047) data 0.000 (0.001) loss 2.5684 (2.8536) lr 1.0000e-05 eta 0:12:35\n",
            "epoch [1/10] batch [340/1632] time 0.040 (0.047) data 0.000 (0.001) loss 0.3877 (2.8131) lr 1.0000e-05 eta 0:12:28\n",
            "epoch [1/10] batch [360/1632] time 0.041 (0.047) data 0.000 (0.001) loss 0.5273 (2.7840) lr 1.0000e-05 eta 0:12:22\n",
            "epoch [1/10] batch [380/1632] time 0.041 (0.046) data 0.000 (0.001) loss 0.8091 (2.8129) lr 1.0000e-05 eta 0:12:16\n",
            "epoch [1/10] batch [400/1632] time 0.041 (0.046) data 0.000 (0.001) loss 0.1326 (2.8121) lr 1.0000e-05 eta 0:12:11\n",
            "epoch [1/10] batch [420/1632] time 0.041 (0.046) data 0.000 (0.001) loss 0.9009 (2.8106) lr 1.0000e-05 eta 0:12:07\n",
            "epoch [1/10] batch [440/1632] time 0.045 (0.046) data 0.000 (0.001) loss 6.9375 (2.7937) lr 1.0000e-05 eta 0:12:02\n",
            "epoch [1/10] batch [460/1632] time 0.041 (0.045) data 0.000 (0.001) loss 0.0112 (2.7586) lr 1.0000e-05 eta 0:11:58\n",
            "epoch [1/10] batch [480/1632] time 0.044 (0.045) data 0.000 (0.001) loss 0.0148 (2.7648) lr 1.0000e-05 eta 0:11:55\n",
            "epoch [1/10] batch [500/1632] time 0.043 (0.045) data 0.000 (0.001) loss 1.8164 (2.7524) lr 1.0000e-05 eta 0:11:52\n",
            "epoch [1/10] batch [520/1632] time 0.041 (0.045) data 0.000 (0.001) loss 8.9375 (2.7657) lr 1.0000e-05 eta 0:11:49\n",
            "epoch [1/10] batch [540/1632] time 0.041 (0.045) data 0.000 (0.001) loss 1.1973 (2.7505) lr 1.0000e-05 eta 0:11:46\n",
            "epoch [1/10] batch [560/1632] time 0.041 (0.045) data 0.000 (0.001) loss 1.4805 (2.7337) lr 1.0000e-05 eta 0:11:43\n",
            "epoch [1/10] batch [580/1632] time 0.041 (0.045) data 0.000 (0.001) loss 1.6660 (2.7258) lr 1.0000e-05 eta 0:11:41\n",
            "epoch [1/10] batch [600/1632] time 0.041 (0.044) data 0.000 (0.001) loss 0.0025 (2.6919) lr 1.0000e-05 eta 0:11:38\n",
            "epoch [1/10] batch [620/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.0630 (2.6989) lr 1.0000e-05 eta 0:11:36\n",
            "epoch [1/10] batch [640/1632] time 0.042 (0.044) data 0.000 (0.001) loss 0.8896 (2.6900) lr 1.0000e-05 eta 0:11:34\n",
            "epoch [1/10] batch [660/1632] time 0.039 (0.044) data 0.000 (0.001) loss 2.7344 (2.7116) lr 1.0000e-05 eta 0:11:32\n",
            "epoch [1/10] batch [680/1632] time 0.042 (0.044) data 0.000 (0.001) loss 8.1562 (2.7160) lr 1.0000e-05 eta 0:11:29\n",
            "epoch [1/10] batch [700/1632] time 0.042 (0.044) data 0.000 (0.001) loss 0.6016 (2.6800) lr 1.0000e-05 eta 0:11:27\n",
            "epoch [1/10] batch [720/1632] time 0.045 (0.044) data 0.000 (0.001) loss 2.6738 (2.6571) lr 1.0000e-05 eta 0:11:26\n",
            "epoch [1/10] batch [740/1632] time 0.040 (0.044) data 0.000 (0.001) loss 0.0804 (2.6438) lr 1.0000e-05 eta 0:11:24\n",
            "epoch [1/10] batch [760/1632] time 0.039 (0.044) data 0.000 (0.001) loss 2.5254 (2.6453) lr 1.0000e-05 eta 0:11:22\n",
            "epoch [1/10] batch [780/1632] time 0.039 (0.044) data 0.000 (0.001) loss 0.9058 (2.6302) lr 1.0000e-05 eta 0:11:20\n",
            "epoch [1/10] batch [800/1632] time 0.040 (0.044) data 0.000 (0.001) loss 11.2422 (2.6387) lr 1.0000e-05 eta 0:11:18\n",
            "epoch [1/10] batch [820/1632] time 0.042 (0.044) data 0.000 (0.001) loss 2.1172 (2.6292) lr 1.0000e-05 eta 0:11:16\n",
            "epoch [1/10] batch [840/1632] time 0.041 (0.044) data 0.000 (0.001) loss 7.1133 (2.6150) lr 1.0000e-05 eta 0:11:15\n",
            "epoch [1/10] batch [860/1632] time 0.043 (0.044) data 0.000 (0.001) loss 0.7031 (2.5950) lr 1.0000e-05 eta 0:11:14\n",
            "epoch [1/10] batch [880/1632] time 0.042 (0.044) data 0.000 (0.001) loss 5.6797 (2.5929) lr 1.0000e-05 eta 0:11:12\n",
            "epoch [1/10] batch [900/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0213 (2.5713) lr 1.0000e-05 eta 0:11:10\n",
            "epoch [1/10] batch [920/1632] time 0.044 (0.043) data 0.000 (0.001) loss 1.6465 (2.5693) lr 1.0000e-05 eta 0:11:09\n",
            "epoch [1/10] batch [940/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1938 (2.5726) lr 1.0000e-05 eta 0:11:07\n",
            "epoch [1/10] batch [960/1632] time 0.039 (0.043) data 0.000 (0.001) loss 0.9160 (2.5633) lr 1.0000e-05 eta 0:11:06\n",
            "epoch [1/10] batch [980/1632] time 0.041 (0.043) data 0.000 (0.001) loss 7.5508 (2.5887) lr 1.0000e-05 eta 0:11:04\n",
            "epoch [1/10] batch [1000/1632] time 0.042 (0.043) data 0.000 (0.001) loss 3.1816 (2.5962) lr 1.0000e-05 eta 0:11:03\n",
            "epoch [1/10] batch [1020/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.9658 (2.5896) lr 1.0000e-05 eta 0:11:01\n",
            "epoch [1/10] batch [1040/1632] time 0.039 (0.043) data 0.000 (0.001) loss 3.8379 (2.5862) lr 1.0000e-05 eta 0:11:01\n",
            "epoch [1/10] batch [1060/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0087 (2.5722) lr 1.0000e-05 eta 0:10:59\n",
            "epoch [1/10] batch [1080/1632] time 0.039 (0.043) data 0.000 (0.001) loss 0.8462 (2.5650) lr 1.0000e-05 eta 0:10:58\n",
            "epoch [1/10] batch [1100/1632] time 0.041 (0.043) data 0.000 (0.001) loss 6.3672 (2.5627) lr 1.0000e-05 eta 0:10:56\n",
            "epoch [1/10] batch [1120/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0790 (2.5542) lr 1.0000e-05 eta 0:10:55\n",
            "epoch [1/10] batch [1140/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.5000 (2.5361) lr 1.0000e-05 eta 0:10:54\n",
            "epoch [1/10] batch [1160/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.5293 (2.5264) lr 1.0000e-05 eta 0:10:52\n",
            "epoch [1/10] batch [1180/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.8521 (2.5135) lr 1.0000e-05 eta 0:10:51\n",
            "epoch [1/10] batch [1200/1632] time 0.043 (0.043) data 0.000 (0.001) loss 3.6875 (2.5111) lr 1.0000e-05 eta 0:10:50\n",
            "epoch [1/10] batch [1220/1632] time 0.040 (0.043) data 0.000 (0.001) loss 2.8848 (2.5180) lr 1.0000e-05 eta 0:10:49\n",
            "epoch [1/10] batch [1240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 2.3203 (2.5161) lr 1.0000e-05 eta 0:10:48\n",
            "epoch [1/10] batch [1260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.6602 (2.5195) lr 1.0000e-05 eta 0:10:47\n",
            "epoch [1/10] batch [1280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 6.3516 (2.5105) lr 1.0000e-05 eta 0:10:46\n",
            "epoch [1/10] batch [1300/1632] time 0.042 (0.043) data 0.000 (0.001) loss 2.4609 (2.5016) lr 1.0000e-05 eta 0:10:45\n",
            "epoch [1/10] batch [1320/1632] time 0.040 (0.043) data 0.000 (0.001) loss 8.3047 (2.4890) lr 1.0000e-05 eta 0:10:43\n",
            "epoch [1/10] batch [1340/1632] time 0.043 (0.043) data 0.000 (0.001) loss 3.0645 (2.4835) lr 1.0000e-05 eta 0:10:42\n",
            "epoch [1/10] batch [1360/1632] time 0.039 (0.043) data 0.000 (0.000) loss 6.4727 (2.4790) lr 1.0000e-05 eta 0:10:41\n",
            "epoch [1/10] batch [1380/1632] time 0.046 (0.043) data 0.000 (0.000) loss 2.9453 (2.4836) lr 1.0000e-05 eta 0:10:40\n",
            "epoch [1/10] batch [1400/1632] time 0.042 (0.043) data 0.000 (0.000) loss 4.3359 (2.4965) lr 1.0000e-05 eta 0:10:39\n",
            "epoch [1/10] batch [1420/1632] time 0.039 (0.043) data 0.000 (0.000) loss 0.1429 (2.4760) lr 1.0000e-05 eta 0:10:38\n",
            "epoch [1/10] batch [1440/1632] time 0.043 (0.043) data 0.000 (0.000) loss 0.1765 (2.4710) lr 1.0000e-05 eta 0:10:37\n",
            "epoch [1/10] batch [1460/1632] time 0.042 (0.043) data 0.000 (0.000) loss 0.7144 (2.4582) lr 1.0000e-05 eta 0:10:36\n",
            "epoch [1/10] batch [1480/1632] time 0.040 (0.043) data 0.000 (0.000) loss 0.4910 (2.4514) lr 1.0000e-05 eta 0:10:35\n",
            "epoch [1/10] batch [1500/1632] time 0.040 (0.043) data 0.000 (0.000) loss 3.1855 (2.4499) lr 1.0000e-05 eta 0:10:34\n",
            "epoch [1/10] batch [1520/1632] time 0.041 (0.043) data 0.000 (0.000) loss 1.2715 (2.4371) lr 1.0000e-05 eta 0:10:33\n",
            "epoch [1/10] batch [1540/1632] time 0.041 (0.043) data 0.001 (0.000) loss 0.1903 (2.4307) lr 1.0000e-05 eta 0:10:32\n",
            "epoch [1/10] batch [1560/1632] time 0.041 (0.043) data 0.000 (0.000) loss 2.2305 (2.4247) lr 1.0000e-05 eta 0:10:31\n",
            "epoch [1/10] batch [1580/1632] time 0.039 (0.043) data 0.000 (0.000) loss 4.1172 (2.4342) lr 1.0000e-05 eta 0:10:29\n",
            "epoch [1/10] batch [1600/1632] time 0.040 (0.043) data 0.000 (0.000) loss 1.7305 (2.4210) lr 1.0000e-05 eta 0:10:28\n",
            "epoch [1/10] batch [1620/1632] time 0.039 (0.043) data 0.000 (0.000) loss 0.5083 (2.4138) lr 1.0000e-05 eta 0:10:27\n",
            "epoch [2/10] batch [20/1632] time 0.041 (0.056) data 0.000 (0.013) loss 0.3503 (2.5333) lr 2.0000e-03 eta 0:13:37\n",
            "epoch [2/10] batch [40/1632] time 0.040 (0.048) data 0.000 (0.007) loss 0.3657 (1.9764) lr 2.0000e-03 eta 0:11:48\n",
            "epoch [2/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 8.4453 (2.1046) lr 2.0000e-03 eta 0:11:13\n",
            "epoch [2/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.7461 (1.8860) lr 2.0000e-03 eta 0:10:56\n",
            "epoch [2/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 2.7676 (1.9458) lr 2.0000e-03 eta 0:10:46\n",
            "epoch [2/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.3853 (1.9669) lr 2.0000e-03 eta 0:10:39\n",
            "epoch [2/10] batch [140/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.3828 (1.9238) lr 2.0000e-03 eta 0:10:33\n",
            "epoch [2/10] batch [160/1632] time 0.041 (0.043) data 0.000 (0.002) loss 2.1250 (1.9078) lr 2.0000e-03 eta 0:10:28\n",
            "epoch [2/10] batch [180/1632] time 0.040 (0.043) data 0.000 (0.002) loss 0.6636 (1.8456) lr 2.0000e-03 eta 0:10:24\n",
            "epoch [2/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.1160 (1.7822) lr 2.0000e-03 eta 0:10:20\n",
            "epoch [2/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.3044 (1.7333) lr 2.0000e-03 eta 0:10:18\n",
            "epoch [2/10] batch [240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2139 (1.7077) lr 2.0000e-03 eta 0:10:18\n",
            "epoch [2/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.9312 (1.6877) lr 2.0000e-03 eta 0:10:15\n",
            "epoch [2/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0019 (1.6806) lr 2.0000e-03 eta 0:10:13\n",
            "epoch [2/10] batch [300/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0015 (1.6645) lr 2.0000e-03 eta 0:10:12\n",
            "epoch [2/10] batch [320/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.5547 (1.6392) lr 2.0000e-03 eta 0:10:09\n",
            "epoch [2/10] batch [340/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2179 (1.6399) lr 2.0000e-03 eta 0:10:07\n",
            "epoch [2/10] batch [360/1632] time 0.040 (0.042) data 0.000 (0.001) loss 5.4922 (1.6526) lr 2.0000e-03 eta 0:10:05\n",
            "epoch [2/10] batch [380/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3213 (1.6235) lr 2.0000e-03 eta 0:10:03\n",
            "epoch [2/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3018 (1.6034) lr 2.0000e-03 eta 0:10:01\n",
            "epoch [2/10] batch [420/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8848 (1.5697) lr 2.0000e-03 eta 0:10:01\n",
            "epoch [2/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6118 (1.5660) lr 2.0000e-03 eta 0:10:00\n",
            "epoch [2/10] batch [460/1632] time 0.045 (0.042) data 0.000 (0.001) loss 1.7441 (1.5410) lr 2.0000e-03 eta 0:09:59\n",
            "epoch [2/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.5664 (1.5387) lr 2.0000e-03 eta 0:09:57\n",
            "epoch [2/10] batch [500/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.4854 (1.5416) lr 2.0000e-03 eta 0:09:56\n",
            "epoch [2/10] batch [520/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.8711 (1.5571) lr 2.0000e-03 eta 0:09:55\n",
            "epoch [2/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3535 (1.5669) lr 2.0000e-03 eta 0:09:54\n",
            "epoch [2/10] batch [560/1632] time 0.046 (0.042) data 0.000 (0.001) loss 0.6577 (1.5778) lr 2.0000e-03 eta 0:09:54\n",
            "epoch [2/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0273 (1.5746) lr 2.0000e-03 eta 0:09:53\n",
            "epoch [2/10] batch [600/1632] time 0.040 (0.042) data 0.000 (0.001) loss 5.3438 (1.5589) lr 2.0000e-03 eta 0:09:52\n",
            "epoch [2/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0634 (1.5567) lr 2.0000e-03 eta 0:09:50\n",
            "epoch [2/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0482 (1.5477) lr 2.0000e-03 eta 0:09:49\n",
            "epoch [2/10] batch [660/1632] time 0.042 (0.042) data 0.000 (0.001) loss 7.3594 (1.5833) lr 2.0000e-03 eta 0:09:47\n",
            "epoch [2/10] batch [680/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1276 (1.5709) lr 2.0000e-03 eta 0:09:46\n",
            "epoch [2/10] batch [700/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.3118 (1.5682) lr 2.0000e-03 eta 0:09:45\n",
            "epoch [2/10] batch [720/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.4551 (1.5753) lr 2.0000e-03 eta 0:09:44\n",
            "epoch [2/10] batch [740/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.5791 (1.5829) lr 2.0000e-03 eta 0:09:43\n",
            "epoch [2/10] batch [760/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.7061 (1.5868) lr 2.0000e-03 eta 0:09:42\n",
            "epoch [2/10] batch [780/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0836 (1.5841) lr 2.0000e-03 eta 0:09:41\n",
            "epoch [2/10] batch [800/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0535 (1.5829) lr 2.0000e-03 eta 0:09:40\n",
            "epoch [2/10] batch [820/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.2119 (1.5753) lr 2.0000e-03 eta 0:09:39\n",
            "epoch [2/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.1475 (1.5725) lr 2.0000e-03 eta 0:09:37\n",
            "epoch [2/10] batch [860/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.4648 (1.5560) lr 2.0000e-03 eta 0:09:36\n",
            "epoch [2/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1379 (1.5832) lr 2.0000e-03 eta 0:09:36\n",
            "epoch [2/10] batch [900/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.3555 (1.5773) lr 2.0000e-03 eta 0:09:35\n",
            "epoch [2/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.8828 (1.5723) lr 2.0000e-03 eta 0:09:34\n",
            "epoch [2/10] batch [940/1632] time 0.046 (0.042) data 0.000 (0.001) loss 0.2172 (1.5616) lr 2.0000e-03 eta 0:09:33\n",
            "epoch [2/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1520 (1.5423) lr 2.0000e-03 eta 0:09:32\n",
            "epoch [2/10] batch [980/1632] time 0.040 (0.042) data 0.000 (0.001) loss 3.2324 (1.5450) lr 2.0000e-03 eta 0:09:31\n",
            "epoch [2/10] batch [1000/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0415 (1.5369) lr 2.0000e-03 eta 0:09:30\n",
            "epoch [2/10] batch [1020/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1338 (1.5349) lr 2.0000e-03 eta 0:09:29\n",
            "epoch [2/10] batch [1040/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4331 (1.5353) lr 2.0000e-03 eta 0:09:27\n",
            "epoch [2/10] batch [1060/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.0137 (1.5327) lr 2.0000e-03 eta 0:09:26\n",
            "epoch [2/10] batch [1080/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.3323 (1.5335) lr 2.0000e-03 eta 0:09:25\n",
            "epoch [2/10] batch [1100/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.7998 (1.5388) lr 2.0000e-03 eta 0:09:24\n",
            "epoch [2/10] batch [1120/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2996 (1.5299) lr 2.0000e-03 eta 0:09:23\n",
            "epoch [2/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0694 (1.5378) lr 2.0000e-03 eta 0:09:22\n",
            "epoch [2/10] batch [1160/1632] time 0.040 (0.041) data 0.000 (0.000) loss 0.1849 (1.5364) lr 2.0000e-03 eta 0:09:21\n",
            "epoch [2/10] batch [1180/1632] time 0.040 (0.041) data 0.000 (0.000) loss 0.5269 (1.5248) lr 2.0000e-03 eta 0:09:20\n",
            "epoch [2/10] batch [1200/1632] time 0.040 (0.041) data 0.000 (0.000) loss 0.1735 (1.5165) lr 2.0000e-03 eta 0:09:19\n",
            "epoch [2/10] batch [1220/1632] time 0.040 (0.041) data 0.000 (0.000) loss 1.2676 (1.5107) lr 2.0000e-03 eta 0:09:17\n",
            "epoch [2/10] batch [1240/1632] time 0.042 (0.041) data 0.000 (0.000) loss 6.7695 (1.5180) lr 2.0000e-03 eta 0:09:17\n",
            "epoch [2/10] batch [1260/1632] time 0.041 (0.041) data 0.000 (0.000) loss 0.0087 (1.5059) lr 2.0000e-03 eta 0:09:16\n",
            "epoch [2/10] batch [1280/1632] time 0.042 (0.041) data 0.000 (0.000) loss 0.0412 (1.4994) lr 2.0000e-03 eta 0:09:15\n",
            "epoch [2/10] batch [1300/1632] time 0.041 (0.041) data 0.000 (0.000) loss 0.1181 (1.4992) lr 2.0000e-03 eta 0:09:15\n",
            "epoch [2/10] batch [1320/1632] time 0.042 (0.041) data 0.000 (0.000) loss 0.5874 (1.5038) lr 2.0000e-03 eta 0:09:14\n",
            "epoch [2/10] batch [1340/1632] time 0.040 (0.041) data 0.000 (0.000) loss 1.3145 (1.5001) lr 2.0000e-03 eta 0:09:13\n",
            "epoch [2/10] batch [1360/1632] time 0.040 (0.041) data 0.000 (0.000) loss 0.0372 (1.4986) lr 2.0000e-03 eta 0:09:12\n",
            "epoch [2/10] batch [1380/1632] time 0.040 (0.041) data 0.000 (0.000) loss 0.5796 (1.5040) lr 2.0000e-03 eta 0:09:11\n",
            "epoch [2/10] batch [1400/1632] time 0.040 (0.041) data 0.000 (0.000) loss 0.0303 (1.4989) lr 2.0000e-03 eta 0:09:10\n",
            "epoch [2/10] batch [1420/1632] time 0.040 (0.041) data 0.000 (0.000) loss 1.9736 (1.4960) lr 2.0000e-03 eta 0:09:10\n",
            "epoch [2/10] batch [1440/1632] time 0.041 (0.041) data 0.000 (0.000) loss 0.0763 (1.4897) lr 2.0000e-03 eta 0:09:09\n",
            "epoch [2/10] batch [1460/1632] time 0.042 (0.041) data 0.000 (0.000) loss 2.4297 (1.4904) lr 2.0000e-03 eta 0:09:08\n",
            "epoch [2/10] batch [1480/1632] time 0.041 (0.041) data 0.000 (0.000) loss 5.6094 (1.4868) lr 2.0000e-03 eta 0:09:07\n",
            "epoch [2/10] batch [1500/1632] time 0.041 (0.041) data 0.000 (0.000) loss 6.2188 (1.4901) lr 2.0000e-03 eta 0:09:07\n",
            "epoch [2/10] batch [1520/1632] time 0.041 (0.041) data 0.000 (0.000) loss 2.6582 (1.4848) lr 2.0000e-03 eta 0:09:06\n",
            "epoch [2/10] batch [1540/1632] time 0.042 (0.041) data 0.000 (0.000) loss 1.1562 (1.4788) lr 2.0000e-03 eta 0:09:05\n",
            "epoch [2/10] batch [1560/1632] time 0.040 (0.041) data 0.000 (0.000) loss 0.6162 (1.4724) lr 2.0000e-03 eta 0:09:04\n",
            "epoch [2/10] batch [1580/1632] time 0.042 (0.041) data 0.000 (0.000) loss 0.0350 (1.4733) lr 2.0000e-03 eta 0:09:03\n",
            "epoch [2/10] batch [1600/1632] time 0.041 (0.041) data 0.000 (0.000) loss 1.3047 (1.4747) lr 2.0000e-03 eta 0:09:02\n",
            "epoch [2/10] batch [1620/1632] time 0.039 (0.041) data 0.000 (0.000) loss 0.0829 (1.4739) lr 2.0000e-03 eta 0:09:02\n",
            "epoch [3/10] batch [20/1632] time 0.041 (0.056) data 0.000 (0.013) loss 0.9844 (1.0629) lr 1.9511e-03 eta 0:12:07\n",
            "epoch [3/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.007) loss 1.6719 (1.1751) lr 1.9511e-03 eta 0:10:31\n",
            "epoch [3/10] batch [60/1632] time 0.042 (0.046) data 0.000 (0.005) loss 0.0923 (1.2160) lr 1.9511e-03 eta 0:09:58\n",
            "epoch [3/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.9194 (1.2191) lr 1.9511e-03 eta 0:09:42\n",
            "epoch [3/10] batch [100/1632] time 0.040 (0.044) data 0.000 (0.003) loss 4.5742 (1.1793) lr 1.9511e-03 eta 0:09:31\n",
            "epoch [3/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 2.5938 (1.2159) lr 1.9511e-03 eta 0:09:24\n",
            "epoch [3/10] batch [140/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.5864 (1.1962) lr 1.9511e-03 eta 0:09:18\n",
            "epoch [3/10] batch [160/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.0031 (1.1643) lr 1.9511e-03 eta 0:09:14\n",
            "epoch [3/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 3.2812 (1.2122) lr 1.9511e-03 eta 0:09:10\n",
            "epoch [3/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.1786 (1.2667) lr 1.9511e-03 eta 0:09:07\n",
            "epoch [3/10] batch [220/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.5342 (1.2899) lr 1.9511e-03 eta 0:09:04\n",
            "epoch [3/10] batch [240/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.2578 (1.2698) lr 1.9511e-03 eta 0:09:02\n",
            "epoch [3/10] batch [260/1632] time 0.043 (0.042) data 0.000 (0.001) loss 5.2930 (1.2798) lr 1.9511e-03 eta 0:09:01\n",
            "epoch [3/10] batch [280/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0008 (1.2869) lr 1.9511e-03 eta 0:08:59\n",
            "epoch [3/10] batch [300/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.0342 (1.2579) lr 1.9511e-03 eta 0:08:58\n",
            "epoch [3/10] batch [320/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.6924 (1.2870) lr 1.9511e-03 eta 0:08:56\n",
            "epoch [3/10] batch [340/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0427 (1.2798) lr 1.9511e-03 eta 0:08:56\n",
            "epoch [3/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2705 (1.2556) lr 1.9511e-03 eta 0:08:56\n",
            "epoch [3/10] batch [380/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1118 (1.2284) lr 1.9511e-03 eta 0:08:55\n",
            "epoch [3/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0117 (1.2409) lr 1.9511e-03 eta 0:08:54\n",
            "epoch [3/10] batch [420/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1635 (1.2272) lr 1.9511e-03 eta 0:08:53\n",
            "epoch [3/10] batch [440/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1877 (1.2154) lr 1.9511e-03 eta 0:08:52\n",
            "epoch [3/10] batch [460/1632] time 0.041 (0.042) data 0.000 (0.001) loss 3.1484 (1.2173) lr 1.9511e-03 eta 0:08:51\n",
            "epoch [3/10] batch [480/1632] time 0.045 (0.042) data 0.000 (0.001) loss 2.6523 (1.2359) lr 1.9511e-03 eta 0:08:51\n",
            "epoch [3/10] batch [500/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.6934 (1.2471) lr 1.9511e-03 eta 0:08:50\n",
            "epoch [3/10] batch [520/1632] time 0.046 (0.042) data 0.000 (0.001) loss 1.4795 (1.2370) lr 1.9511e-03 eta 0:08:49\n",
            "epoch [3/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.0098 (1.2226) lr 1.9511e-03 eta 0:08:48\n",
            "epoch [3/10] batch [560/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.2715 (1.2043) lr 1.9511e-03 eta 0:08:47\n",
            "epoch [3/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3118 (1.2073) lr 1.9511e-03 eta 0:08:46\n",
            "epoch [3/10] batch [600/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.1855 (1.2085) lr 1.9511e-03 eta 0:08:46\n",
            "epoch [3/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.1113 (1.2113) lr 1.9511e-03 eta 0:08:45\n",
            "epoch [3/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0369 (1.2070) lr 1.9511e-03 eta 0:08:45\n",
            "epoch [3/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0148 (1.1985) lr 1.9511e-03 eta 0:08:44\n",
            "epoch [3/10] batch [680/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.9238 (1.2027) lr 1.9511e-03 eta 0:08:43\n",
            "epoch [3/10] batch [700/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0017 (1.1790) lr 1.9511e-03 eta 0:08:42\n",
            "epoch [3/10] batch [720/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3936 (1.1749) lr 1.9511e-03 eta 0:08:42\n",
            "epoch [3/10] batch [740/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.7764 (1.1917) lr 1.9511e-03 eta 0:08:41\n",
            "epoch [3/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.4077 (1.1769) lr 1.9511e-03 eta 0:08:40\n",
            "epoch [3/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.0312 (1.1721) lr 1.9511e-03 eta 0:08:39\n",
            "epoch [3/10] batch [800/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0313 (1.1676) lr 1.9511e-03 eta 0:08:37\n",
            "epoch [3/10] batch [820/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.2130 (1.1565) lr 1.9511e-03 eta 0:08:36\n",
            "epoch [3/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8335 (1.1525) lr 1.9511e-03 eta 0:08:35\n",
            "epoch [3/10] batch [860/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0210 (1.1543) lr 1.9511e-03 eta 0:08:34\n",
            "epoch [3/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1917 (1.1517) lr 1.9511e-03 eta 0:08:32\n",
            "epoch [3/10] batch [900/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0198 (1.1500) lr 1.9511e-03 eta 0:08:31\n",
            "epoch [3/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4541 (1.1375) lr 1.9511e-03 eta 0:08:30\n",
            "epoch [3/10] batch [940/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.1648 (1.1438) lr 1.9511e-03 eta 0:08:29\n",
            "epoch [3/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0568 (1.1427) lr 1.9511e-03 eta 0:08:28\n",
            "epoch [3/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 3.3125 (1.1431) lr 1.9511e-03 eta 0:08:27\n",
            "epoch [3/10] batch [1000/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2888 (1.1334) lr 1.9511e-03 eta 0:08:26\n",
            "epoch [3/10] batch [1020/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0187 (1.1302) lr 1.9511e-03 eta 0:08:26\n",
            "epoch [3/10] batch [1040/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4072 (1.1251) lr 1.9511e-03 eta 0:08:24\n",
            "epoch [3/10] batch [1060/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.5552 (1.1457) lr 1.9511e-03 eta 0:08:24\n",
            "epoch [3/10] batch [1080/1632] time 0.044 (0.042) data 0.000 (0.000) loss 1.5283 (1.1380) lr 1.9511e-03 eta 0:08:22\n",
            "epoch [3/10] batch [1100/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2118 (1.1317) lr 1.9511e-03 eta 0:08:21\n",
            "epoch [3/10] batch [1120/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.4590 (1.1235) lr 1.9511e-03 eta 0:08:20\n",
            "epoch [3/10] batch [1140/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2634 (1.1304) lr 1.9511e-03 eta 0:08:19\n",
            "epoch [3/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0135 (1.1320) lr 1.9511e-03 eta 0:08:18\n",
            "epoch [3/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.5059 (1.1324) lr 1.9511e-03 eta 0:08:17\n",
            "epoch [3/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.9355 (1.1225) lr 1.9511e-03 eta 0:08:16\n",
            "epoch [3/10] batch [1220/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.8936 (1.1160) lr 1.9511e-03 eta 0:08:15\n",
            "epoch [3/10] batch [1240/1632] time 0.045 (0.042) data 0.000 (0.000) loss 1.0615 (1.1173) lr 1.9511e-03 eta 0:08:14\n",
            "epoch [3/10] batch [1260/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.1064 (1.1241) lr 1.9511e-03 eta 0:08:13\n",
            "epoch [3/10] batch [1280/1632] time 0.042 (0.042) data 0.001 (0.000) loss 1.1064 (1.1187) lr 1.9511e-03 eta 0:08:13\n",
            "epoch [3/10] batch [1300/1632] time 0.043 (0.042) data 0.000 (0.000) loss 3.4180 (1.1168) lr 1.9511e-03 eta 0:08:12\n",
            "epoch [3/10] batch [1320/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0143 (1.1076) lr 1.9511e-03 eta 0:08:11\n",
            "epoch [3/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.5645 (1.1074) lr 1.9511e-03 eta 0:08:10\n",
            "epoch [3/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.1064 (1.0991) lr 1.9511e-03 eta 0:08:09\n",
            "epoch [3/10] batch [1380/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.7344 (1.0978) lr 1.9511e-03 eta 0:08:09\n",
            "epoch [3/10] batch [1400/1632] time 0.043 (0.042) data 0.000 (0.000) loss 3.7012 (1.0957) lr 1.9511e-03 eta 0:08:08\n",
            "epoch [3/10] batch [1420/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.7471 (1.0911) lr 1.9511e-03 eta 0:08:07\n",
            "epoch [3/10] batch [1440/1632] time 0.043 (0.042) data 0.000 (0.000) loss 2.6289 (1.0909) lr 1.9511e-03 eta 0:08:06\n",
            "epoch [3/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3101 (1.0899) lr 1.9511e-03 eta 0:08:05\n",
            "epoch [3/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1406 (1.0911) lr 1.9511e-03 eta 0:08:04\n",
            "epoch [3/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.9590 (1.0888) lr 1.9511e-03 eta 0:08:03\n",
            "epoch [3/10] batch [1520/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0299 (1.0847) lr 1.9511e-03 eta 0:08:03\n",
            "epoch [3/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0569 (1.0876) lr 1.9511e-03 eta 0:08:02\n",
            "epoch [3/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6128 (1.0838) lr 1.9511e-03 eta 0:08:01\n",
            "epoch [3/10] batch [1580/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1061 (1.0808) lr 1.9511e-03 eta 0:08:00\n",
            "epoch [3/10] batch [1600/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.5723 (1.0800) lr 1.9511e-03 eta 0:07:59\n",
            "epoch [3/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 2.0977 (1.0765) lr 1.9511e-03 eta 0:07:58\n",
            "epoch [4/10] batch [20/1632] time 0.042 (0.056) data 0.000 (0.013) loss 3.1211 (1.1785) lr 1.8090e-03 eta 0:10:39\n",
            "epoch [4/10] batch [40/1632] time 0.041 (0.049) data 0.000 (0.007) loss 0.4175 (1.0105) lr 1.8090e-03 eta 0:09:15\n",
            "epoch [4/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.005) loss 2.4902 (0.9980) lr 1.8090e-03 eta 0:08:47\n",
            "epoch [4/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.004) loss 0.3379 (1.0248) lr 1.8090e-03 eta 0:08:31\n",
            "epoch [4/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 1.6875 (0.9928) lr 1.8090e-03 eta 0:08:22\n",
            "epoch [4/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.1970 (1.0084) lr 1.8090e-03 eta 0:08:15\n",
            "epoch [4/10] batch [140/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.8203 (1.0147) lr 1.8090e-03 eta 0:08:11\n",
            "epoch [4/10] batch [160/1632] time 0.042 (0.043) data 0.000 (0.002) loss 2.3848 (1.0274) lr 1.8090e-03 eta 0:08:07\n",
            "epoch [4/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 2.6250 (1.0058) lr 1.8090e-03 eta 0:08:04\n",
            "epoch [4/10] batch [200/1632] time 0.043 (0.043) data 0.000 (0.002) loss 0.2559 (1.0158) lr 1.8090e-03 eta 0:08:02\n",
            "epoch [4/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.2949 (1.0439) lr 1.8090e-03 eta 0:08:00\n",
            "epoch [4/10] batch [240/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.3184 (1.0563) lr 1.8090e-03 eta 0:07:58\n",
            "epoch [4/10] batch [260/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0940 (1.0517) lr 1.8090e-03 eta 0:07:57\n",
            "epoch [4/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0900 (1.0369) lr 1.8090e-03 eta 0:07:55\n",
            "epoch [4/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.1064 (1.0409) lr 1.8090e-03 eta 0:07:54\n",
            "epoch [4/10] batch [320/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0892 (1.0448) lr 1.8090e-03 eta 0:07:53\n",
            "epoch [4/10] batch [340/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0419 (1.0117) lr 1.8090e-03 eta 0:07:51\n",
            "epoch [4/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0439 (0.9910) lr 1.8090e-03 eta 0:07:50\n",
            "epoch [4/10] batch [380/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0591 (0.9768) lr 1.8090e-03 eta 0:07:48\n",
            "epoch [4/10] batch [400/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0095 (0.9496) lr 1.8090e-03 eta 0:07:47\n",
            "epoch [4/10] batch [420/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.8638 (0.9491) lr 1.8090e-03 eta 0:07:46\n",
            "epoch [4/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0105 (0.9413) lr 1.8090e-03 eta 0:07:45\n",
            "epoch [4/10] batch [460/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5249 (0.9696) lr 1.8090e-03 eta 0:07:44\n",
            "epoch [4/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0166 (0.9882) lr 1.8090e-03 eta 0:07:43\n",
            "epoch [4/10] batch [500/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.0107 (0.9611) lr 1.8090e-03 eta 0:07:42\n",
            "epoch [4/10] batch [520/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0215 (0.9394) lr 1.8090e-03 eta 0:07:42\n",
            "epoch [4/10] batch [540/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0170 (0.9258) lr 1.8090e-03 eta 0:07:42\n",
            "epoch [4/10] batch [560/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0558 (0.9155) lr 1.8090e-03 eta 0:07:40\n",
            "epoch [4/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3523 (0.9161) lr 1.8090e-03 eta 0:07:39\n",
            "epoch [4/10] batch [600/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2581 (0.9272) lr 1.8090e-03 eta 0:07:37\n",
            "epoch [4/10] batch [620/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1692 (0.9146) lr 1.8090e-03 eta 0:07:36\n",
            "epoch [4/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0065 (0.9241) lr 1.8090e-03 eta 0:07:34\n",
            "epoch [4/10] batch [660/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.7461 (0.9345) lr 1.8090e-03 eta 0:07:33\n",
            "epoch [4/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0939 (0.9379) lr 1.8090e-03 eta 0:07:32\n",
            "epoch [4/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.3164 (0.9344) lr 1.8090e-03 eta 0:07:31\n",
            "epoch [4/10] batch [720/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0559 (0.9372) lr 1.8090e-03 eta 0:07:30\n",
            "epoch [4/10] batch [740/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.1250 (0.9550) lr 1.8090e-03 eta 0:07:30\n",
            "epoch [4/10] batch [760/1632] time 0.045 (0.042) data 0.000 (0.001) loss 5.7461 (0.9585) lr 1.8090e-03 eta 0:07:28\n",
            "epoch [4/10] batch [780/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.0645 (0.9631) lr 1.8090e-03 eta 0:07:27\n",
            "epoch [4/10] batch [800/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0237 (0.9675) lr 1.8090e-03 eta 0:07:26\n",
            "epoch [4/10] batch [820/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.6924 (0.9745) lr 1.8090e-03 eta 0:07:25\n",
            "epoch [4/10] batch [840/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.2930 (0.9741) lr 1.8090e-03 eta 0:07:24\n",
            "epoch [4/10] batch [860/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.3945 (0.9629) lr 1.8090e-03 eta 0:07:23\n",
            "epoch [4/10] batch [880/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.3225 (0.9635) lr 1.8090e-03 eta 0:07:22\n",
            "epoch [4/10] batch [900/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.6904 (0.9682) lr 1.8090e-03 eta 0:07:21\n",
            "epoch [4/10] batch [920/1632] time 0.045 (0.042) data 0.000 (0.001) loss 1.3955 (0.9670) lr 1.8090e-03 eta 0:07:20\n",
            "epoch [4/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.2207 (0.9608) lr 1.8090e-03 eta 0:07:19\n",
            "epoch [4/10] batch [960/1632] time 0.040 (0.042) data 0.000 (0.001) loss 4.9414 (0.9933) lr 1.8090e-03 eta 0:07:18\n",
            "epoch [4/10] batch [980/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0715 (0.9847) lr 1.8090e-03 eta 0:07:17\n",
            "epoch [4/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3047 (0.9867) lr 1.8090e-03 eta 0:07:16\n",
            "epoch [4/10] batch [1020/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.3633 (0.9852) lr 1.8090e-03 eta 0:07:15\n",
            "epoch [4/10] batch [1040/1632] time 0.042 (0.042) data 0.000 (0.001) loss 4.9883 (0.9873) lr 1.8090e-03 eta 0:07:14\n",
            "epoch [4/10] batch [1060/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2974 (0.9786) lr 1.8090e-03 eta 0:07:13\n",
            "epoch [4/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.001) loss 7.5859 (0.9877) lr 1.8090e-03 eta 0:07:13\n",
            "epoch [4/10] batch [1100/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5298 (0.9925) lr 1.8090e-03 eta 0:07:12\n",
            "epoch [4/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1108 (0.9952) lr 1.8090e-03 eta 0:07:11\n",
            "epoch [4/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4441 (0.9937) lr 1.8090e-03 eta 0:07:10\n",
            "epoch [4/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1174 (0.9966) lr 1.8090e-03 eta 0:07:09\n",
            "epoch [4/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.0273 (0.9914) lr 1.8090e-03 eta 0:07:08\n",
            "epoch [4/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.7129 (0.9901) lr 1.8090e-03 eta 0:07:07\n",
            "epoch [4/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0040 (0.9894) lr 1.8090e-03 eta 0:07:06\n",
            "epoch [4/10] batch [1240/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0786 (0.9910) lr 1.8090e-03 eta 0:07:05\n",
            "epoch [4/10] batch [1260/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0391 (0.9882) lr 1.8090e-03 eta 0:07:04\n",
            "epoch [4/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5273 (0.9940) lr 1.8090e-03 eta 0:07:03\n",
            "epoch [4/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1770 (0.9861) lr 1.8090e-03 eta 0:07:02\n",
            "epoch [4/10] batch [1320/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.7500 (0.9785) lr 1.8090e-03 eta 0:07:01\n",
            "epoch [4/10] batch [1340/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.4453 (0.9797) lr 1.8090e-03 eta 0:07:01\n",
            "epoch [4/10] batch [1360/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1192 (0.9807) lr 1.8090e-03 eta 0:07:00\n",
            "epoch [4/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2351 (0.9748) lr 1.8090e-03 eta 0:06:59\n",
            "epoch [4/10] batch [1400/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0006 (0.9714) lr 1.8090e-03 eta 0:06:58\n",
            "epoch [4/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0648 (0.9739) lr 1.8090e-03 eta 0:06:57\n",
            "epoch [4/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2450 (0.9740) lr 1.8090e-03 eta 0:06:56\n",
            "epoch [4/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2378 (0.9730) lr 1.8090e-03 eta 0:06:55\n",
            "epoch [4/10] batch [1480/1632] time 0.043 (0.042) data 0.000 (0.000) loss 2.1816 (0.9841) lr 1.8090e-03 eta 0:06:54\n",
            "epoch [4/10] batch [1500/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0456 (0.9820) lr 1.8090e-03 eta 0:06:54\n",
            "epoch [4/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.8799 (0.9775) lr 1.8090e-03 eta 0:06:53\n",
            "epoch [4/10] batch [1540/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.1547 (0.9727) lr 1.8090e-03 eta 0:06:52\n",
            "epoch [4/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0348 (0.9740) lr 1.8090e-03 eta 0:06:52\n",
            "epoch [4/10] batch [1580/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1338 (0.9741) lr 1.8090e-03 eta 0:06:51\n",
            "epoch [4/10] batch [1600/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.7656 (0.9778) lr 1.8090e-03 eta 0:06:50\n",
            "epoch [4/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.2043 (0.9812) lr 1.8090e-03 eta 0:06:49\n",
            "epoch [5/10] batch [20/1632] time 0.046 (0.055) data 0.000 (0.012) loss 0.1628 (0.9604) lr 1.5878e-03 eta 0:08:58\n",
            "epoch [5/10] batch [40/1632] time 0.040 (0.049) data 0.000 (0.006) loss 0.1816 (0.8673) lr 1.5878e-03 eta 0:07:53\n",
            "epoch [5/10] batch [60/1632] time 0.042 (0.047) data 0.000 (0.004) loss 0.5571 (0.7247) lr 1.5878e-03 eta 0:07:34\n",
            "epoch [5/10] batch [80/1632] time 0.043 (0.046) data 0.000 (0.003) loss 2.5020 (0.8273) lr 1.5878e-03 eta 0:07:23\n",
            "epoch [5/10] batch [100/1632] time 0.043 (0.045) data 0.000 (0.003) loss 4.6055 (0.8733) lr 1.5878e-03 eta 0:07:16\n",
            "epoch [5/10] batch [120/1632] time 0.042 (0.045) data 0.000 (0.002) loss 0.5420 (0.9158) lr 1.5878e-03 eta 0:07:10\n",
            "epoch [5/10] batch [140/1632] time 0.041 (0.044) data 0.000 (0.002) loss 1.1934 (0.9280) lr 1.5878e-03 eta 0:07:05\n",
            "epoch [5/10] batch [160/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0611 (0.9235) lr 1.5878e-03 eta 0:07:01\n",
            "epoch [5/10] batch [180/1632] time 0.042 (0.044) data 0.000 (0.002) loss 0.2416 (0.8938) lr 1.5878e-03 eta 0:06:58\n",
            "epoch [5/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2854 (0.8763) lr 1.5878e-03 eta 0:06:55\n",
            "epoch [5/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0249 (0.8793) lr 1.5878e-03 eta 0:06:52\n",
            "epoch [5/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 3.3105 (0.9071) lr 1.5878e-03 eta 0:06:50\n",
            "epoch [5/10] batch [260/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0880 (0.8999) lr 1.5878e-03 eta 0:06:47\n",
            "epoch [5/10] batch [280/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0522 (0.8882) lr 1.5878e-03 eta 0:06:45\n",
            "epoch [5/10] batch [300/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0190 (0.8709) lr 1.5878e-03 eta 0:06:43\n",
            "epoch [5/10] batch [320/1632] time 0.040 (0.042) data 0.000 (0.001) loss 4.0430 (0.8788) lr 1.5878e-03 eta 0:06:41\n",
            "epoch [5/10] batch [340/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0279 (0.8508) lr 1.5878e-03 eta 0:06:39\n",
            "epoch [5/10] batch [360/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.9082 (0.8307) lr 1.5878e-03 eta 0:06:39\n",
            "epoch [5/10] batch [380/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.9048 (0.8174) lr 1.5878e-03 eta 0:06:39\n",
            "epoch [5/10] batch [400/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0130 (0.8261) lr 1.5878e-03 eta 0:06:39\n",
            "epoch [5/10] batch [420/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0337 (0.8203) lr 1.5878e-03 eta 0:06:37\n",
            "epoch [5/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1488 (0.8040) lr 1.5878e-03 eta 0:06:36\n",
            "epoch [5/10] batch [460/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.1602 (0.8071) lr 1.5878e-03 eta 0:06:35\n",
            "epoch [5/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1218 (0.7917) lr 1.5878e-03 eta 0:06:33\n",
            "epoch [5/10] batch [500/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1057 (0.7927) lr 1.5878e-03 eta 0:06:32\n",
            "epoch [5/10] batch [520/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.1445 (0.8249) lr 1.5878e-03 eta 0:06:30\n",
            "epoch [5/10] batch [540/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1172 (0.8311) lr 1.5878e-03 eta 0:06:29\n",
            "epoch [5/10] batch [560/1632] time 0.039 (0.042) data 0.000 (0.001) loss 0.0280 (0.8274) lr 1.5878e-03 eta 0:06:28\n",
            "epoch [5/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1058 (0.8302) lr 1.5878e-03 eta 0:06:26\n",
            "epoch [5/10] batch [600/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.5977 (0.8272) lr 1.5878e-03 eta 0:06:25\n",
            "epoch [5/10] batch [620/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.6445 (0.8230) lr 1.5878e-03 eta 0:06:24\n",
            "epoch [5/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0395 (0.8125) lr 1.5878e-03 eta 0:06:23\n",
            "epoch [5/10] batch [660/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.4668 (0.8140) lr 1.5878e-03 eta 0:06:22\n",
            "epoch [5/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0275 (0.8168) lr 1.5878e-03 eta 0:06:21\n",
            "epoch [5/10] batch [700/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.3599 (0.8072) lr 1.5878e-03 eta 0:06:20\n",
            "epoch [5/10] batch [720/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.3337 (0.8151) lr 1.5878e-03 eta 0:06:19\n",
            "epoch [5/10] batch [740/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0031 (0.8153) lr 1.5878e-03 eta 0:06:18\n",
            "epoch [5/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.2405 (0.8172) lr 1.5878e-03 eta 0:06:17\n",
            "epoch [5/10] batch [780/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.2100 (0.8326) lr 1.5878e-03 eta 0:06:16\n",
            "epoch [5/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.3350 (0.8288) lr 1.5878e-03 eta 0:06:15\n",
            "epoch [5/10] batch [820/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.4368 (0.8338) lr 1.5878e-03 eta 0:06:14\n",
            "epoch [5/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.9702 (0.8354) lr 1.5878e-03 eta 0:06:13\n",
            "epoch [5/10] batch [860/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0040 (0.8288) lr 1.5878e-03 eta 0:06:12\n",
            "epoch [5/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0021 (0.8205) lr 1.5878e-03 eta 0:06:11\n",
            "epoch [5/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.8438 (0.8288) lr 1.5878e-03 eta 0:06:10\n",
            "epoch [5/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0437 (0.8310) lr 1.5878e-03 eta 0:06:10\n",
            "epoch [5/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4658 (0.8319) lr 1.5878e-03 eta 0:06:09\n",
            "epoch [5/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1404 (0.8307) lr 1.5878e-03 eta 0:06:08\n",
            "epoch [5/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3560 (0.8364) lr 1.5878e-03 eta 0:06:07\n",
            "epoch [5/10] batch [1000/1632] time 0.046 (0.042) data 0.000 (0.000) loss 1.1211 (0.8412) lr 1.5878e-03 eta 0:06:06\n",
            "epoch [5/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4626 (0.8373) lr 1.5878e-03 eta 0:06:06\n",
            "epoch [5/10] batch [1040/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.5342 (0.8373) lr 1.5878e-03 eta 0:06:05\n",
            "epoch [5/10] batch [1060/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.1514 (0.8348) lr 1.5878e-03 eta 0:06:04\n",
            "epoch [5/10] batch [1080/1632] time 0.047 (0.042) data 0.000 (0.000) loss 0.5127 (0.8351) lr 1.5878e-03 eta 0:06:04\n",
            "epoch [5/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1098 (0.8298) lr 1.5878e-03 eta 0:06:03\n",
            "epoch [5/10] batch [1120/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.8027 (0.8421) lr 1.5878e-03 eta 0:06:02\n",
            "epoch [5/10] batch [1140/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0068 (0.8414) lr 1.5878e-03 eta 0:06:01\n",
            "epoch [5/10] batch [1160/1632] time 0.041 (0.042) data 0.000 (0.000) loss 4.5586 (0.8479) lr 1.5878e-03 eta 0:06:00\n",
            "epoch [5/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0131 (0.8401) lr 1.5878e-03 eta 0:05:59\n",
            "epoch [5/10] batch [1200/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.8350 (0.8548) lr 1.5878e-03 eta 0:05:58\n",
            "epoch [5/10] batch [1220/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.7871 (0.8527) lr 1.5878e-03 eta 0:05:58\n",
            "epoch [5/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0605 (0.8534) lr 1.5878e-03 eta 0:05:57\n",
            "epoch [5/10] batch [1260/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0157 (0.8501) lr 1.5878e-03 eta 0:05:56\n",
            "epoch [5/10] batch [1280/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.7588 (0.8480) lr 1.5878e-03 eta 0:05:55\n",
            "epoch [5/10] batch [1300/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2805 (0.8461) lr 1.5878e-03 eta 0:05:54\n",
            "epoch [5/10] batch [1320/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2247 (0.8426) lr 1.5878e-03 eta 0:05:53\n",
            "epoch [5/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 5.3125 (0.8489) lr 1.5878e-03 eta 0:05:52\n",
            "epoch [5/10] batch [1360/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.7285 (0.8466) lr 1.5878e-03 eta 0:05:51\n",
            "epoch [5/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.0391 (0.8514) lr 1.5878e-03 eta 0:05:51\n",
            "epoch [5/10] batch [1400/1632] time 0.039 (0.042) data 0.000 (0.000) loss 1.3633 (0.8522) lr 1.5878e-03 eta 0:05:50\n",
            "epoch [5/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4639 (0.8517) lr 1.5878e-03 eta 0:05:49\n",
            "epoch [5/10] batch [1440/1632] time 0.048 (0.042) data 0.000 (0.000) loss 0.6421 (0.8510) lr 1.5878e-03 eta 0:05:48\n",
            "epoch [5/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.8164 (0.8460) lr 1.5878e-03 eta 0:05:48\n",
            "epoch [5/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9062 (0.8403) lr 1.5878e-03 eta 0:05:47\n",
            "epoch [5/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.7812 (0.8401) lr 1.5878e-03 eta 0:05:46\n",
            "epoch [5/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5527 (0.8370) lr 1.5878e-03 eta 0:05:45\n",
            "epoch [5/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1211 (0.8394) lr 1.5878e-03 eta 0:05:44\n",
            "epoch [5/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0081 (0.8426) lr 1.5878e-03 eta 0:05:43\n",
            "epoch [5/10] batch [1580/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.7583 (0.8462) lr 1.5878e-03 eta 0:05:42\n",
            "epoch [5/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4495 (0.8483) lr 1.5878e-03 eta 0:05:41\n",
            "epoch [5/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.2212 (0.8419) lr 1.5878e-03 eta 0:05:40\n",
            "epoch [6/10] batch [20/1632] time 0.040 (0.056) data 0.000 (0.013) loss 0.0751 (1.0017) lr 1.3090e-03 eta 0:07:34\n",
            "epoch [6/10] batch [40/1632] time 0.041 (0.049) data 0.000 (0.007) loss 2.8320 (1.0207) lr 1.3090e-03 eta 0:06:34\n",
            "epoch [6/10] batch [60/1632] time 0.041 (0.047) data 0.000 (0.005) loss 0.0497 (0.9855) lr 1.3090e-03 eta 0:06:17\n",
            "epoch [6/10] batch [80/1632] time 0.041 (0.046) data 0.000 (0.003) loss 0.0433 (1.0045) lr 1.3090e-03 eta 0:06:08\n",
            "epoch [6/10] batch [100/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.9414 (1.0177) lr 1.3090e-03 eta 0:06:01\n",
            "epoch [6/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 1.3691 (0.9722) lr 1.3090e-03 eta 0:05:56\n",
            "epoch [6/10] batch [140/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.8457 (0.9459) lr 1.3090e-03 eta 0:05:52\n",
            "epoch [6/10] batch [160/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.6851 (0.9501) lr 1.3090e-03 eta 0:05:48\n",
            "epoch [6/10] batch [180/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.0002 (0.9820) lr 1.3090e-03 eta 0:05:45\n",
            "epoch [6/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.1192 (0.9486) lr 1.3090e-03 eta 0:05:42\n",
            "epoch [6/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.1357 (0.9513) lr 1.3090e-03 eta 0:05:42\n",
            "epoch [6/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.2037 (0.9420) lr 1.3090e-03 eta 0:05:40\n",
            "epoch [6/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0165 (0.9856) lr 1.3090e-03 eta 0:05:37\n",
            "epoch [6/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.3298 (0.9853) lr 1.3090e-03 eta 0:05:36\n",
            "epoch [6/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0003 (1.0262) lr 1.3090e-03 eta 0:05:34\n",
            "epoch [6/10] batch [320/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0876 (0.9969) lr 1.3090e-03 eta 0:05:32\n",
            "epoch [6/10] batch [340/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0700 (0.9815) lr 1.3090e-03 eta 0:05:32\n",
            "epoch [6/10] batch [360/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.6436 (0.9802) lr 1.3090e-03 eta 0:05:31\n",
            "epoch [6/10] batch [380/1632] time 0.043 (0.043) data 0.000 (0.001) loss 8.1719 (0.9856) lr 1.3090e-03 eta 0:05:30\n",
            "epoch [6/10] batch [400/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.0561 (0.9669) lr 1.3090e-03 eta 0:05:30\n",
            "epoch [6/10] batch [420/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1279 (0.9455) lr 1.3090e-03 eta 0:05:29\n",
            "epoch [6/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1826 (0.9209) lr 1.3090e-03 eta 0:05:28\n",
            "epoch [6/10] batch [460/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0524 (0.9212) lr 1.3090e-03 eta 0:05:27\n",
            "epoch [6/10] batch [480/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.2150 (0.8955) lr 1.3090e-03 eta 0:05:26\n",
            "epoch [6/10] batch [500/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3679 (0.8925) lr 1.3090e-03 eta 0:05:25\n",
            "epoch [6/10] batch [520/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2428 (0.8831) lr 1.3090e-03 eta 0:05:24\n",
            "epoch [6/10] batch [540/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.9668 (0.8817) lr 1.3090e-03 eta 0:05:23\n",
            "epoch [6/10] batch [560/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0963 (0.8702) lr 1.3090e-03 eta 0:05:22\n",
            "epoch [6/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7485 (0.8557) lr 1.3090e-03 eta 0:05:20\n",
            "epoch [6/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0378 (0.8583) lr 1.3090e-03 eta 0:05:19\n",
            "epoch [6/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1213 (0.8599) lr 1.3090e-03 eta 0:05:18\n",
            "epoch [6/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0732 (0.8643) lr 1.3090e-03 eta 0:05:17\n",
            "epoch [6/10] batch [660/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0304 (0.8543) lr 1.3090e-03 eta 0:05:16\n",
            "epoch [6/10] batch [680/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2031 (0.8436) lr 1.3090e-03 eta 0:05:15\n",
            "epoch [6/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0540 (0.8336) lr 1.3090e-03 eta 0:05:14\n",
            "epoch [6/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2053 (0.8308) lr 1.3090e-03 eta 0:05:13\n",
            "epoch [6/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1063 (0.8330) lr 1.3090e-03 eta 0:05:12\n",
            "epoch [6/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0874 (0.8345) lr 1.3090e-03 eta 0:05:11\n",
            "epoch [6/10] batch [780/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0504 (0.8332) lr 1.3090e-03 eta 0:05:10\n",
            "epoch [6/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0104 (0.8193) lr 1.3090e-03 eta 0:05:09\n",
            "epoch [6/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6479 (0.8097) lr 1.3090e-03 eta 0:05:08\n",
            "epoch [6/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7134 (0.8006) lr 1.3090e-03 eta 0:05:07\n",
            "epoch [6/10] batch [860/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.2168 (0.7979) lr 1.3090e-03 eta 0:05:06\n",
            "epoch [6/10] batch [880/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.0303 (0.7882) lr 1.3090e-03 eta 0:05:05\n",
            "epoch [6/10] batch [900/1632] time 0.042 (0.042) data 0.000 (0.001) loss 5.2188 (0.7869) lr 1.3090e-03 eta 0:05:05\n",
            "epoch [6/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.6934 (0.7951) lr 1.3090e-03 eta 0:05:04\n",
            "epoch [6/10] batch [940/1632] time 0.040 (0.042) data 0.000 (0.001) loss 2.1426 (0.8033) lr 1.3090e-03 eta 0:05:03\n",
            "epoch [6/10] batch [960/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.8579 (0.8046) lr 1.3090e-03 eta 0:05:02\n",
            "epoch [6/10] batch [980/1632] time 0.043 (0.042) data 0.000 (0.001) loss 3.2188 (0.8049) lr 1.3090e-03 eta 0:05:01\n",
            "epoch [6/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.9111 (0.8101) lr 1.3090e-03 eta 0:05:00\n",
            "epoch [6/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0187 (0.8125) lr 1.3090e-03 eta 0:04:59\n",
            "epoch [6/10] batch [1040/1632] time 0.041 (0.042) data 0.000 (0.001) loss 8.3594 (0.8184) lr 1.3090e-03 eta 0:04:59\n",
            "epoch [6/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.0059 (0.8146) lr 1.3090e-03 eta 0:04:58\n",
            "epoch [6/10] batch [1080/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0340 (0.8180) lr 1.3090e-03 eta 0:04:57\n",
            "epoch [6/10] batch [1100/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.0410 (0.8306) lr 1.3090e-03 eta 0:04:56\n",
            "epoch [6/10] batch [1120/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0045 (0.8293) lr 1.3090e-03 eta 0:04:55\n",
            "epoch [6/10] batch [1140/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.5840 (0.8317) lr 1.3090e-03 eta 0:04:54\n",
            "epoch [6/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.5693 (0.8279) lr 1.3090e-03 eta 0:04:54\n",
            "epoch [6/10] batch [1180/1632] time 0.041 (0.042) data 0.000 (0.000) loss 3.5547 (0.8297) lr 1.3090e-03 eta 0:04:53\n",
            "epoch [6/10] batch [1200/1632] time 0.040 (0.042) data 0.000 (0.000) loss 1.0068 (0.8281) lr 1.3090e-03 eta 0:04:52\n",
            "epoch [6/10] batch [1220/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1536 (0.8356) lr 1.3090e-03 eta 0:04:51\n",
            "epoch [6/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2059 (0.8361) lr 1.3090e-03 eta 0:04:50\n",
            "epoch [6/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2458 (0.8380) lr 1.3090e-03 eta 0:04:49\n",
            "epoch [6/10] batch [1280/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.3103 (0.8496) lr 1.3090e-03 eta 0:04:48\n",
            "epoch [6/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3228 (0.8475) lr 1.3090e-03 eta 0:04:47\n",
            "epoch [6/10] batch [1320/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.6465 (0.8479) lr 1.3090e-03 eta 0:04:46\n",
            "epoch [6/10] batch [1340/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0341 (0.8497) lr 1.3090e-03 eta 0:04:45\n",
            "epoch [6/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1368 (0.8444) lr 1.3090e-03 eta 0:04:44\n",
            "epoch [6/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0493 (0.8383) lr 1.3090e-03 eta 0:04:43\n",
            "epoch [6/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3887 (0.8390) lr 1.3090e-03 eta 0:04:43\n",
            "epoch [6/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.9385 (0.8351) lr 1.3090e-03 eta 0:04:42\n",
            "epoch [6/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1335 (0.8316) lr 1.3090e-03 eta 0:04:41\n",
            "epoch [6/10] batch [1460/1632] time 0.043 (0.042) data 0.000 (0.000) loss 6.8086 (0.8316) lr 1.3090e-03 eta 0:04:40\n",
            "epoch [6/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.6851 (0.8349) lr 1.3090e-03 eta 0:04:39\n",
            "epoch [6/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0033 (0.8300) lr 1.3090e-03 eta 0:04:38\n",
            "epoch [6/10] batch [1520/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.1625 (0.8336) lr 1.3090e-03 eta 0:04:38\n",
            "epoch [6/10] batch [1540/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3359 (0.8421) lr 1.3090e-03 eta 0:04:37\n",
            "epoch [6/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.6426 (0.8429) lr 1.3090e-03 eta 0:04:36\n",
            "epoch [6/10] batch [1580/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1580 (0.8421) lr 1.3090e-03 eta 0:04:35\n",
            "epoch [6/10] batch [1600/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0395 (0.8383) lr 1.3090e-03 eta 0:04:34\n",
            "epoch [6/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.8613 (0.8326) lr 1.3090e-03 eta 0:04:33\n",
            "epoch [7/10] batch [20/1632] time 0.040 (0.055) data 0.000 (0.013) loss 0.0142 (0.8029) lr 1.0000e-03 eta 0:05:58\n",
            "epoch [7/10] batch [40/1632] time 0.041 (0.049) data 0.000 (0.006) loss 0.0363 (0.6871) lr 1.0000e-03 eta 0:05:17\n",
            "epoch [7/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.0119 (0.7211) lr 1.0000e-03 eta 0:04:57\n",
            "epoch [7/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 0.0354 (0.6966) lr 1.0000e-03 eta 0:04:48\n",
            "epoch [7/10] batch [100/1632] time 0.040 (0.044) data 0.000 (0.003) loss 0.0119 (0.6936) lr 1.0000e-03 eta 0:04:41\n",
            "epoch [7/10] batch [120/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.2605 (0.7406) lr 1.0000e-03 eta 0:04:38\n",
            "epoch [7/10] batch [140/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.6064 (0.7061) lr 1.0000e-03 eta 0:04:35\n",
            "epoch [7/10] batch [160/1632] time 0.043 (0.043) data 0.000 (0.002) loss 0.1721 (0.6809) lr 1.0000e-03 eta 0:04:33\n",
            "epoch [7/10] batch [180/1632] time 0.040 (0.043) data 0.000 (0.002) loss 0.8677 (0.7171) lr 1.0000e-03 eta 0:04:32\n",
            "epoch [7/10] batch [200/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.0281 (0.7153) lr 1.0000e-03 eta 0:04:30\n",
            "epoch [7/10] batch [220/1632] time 0.040 (0.043) data 0.000 (0.001) loss 1.1494 (0.7463) lr 1.0000e-03 eta 0:04:29\n",
            "epoch [7/10] batch [240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 1.9209 (0.7375) lr 1.0000e-03 eta 0:04:28\n",
            "epoch [7/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.1031 (0.7374) lr 1.0000e-03 eta 0:04:27\n",
            "epoch [7/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0170 (0.7556) lr 1.0000e-03 eta 0:04:26\n",
            "epoch [7/10] batch [300/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.4148 (0.7670) lr 1.0000e-03 eta 0:04:24\n",
            "epoch [7/10] batch [320/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.0430 (0.7876) lr 1.0000e-03 eta 0:04:23\n",
            "epoch [7/10] batch [340/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.3142 (0.7777) lr 1.0000e-03 eta 0:04:21\n",
            "epoch [7/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0761 (0.7533) lr 1.0000e-03 eta 0:04:20\n",
            "epoch [7/10] batch [380/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0036 (0.7852) lr 1.0000e-03 eta 0:04:19\n",
            "epoch [7/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.2969 (0.8020) lr 1.0000e-03 eta 0:04:18\n",
            "epoch [7/10] batch [420/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2573 (0.7918) lr 1.0000e-03 eta 0:04:17\n",
            "epoch [7/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0132 (0.8017) lr 1.0000e-03 eta 0:04:16\n",
            "epoch [7/10] batch [460/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.2891 (0.8056) lr 1.0000e-03 eta 0:04:15\n",
            "epoch [7/10] batch [480/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1436 (0.8367) lr 1.0000e-03 eta 0:04:14\n",
            "epoch [7/10] batch [500/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.4783 (0.8257) lr 1.0000e-03 eta 0:04:13\n",
            "epoch [7/10] batch [520/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4460 (0.8111) lr 1.0000e-03 eta 0:04:13\n",
            "epoch [7/10] batch [540/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0171 (0.8151) lr 1.0000e-03 eta 0:04:12\n",
            "epoch [7/10] batch [560/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.3765 (0.8149) lr 1.0000e-03 eta 0:04:12\n",
            "epoch [7/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.7695 (0.8172) lr 1.0000e-03 eta 0:04:11\n",
            "epoch [7/10] batch [600/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0447 (0.8324) lr 1.0000e-03 eta 0:04:10\n",
            "epoch [7/10] batch [620/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0595 (0.8290) lr 1.0000e-03 eta 0:04:10\n",
            "epoch [7/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0206 (0.8404) lr 1.0000e-03 eta 0:04:08\n",
            "epoch [7/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.2649 (0.8356) lr 1.0000e-03 eta 0:04:07\n",
            "epoch [7/10] batch [680/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0532 (0.8250) lr 1.0000e-03 eta 0:04:06\n",
            "epoch [7/10] batch [700/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.8379 (0.8400) lr 1.0000e-03 eta 0:04:05\n",
            "epoch [7/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1231 (0.8339) lr 1.0000e-03 eta 0:04:04\n",
            "epoch [7/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0574 (0.8220) lr 1.0000e-03 eta 0:04:03\n",
            "epoch [7/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1464 (0.8188) lr 1.0000e-03 eta 0:04:02\n",
            "epoch [7/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3528 (0.8115) lr 1.0000e-03 eta 0:04:01\n",
            "epoch [7/10] batch [800/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.0896 (0.8069) lr 1.0000e-03 eta 0:04:00\n",
            "epoch [7/10] batch [820/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0271 (0.8021) lr 1.0000e-03 eta 0:03:59\n",
            "epoch [7/10] batch [840/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.6807 (0.8054) lr 1.0000e-03 eta 0:03:59\n",
            "epoch [7/10] batch [860/1632] time 0.048 (0.042) data 0.000 (0.001) loss 1.0039 (0.8018) lr 1.0000e-03 eta 0:03:58\n",
            "epoch [7/10] batch [880/1632] time 0.043 (0.042) data 0.000 (0.001) loss 1.0576 (0.7991) lr 1.0000e-03 eta 0:03:57\n",
            "epoch [7/10] batch [900/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.2195 (0.7964) lr 1.0000e-03 eta 0:03:56\n",
            "epoch [7/10] batch [920/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.2301 (0.7891) lr 1.0000e-03 eta 0:03:55\n",
            "epoch [7/10] batch [940/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.5874 (0.7872) lr 1.0000e-03 eta 0:03:55\n",
            "epoch [7/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.6973 (0.7772) lr 1.0000e-03 eta 0:03:54\n",
            "epoch [7/10] batch [980/1632] time 0.043 (0.042) data 0.001 (0.000) loss 0.6177 (0.7744) lr 1.0000e-03 eta 0:03:53\n",
            "epoch [7/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1137 (0.7802) lr 1.0000e-03 eta 0:03:52\n",
            "epoch [7/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.000) loss 1.3223 (0.7818) lr 1.0000e-03 eta 0:03:51\n",
            "epoch [7/10] batch [1040/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.4290 (0.7828) lr 1.0000e-03 eta 0:03:50\n",
            "epoch [7/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2454 (0.7770) lr 1.0000e-03 eta 0:03:49\n",
            "epoch [7/10] batch [1080/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0372 (0.7769) lr 1.0000e-03 eta 0:03:48\n",
            "epoch [7/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4446 (0.7765) lr 1.0000e-03 eta 0:03:47\n",
            "epoch [7/10] batch [1120/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.2446 (0.7710) lr 1.0000e-03 eta 0:03:46\n",
            "epoch [7/10] batch [1140/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0357 (0.7637) lr 1.0000e-03 eta 0:03:46\n",
            "epoch [7/10] batch [1160/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0253 (0.7542) lr 1.0000e-03 eta 0:03:45\n",
            "epoch [7/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0446 (0.7535) lr 1.0000e-03 eta 0:03:44\n",
            "epoch [7/10] batch [1200/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1031 (0.7521) lr 1.0000e-03 eta 0:03:43\n",
            "epoch [7/10] batch [1220/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.9053 (0.7488) lr 1.0000e-03 eta 0:03:42\n",
            "epoch [7/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1847 (0.7530) lr 1.0000e-03 eta 0:03:41\n",
            "epoch [7/10] batch [1260/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9468 (0.7527) lr 1.0000e-03 eta 0:03:41\n",
            "epoch [7/10] batch [1280/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4990 (0.7578) lr 1.0000e-03 eta 0:03:40\n",
            "epoch [7/10] batch [1300/1632] time 0.047 (0.042) data 0.000 (0.000) loss 0.0095 (0.7546) lr 1.0000e-03 eta 0:03:39\n",
            "epoch [7/10] batch [1320/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0067 (0.7546) lr 1.0000e-03 eta 0:03:38\n",
            "epoch [7/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4321 (0.7526) lr 1.0000e-03 eta 0:03:37\n",
            "epoch [7/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2927 (0.7513) lr 1.0000e-03 eta 0:03:36\n",
            "epoch [7/10] batch [1380/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0017 (0.7499) lr 1.0000e-03 eta 0:03:35\n",
            "epoch [7/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 2.9961 (0.7566) lr 1.0000e-03 eta 0:03:34\n",
            "epoch [7/10] batch [1420/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1455 (0.7558) lr 1.0000e-03 eta 0:03:34\n",
            "epoch [7/10] batch [1440/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2412 (0.7571) lr 1.0000e-03 eta 0:03:33\n",
            "epoch [7/10] batch [1460/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0817 (0.7561) lr 1.0000e-03 eta 0:03:32\n",
            "epoch [7/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0062 (0.7525) lr 1.0000e-03 eta 0:03:31\n",
            "epoch [7/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0148 (0.7551) lr 1.0000e-03 eta 0:03:30\n",
            "epoch [7/10] batch [1520/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.2227 (0.7649) lr 1.0000e-03 eta 0:03:29\n",
            "epoch [7/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0989 (0.7612) lr 1.0000e-03 eta 0:03:29\n",
            "epoch [7/10] batch [1560/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0263 (0.7597) lr 1.0000e-03 eta 0:03:28\n",
            "epoch [7/10] batch [1580/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4626 (0.7570) lr 1.0000e-03 eta 0:03:27\n",
            "epoch [7/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2986 (0.7527) lr 1.0000e-03 eta 0:03:26\n",
            "epoch [7/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 4.7109 (0.7553) lr 1.0000e-03 eta 0:03:25\n",
            "epoch [8/10] batch [20/1632] time 0.045 (0.056) data 0.000 (0.013) loss 0.1448 (0.8792) lr 6.9098e-04 eta 0:04:34\n",
            "epoch [8/10] batch [40/1632] time 0.043 (0.049) data 0.000 (0.007) loss 5.6719 (0.8270) lr 6.9098e-04 eta 0:03:58\n",
            "epoch [8/10] batch [60/1632] time 0.045 (0.047) data 0.000 (0.004) loss 0.0940 (0.6996) lr 6.9098e-04 eta 0:03:45\n",
            "epoch [8/10] batch [80/1632] time 0.041 (0.045) data 0.000 (0.003) loss 2.1758 (0.6768) lr 6.9098e-04 eta 0:03:38\n",
            "epoch [8/10] batch [100/1632] time 0.045 (0.044) data 0.000 (0.003) loss 0.0272 (0.6569) lr 6.9098e-04 eta 0:03:32\n",
            "epoch [8/10] batch [120/1632] time 0.041 (0.044) data 0.000 (0.002) loss 0.0766 (0.6815) lr 6.9098e-04 eta 0:03:29\n",
            "epoch [8/10] batch [140/1632] time 0.045 (0.043) data 0.000 (0.002) loss 0.0610 (0.6940) lr 6.9098e-04 eta 0:03:26\n",
            "epoch [8/10] batch [160/1632] time 0.041 (0.043) data 0.000 (0.002) loss 0.8086 (0.7328) lr 6.9098e-04 eta 0:03:24\n",
            "epoch [8/10] batch [180/1632] time 0.044 (0.043) data 0.000 (0.002) loss 0.0031 (0.7212) lr 6.9098e-04 eta 0:03:23\n",
            "epoch [8/10] batch [200/1632] time 0.043 (0.043) data 0.000 (0.001) loss 0.1741 (0.7397) lr 6.9098e-04 eta 0:03:21\n",
            "epoch [8/10] batch [220/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.2644 (0.7544) lr 6.9098e-04 eta 0:03:20\n",
            "epoch [8/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.8774 (0.7286) lr 6.9098e-04 eta 0:03:19\n",
            "epoch [8/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 1.3662 (0.7262) lr 6.9098e-04 eta 0:03:18\n",
            "epoch [8/10] batch [280/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.8306 (0.7195) lr 6.9098e-04 eta 0:03:17\n",
            "epoch [8/10] batch [300/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0053 (0.7121) lr 6.9098e-04 eta 0:03:16\n",
            "epoch [8/10] batch [320/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0288 (0.7273) lr 6.9098e-04 eta 0:03:15\n",
            "epoch [8/10] batch [340/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.7222 (0.7439) lr 6.9098e-04 eta 0:03:13\n",
            "epoch [8/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.6841 (0.7479) lr 6.9098e-04 eta 0:03:12\n",
            "epoch [8/10] batch [380/1632] time 0.040 (0.042) data 0.000 (0.001) loss 3.6621 (0.7506) lr 6.9098e-04 eta 0:03:10\n",
            "epoch [8/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.7446 (0.7814) lr 6.9098e-04 eta 0:03:09\n",
            "epoch [8/10] batch [420/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0905 (0.7910) lr 6.9098e-04 eta 0:03:08\n",
            "epoch [8/10] batch [440/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3401 (0.8050) lr 6.9098e-04 eta 0:03:07\n",
            "epoch [8/10] batch [460/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0384 (0.7867) lr 6.9098e-04 eta 0:03:06\n",
            "epoch [8/10] batch [480/1632] time 0.041 (0.042) data 0.000 (0.001) loss 5.3203 (0.7788) lr 6.9098e-04 eta 0:03:05\n",
            "epoch [8/10] batch [500/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0582 (0.7603) lr 6.9098e-04 eta 0:03:04\n",
            "epoch [8/10] batch [520/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.9019 (0.7543) lr 6.9098e-04 eta 0:03:03\n",
            "epoch [8/10] batch [540/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0537 (0.7365) lr 6.9098e-04 eta 0:03:02\n",
            "epoch [8/10] batch [560/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0160 (0.7244) lr 6.9098e-04 eta 0:03:01\n",
            "epoch [8/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.5776 (0.7248) lr 6.9098e-04 eta 0:03:01\n",
            "epoch [8/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1069 (0.7221) lr 6.9098e-04 eta 0:03:00\n",
            "epoch [8/10] batch [620/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2192 (0.7159) lr 6.9098e-04 eta 0:02:59\n",
            "epoch [8/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0449 (0.7178) lr 6.9098e-04 eta 0:02:58\n",
            "epoch [8/10] batch [660/1632] time 0.040 (0.042) data 0.000 (0.001) loss 3.6484 (0.7109) lr 6.9098e-04 eta 0:02:57\n",
            "epoch [8/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0587 (0.7294) lr 6.9098e-04 eta 0:02:56\n",
            "epoch [8/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1395 (0.7258) lr 6.9098e-04 eta 0:02:55\n",
            "epoch [8/10] batch [720/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2637 (0.7240) lr 6.9098e-04 eta 0:02:54\n",
            "epoch [8/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3584 (0.7252) lr 6.9098e-04 eta 0:02:53\n",
            "epoch [8/10] batch [760/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.7812 (0.7192) lr 6.9098e-04 eta 0:02:52\n",
            "epoch [8/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 3.0820 (0.7216) lr 6.9098e-04 eta 0:02:51\n",
            "epoch [8/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1016 (0.7186) lr 6.9098e-04 eta 0:02:50\n",
            "epoch [8/10] batch [820/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2440 (0.7167) lr 6.9098e-04 eta 0:02:50\n",
            "epoch [8/10] batch [840/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0465 (0.7107) lr 6.9098e-04 eta 0:02:49\n",
            "epoch [8/10] batch [860/1632] time 0.042 (0.042) data 0.000 (0.001) loss 2.6328 (0.7138) lr 6.9098e-04 eta 0:02:48\n",
            "epoch [8/10] batch [880/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.2747 (0.7176) lr 6.9098e-04 eta 0:02:47\n",
            "epoch [8/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0154 (0.7136) lr 6.9098e-04 eta 0:02:46\n",
            "epoch [8/10] batch [920/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1301 (0.7146) lr 6.9098e-04 eta 0:02:46\n",
            "epoch [8/10] batch [940/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0222 (0.7193) lr 6.9098e-04 eta 0:02:45\n",
            "epoch [8/10] batch [960/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0251 (0.7228) lr 6.9098e-04 eta 0:02:44\n",
            "epoch [8/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.9165 (0.7315) lr 6.9098e-04 eta 0:02:43\n",
            "epoch [8/10] batch [1000/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.6069 (0.7260) lr 6.9098e-04 eta 0:02:42\n",
            "epoch [8/10] batch [1020/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0253 (0.7271) lr 6.9098e-04 eta 0:02:41\n",
            "epoch [8/10] batch [1040/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2180 (0.7310) lr 6.9098e-04 eta 0:02:40\n",
            "epoch [8/10] batch [1060/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4600 (0.7260) lr 6.9098e-04 eta 0:02:39\n",
            "epoch [8/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5786 (0.7293) lr 6.9098e-04 eta 0:02:39\n",
            "epoch [8/10] batch [1100/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0378 (0.7237) lr 6.9098e-04 eta 0:02:38\n",
            "epoch [8/10] batch [1120/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0280 (0.7187) lr 6.9098e-04 eta 0:02:37\n",
            "epoch [8/10] batch [1140/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.6899 (0.7126) lr 6.9098e-04 eta 0:02:36\n",
            "epoch [8/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0901 (0.7077) lr 6.9098e-04 eta 0:02:35\n",
            "epoch [8/10] batch [1180/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.1339 (0.7068) lr 6.9098e-04 eta 0:02:34\n",
            "epoch [8/10] batch [1200/1632] time 0.045 (0.042) data 0.000 (0.000) loss 1.3662 (0.7167) lr 6.9098e-04 eta 0:02:34\n",
            "epoch [8/10] batch [1220/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0254 (0.7150) lr 6.9098e-04 eta 0:02:33\n",
            "epoch [8/10] batch [1240/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.5415 (0.7161) lr 6.9098e-04 eta 0:02:32\n",
            "epoch [8/10] batch [1260/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.5059 (0.7194) lr 6.9098e-04 eta 0:02:31\n",
            "epoch [8/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3342 (0.7214) lr 6.9098e-04 eta 0:02:30\n",
            "epoch [8/10] batch [1300/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.9546 (0.7306) lr 6.9098e-04 eta 0:02:29\n",
            "epoch [8/10] batch [1320/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.4727 (0.7319) lr 6.9098e-04 eta 0:02:29\n",
            "epoch [8/10] batch [1340/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0035 (0.7335) lr 6.9098e-04 eta 0:02:28\n",
            "epoch [8/10] batch [1360/1632] time 0.040 (0.042) data 0.000 (0.000) loss 2.7578 (0.7294) lr 6.9098e-04 eta 0:02:27\n",
            "epoch [8/10] batch [1380/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0536 (0.7263) lr 6.9098e-04 eta 0:02:26\n",
            "epoch [8/10] batch [1400/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0566 (0.7229) lr 6.9098e-04 eta 0:02:25\n",
            "epoch [8/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0743 (0.7278) lr 6.9098e-04 eta 0:02:24\n",
            "epoch [8/10] batch [1440/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0446 (0.7246) lr 6.9098e-04 eta 0:02:23\n",
            "epoch [8/10] batch [1460/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3687 (0.7320) lr 6.9098e-04 eta 0:02:23\n",
            "epoch [8/10] batch [1480/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0286 (0.7300) lr 6.9098e-04 eta 0:02:22\n",
            "epoch [8/10] batch [1500/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1831 (0.7231) lr 6.9098e-04 eta 0:02:21\n",
            "epoch [8/10] batch [1520/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.5449 (0.7249) lr 6.9098e-04 eta 0:02:20\n",
            "epoch [8/10] batch [1540/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.1981 (0.7202) lr 6.9098e-04 eta 0:02:19\n",
            "epoch [8/10] batch [1560/1632] time 0.043 (0.042) data 0.000 (0.000) loss 4.9844 (0.7184) lr 6.9098e-04 eta 0:02:19\n",
            "epoch [8/10] batch [1580/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0178 (0.7194) lr 6.9098e-04 eta 0:02:18\n",
            "epoch [8/10] batch [1600/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.4050 (0.7176) lr 6.9098e-04 eta 0:02:17\n",
            "epoch [8/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.1309 (0.7234) lr 6.9098e-04 eta 0:02:16\n",
            "epoch [9/10] batch [20/1632] time 0.042 (0.056) data 0.000 (0.013) loss 0.3035 (0.7930) lr 4.1221e-04 eta 0:03:02\n",
            "epoch [9/10] batch [40/1632] time 0.042 (0.049) data 0.000 (0.006) loss 0.0046 (0.8865) lr 4.1221e-04 eta 0:02:38\n",
            "epoch [9/10] batch [60/1632] time 0.040 (0.047) data 0.000 (0.004) loss 0.2693 (0.6658) lr 4.1221e-04 eta 0:02:29\n",
            "epoch [9/10] batch [80/1632] time 0.040 (0.045) data 0.000 (0.003) loss 0.1937 (0.6593) lr 4.1221e-04 eta 0:02:23\n",
            "epoch [9/10] batch [100/1632] time 0.041 (0.044) data 0.000 (0.003) loss 0.0378 (0.5805) lr 4.1221e-04 eta 0:02:20\n",
            "epoch [9/10] batch [120/1632] time 0.040 (0.044) data 0.000 (0.002) loss 4.5547 (0.6709) lr 4.1221e-04 eta 0:02:17\n",
            "epoch [9/10] batch [140/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.0091 (0.6276) lr 4.1221e-04 eta 0:02:15\n",
            "epoch [9/10] batch [160/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.7861 (0.7293) lr 4.1221e-04 eta 0:02:14\n",
            "epoch [9/10] batch [180/1632] time 0.042 (0.043) data 0.000 (0.002) loss 0.1201 (0.7107) lr 4.1221e-04 eta 0:02:12\n",
            "epoch [9/10] batch [200/1632] time 0.041 (0.043) data 0.000 (0.001) loss 5.4805 (0.7203) lr 4.1221e-04 eta 0:02:11\n",
            "epoch [9/10] batch [220/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.2434 (0.7226) lr 4.1221e-04 eta 0:02:10\n",
            "epoch [9/10] batch [240/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0903 (0.7501) lr 4.1221e-04 eta 0:02:08\n",
            "epoch [9/10] batch [260/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.5752 (0.7540) lr 4.1221e-04 eta 0:02:07\n",
            "epoch [9/10] batch [280/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.0896 (0.7464) lr 4.1221e-04 eta 0:02:06\n",
            "epoch [9/10] batch [300/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0529 (0.7187) lr 4.1221e-04 eta 0:02:05\n",
            "epoch [9/10] batch [320/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0722 (0.7023) lr 4.1221e-04 eta 0:02:04\n",
            "epoch [9/10] batch [340/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0006 (0.6984) lr 4.1221e-04 eta 0:02:03\n",
            "epoch [9/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 4.1562 (0.7245) lr 4.1221e-04 eta 0:02:02\n",
            "epoch [9/10] batch [380/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0357 (0.7148) lr 4.1221e-04 eta 0:02:01\n",
            "epoch [9/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0202 (0.7070) lr 4.1221e-04 eta 0:02:00\n",
            "epoch [9/10] batch [420/1632] time 0.039 (0.042) data 0.000 (0.001) loss 0.3059 (0.7029) lr 4.1221e-04 eta 0:01:59\n",
            "epoch [9/10] batch [440/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1598 (0.7060) lr 4.1221e-04 eta 0:01:58\n",
            "epoch [9/10] batch [460/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0425 (0.7157) lr 4.1221e-04 eta 0:01:57\n",
            "epoch [9/10] batch [480/1632] time 0.040 (0.042) data 0.000 (0.001) loss 3.1582 (0.7206) lr 4.1221e-04 eta 0:01:56\n",
            "epoch [9/10] batch [500/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.2480 (0.7223) lr 4.1221e-04 eta 0:01:56\n",
            "epoch [9/10] batch [520/1632] time 0.043 (0.042) data 0.000 (0.001) loss 3.2559 (0.7441) lr 4.1221e-04 eta 0:01:55\n",
            "epoch [9/10] batch [540/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0080 (0.7403) lr 4.1221e-04 eta 0:01:54\n",
            "epoch [9/10] batch [560/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0331 (0.7450) lr 4.1221e-04 eta 0:01:53\n",
            "epoch [9/10] batch [580/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0115 (0.7450) lr 4.1221e-04 eta 0:01:52\n",
            "epoch [9/10] batch [600/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.6133 (0.7564) lr 4.1221e-04 eta 0:01:51\n",
            "epoch [9/10] batch [620/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.2373 (0.7427) lr 4.1221e-04 eta 0:01:51\n",
            "epoch [9/10] batch [640/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0679 (0.7338) lr 4.1221e-04 eta 0:01:50\n",
            "epoch [9/10] batch [660/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0218 (0.7219) lr 4.1221e-04 eta 0:01:49\n",
            "epoch [9/10] batch [680/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0164 (0.7277) lr 4.1221e-04 eta 0:01:48\n",
            "epoch [9/10] batch [700/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3647 (0.7382) lr 4.1221e-04 eta 0:01:47\n",
            "epoch [9/10] batch [720/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0583 (0.7379) lr 4.1221e-04 eta 0:01:46\n",
            "epoch [9/10] batch [740/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0759 (0.7443) lr 4.1221e-04 eta 0:01:45\n",
            "epoch [9/10] batch [760/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0425 (0.7360) lr 4.1221e-04 eta 0:01:44\n",
            "epoch [9/10] batch [780/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4614 (0.7407) lr 4.1221e-04 eta 0:01:44\n",
            "epoch [9/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1041 (0.7451) lr 4.1221e-04 eta 0:01:43\n",
            "epoch [9/10] batch [820/1632] time 0.041 (0.042) data 0.000 (0.001) loss 2.2148 (0.7424) lr 4.1221e-04 eta 0:01:42\n",
            "epoch [9/10] batch [840/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4683 (0.7328) lr 4.1221e-04 eta 0:01:41\n",
            "epoch [9/10] batch [860/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0533 (0.7219) lr 4.1221e-04 eta 0:01:40\n",
            "epoch [9/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0107 (0.7161) lr 4.1221e-04 eta 0:01:39\n",
            "epoch [9/10] batch [900/1632] time 0.041 (0.042) data 0.000 (0.001) loss 1.3047 (0.7176) lr 4.1221e-04 eta 0:01:38\n",
            "epoch [9/10] batch [920/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.1599 (0.7192) lr 4.1221e-04 eta 0:01:37\n",
            "epoch [9/10] batch [940/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0047 (0.7180) lr 4.1221e-04 eta 0:01:37\n",
            "epoch [9/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0083 (0.7133) lr 4.1221e-04 eta 0:01:36\n",
            "epoch [9/10] batch [980/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0033 (0.7144) lr 4.1221e-04 eta 0:01:35\n",
            "epoch [9/10] batch [1000/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.4478 (0.7194) lr 4.1221e-04 eta 0:01:34\n",
            "epoch [9/10] batch [1020/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0387 (0.7223) lr 4.1221e-04 eta 0:01:33\n",
            "epoch [9/10] batch [1040/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1353 (0.7277) lr 4.1221e-04 eta 0:01:32\n",
            "epoch [9/10] batch [1060/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.0022 (0.7219) lr 4.1221e-04 eta 0:01:31\n",
            "epoch [9/10] batch [1080/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.2496 (0.7240) lr 4.1221e-04 eta 0:01:31\n",
            "epoch [9/10] batch [1100/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.6553 (0.7207) lr 4.1221e-04 eta 0:01:30\n",
            "epoch [9/10] batch [1120/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3279 (0.7317) lr 4.1221e-04 eta 0:01:29\n",
            "epoch [9/10] batch [1140/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0148 (0.7291) lr 4.1221e-04 eta 0:01:28\n",
            "epoch [9/10] batch [1160/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0260 (0.7227) lr 4.1221e-04 eta 0:01:27\n",
            "epoch [9/10] batch [1180/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1976 (0.7166) lr 4.1221e-04 eta 0:01:26\n",
            "epoch [9/10] batch [1200/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.7759 (0.7143) lr 4.1221e-04 eta 0:01:26\n",
            "epoch [9/10] batch [1220/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.2705 (0.7148) lr 4.1221e-04 eta 0:01:25\n",
            "epoch [9/10] batch [1240/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0049 (0.7128) lr 4.1221e-04 eta 0:01:24\n",
            "epoch [9/10] batch [1260/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0160 (0.7080) lr 4.1221e-04 eta 0:01:23\n",
            "epoch [9/10] batch [1280/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1732 (0.7171) lr 4.1221e-04 eta 0:01:22\n",
            "epoch [9/10] batch [1300/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.0137 (0.7105) lr 4.1221e-04 eta 0:01:21\n",
            "epoch [9/10] batch [1320/1632] time 0.044 (0.042) data 0.000 (0.000) loss 0.0689 (0.7077) lr 4.1221e-04 eta 0:01:21\n",
            "epoch [9/10] batch [1340/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0286 (0.7070) lr 4.1221e-04 eta 0:01:20\n",
            "epoch [9/10] batch [1360/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0067 (0.7012) lr 4.1221e-04 eta 0:01:19\n",
            "epoch [9/10] batch [1380/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0945 (0.7001) lr 4.1221e-04 eta 0:01:18\n",
            "epoch [9/10] batch [1400/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0167 (0.6957) lr 4.1221e-04 eta 0:01:17\n",
            "epoch [9/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0070 (0.6966) lr 4.1221e-04 eta 0:01:16\n",
            "epoch [9/10] batch [1440/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.1356 (0.6957) lr 4.1221e-04 eta 0:01:16\n",
            "epoch [9/10] batch [1460/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0462 (0.6971) lr 4.1221e-04 eta 0:01:15\n",
            "epoch [9/10] batch [1480/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0071 (0.6965) lr 4.1221e-04 eta 0:01:14\n",
            "epoch [9/10] batch [1500/1632] time 0.041 (0.042) data 0.000 (0.000) loss 1.7256 (0.6987) lr 4.1221e-04 eta 0:01:13\n",
            "epoch [9/10] batch [1520/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.3225 (0.6968) lr 4.1221e-04 eta 0:01:12\n",
            "epoch [9/10] batch [1540/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0203 (0.6995) lr 4.1221e-04 eta 0:01:11\n",
            "epoch [9/10] batch [1560/1632] time 0.040 (0.042) data 0.000 (0.000) loss 0.1481 (0.6994) lr 4.1221e-04 eta 0:01:11\n",
            "epoch [9/10] batch [1580/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.6582 (0.7007) lr 4.1221e-04 eta 0:01:10\n",
            "epoch [9/10] batch [1600/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0186 (0.6999) lr 4.1221e-04 eta 0:01:09\n",
            "epoch [9/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.0256 (0.6970) lr 4.1221e-04 eta 0:01:08\n",
            "epoch [10/10] batch [20/1632] time 0.041 (0.056) data 0.000 (0.013) loss 0.0052 (0.5271) lr 1.9098e-04 eta 0:01:30\n",
            "epoch [10/10] batch [40/1632] time 0.041 (0.048) data 0.000 (0.007) loss 4.8516 (0.6039) lr 1.9098e-04 eta 0:01:17\n",
            "epoch [10/10] batch [60/1632] time 0.041 (0.046) data 0.000 (0.004) loss 0.0600 (0.5691) lr 1.9098e-04 eta 0:01:12\n",
            "epoch [10/10] batch [80/1632] time 0.046 (0.045) data 0.000 (0.003) loss 0.0004 (0.5456) lr 1.9098e-04 eta 0:01:09\n",
            "epoch [10/10] batch [100/1632] time 0.040 (0.044) data 0.000 (0.003) loss 1.6250 (0.5811) lr 1.9098e-04 eta 0:01:07\n",
            "epoch [10/10] batch [120/1632] time 0.045 (0.044) data 0.000 (0.002) loss 1.4248 (0.6168) lr 1.9098e-04 eta 0:01:06\n",
            "epoch [10/10] batch [140/1632] time 0.040 (0.044) data 0.000 (0.002) loss 0.0759 (0.5888) lr 1.9098e-04 eta 0:01:05\n",
            "epoch [10/10] batch [160/1632] time 0.046 (0.043) data 0.000 (0.002) loss 0.3188 (0.6905) lr 1.9098e-04 eta 0:01:04\n",
            "epoch [10/10] batch [180/1632] time 0.039 (0.043) data 0.000 (0.002) loss 0.0497 (0.7161) lr 1.9098e-04 eta 0:01:02\n",
            "epoch [10/10] batch [200/1632] time 0.045 (0.043) data 0.000 (0.002) loss 0.0775 (0.7462) lr 1.9098e-04 eta 0:01:01\n",
            "epoch [10/10] batch [220/1632] time 0.041 (0.043) data 0.000 (0.001) loss 0.0207 (0.7611) lr 1.9098e-04 eta 0:01:00\n",
            "epoch [10/10] batch [240/1632] time 0.042 (0.043) data 0.000 (0.001) loss 4.1523 (0.8074) lr 1.9098e-04 eta 0:00:59\n",
            "epoch [10/10] batch [260/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.1257 (0.7887) lr 1.9098e-04 eta 0:00:58\n",
            "epoch [10/10] batch [280/1632] time 0.042 (0.043) data 0.000 (0.001) loss 0.4082 (0.7947) lr 1.9098e-04 eta 0:00:57\n",
            "epoch [10/10] batch [300/1632] time 0.040 (0.043) data 0.000 (0.001) loss 0.0606 (0.7822) lr 1.9098e-04 eta 0:00:56\n",
            "epoch [10/10] batch [320/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0751 (0.7826) lr 1.9098e-04 eta 0:00:55\n",
            "epoch [10/10] batch [340/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0630 (0.7727) lr 1.9098e-04 eta 0:00:54\n",
            "epoch [10/10] batch [360/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.3481 (0.7538) lr 1.9098e-04 eta 0:00:53\n",
            "epoch [10/10] batch [380/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.4023 (0.7647) lr 1.9098e-04 eta 0:00:52\n",
            "epoch [10/10] batch [400/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.1013 (0.7492) lr 1.9098e-04 eta 0:00:51\n",
            "epoch [10/10] batch [420/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.2325 (0.7361) lr 1.9098e-04 eta 0:00:51\n",
            "epoch [10/10] batch [440/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0800 (0.7465) lr 1.9098e-04 eta 0:00:50\n",
            "epoch [10/10] batch [460/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.6367 (0.7564) lr 1.9098e-04 eta 0:00:49\n",
            "epoch [10/10] batch [480/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0215 (0.7739) lr 1.9098e-04 eta 0:00:48\n",
            "epoch [10/10] batch [500/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.4258 (0.7630) lr 1.9098e-04 eta 0:00:47\n",
            "epoch [10/10] batch [520/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.4802 (0.7841) lr 1.9098e-04 eta 0:00:46\n",
            "epoch [10/10] batch [540/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.1239 (0.7775) lr 1.9098e-04 eta 0:00:45\n",
            "epoch [10/10] batch [560/1632] time 0.044 (0.042) data 0.000 (0.001) loss 2.2871 (0.7710) lr 1.9098e-04 eta 0:00:44\n",
            "epoch [10/10] batch [580/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0274 (0.7741) lr 1.9098e-04 eta 0:00:43\n",
            "epoch [10/10] batch [600/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0036 (0.7618) lr 1.9098e-04 eta 0:00:43\n",
            "epoch [10/10] batch [620/1632] time 0.044 (0.042) data 0.000 (0.001) loss 0.0449 (0.7566) lr 1.9098e-04 eta 0:00:42\n",
            "epoch [10/10] batch [640/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.6846 (0.7657) lr 1.9098e-04 eta 0:00:41\n",
            "epoch [10/10] batch [660/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.1273 (0.7798) lr 1.9098e-04 eta 0:00:40\n",
            "epoch [10/10] batch [680/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.0102 (0.7788) lr 1.9098e-04 eta 0:00:39\n",
            "epoch [10/10] batch [700/1632] time 0.040 (0.042) data 0.000 (0.001) loss 1.2012 (0.7738) lr 1.9098e-04 eta 0:00:39\n",
            "epoch [10/10] batch [720/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.4626 (0.7702) lr 1.9098e-04 eta 0:00:38\n",
            "epoch [10/10] batch [740/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0445 (0.7658) lr 1.9098e-04 eta 0:00:37\n",
            "epoch [10/10] batch [760/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.6543 (0.7640) lr 1.9098e-04 eta 0:00:36\n",
            "epoch [10/10] batch [780/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0152 (0.7586) lr 1.9098e-04 eta 0:00:35\n",
            "epoch [10/10] batch [800/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0303 (0.7656) lr 1.9098e-04 eta 0:00:34\n",
            "epoch [10/10] batch [820/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0929 (0.7539) lr 1.9098e-04 eta 0:00:33\n",
            "epoch [10/10] batch [840/1632] time 0.040 (0.042) data 0.000 (0.001) loss 0.0741 (0.7457) lr 1.9098e-04 eta 0:00:33\n",
            "epoch [10/10] batch [860/1632] time 0.045 (0.042) data 0.000 (0.001) loss 0.0279 (0.7434) lr 1.9098e-04 eta 0:00:32\n",
            "epoch [10/10] batch [880/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0946 (0.7357) lr 1.9098e-04 eta 0:00:31\n",
            "epoch [10/10] batch [900/1632] time 0.045 (0.042) data 0.000 (0.001) loss 1.1992 (0.7470) lr 1.9098e-04 eta 0:00:30\n",
            "epoch [10/10] batch [920/1632] time 0.042 (0.042) data 0.000 (0.001) loss 5.3047 (0.7578) lr 1.9098e-04 eta 0:00:29\n",
            "epoch [10/10] batch [940/1632] time 0.043 (0.042) data 0.000 (0.001) loss 0.2959 (0.7605) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [960/1632] time 0.041 (0.042) data 0.000 (0.001) loss 0.0060 (0.7552) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [980/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.0073 (0.7457) lr 1.9098e-04 eta 0:00:27\n",
            "epoch [10/10] batch [1000/1632] time 0.042 (0.042) data 0.000 (0.001) loss 0.8735 (0.7440) lr 1.9098e-04 eta 0:00:26\n",
            "epoch [10/10] batch [1020/1632] time 0.042 (0.042) data 0.000 (0.001) loss 1.0029 (0.7361) lr 1.9098e-04 eta 0:00:25\n",
            "epoch [10/10] batch [1040/1632] time 0.047 (0.042) data 0.000 (0.000) loss 0.0438 (0.7310) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [10/10] batch [1060/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.7612 (0.7328) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [1080/1632] time 0.046 (0.042) data 0.000 (0.000) loss 1.0938 (0.7374) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [1100/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.7520 (0.7332) lr 1.9098e-04 eta 0:00:22\n",
            "epoch [10/10] batch [1120/1632] time 0.045 (0.042) data 0.000 (0.000) loss 0.3772 (0.7336) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [10/10] batch [1140/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0195 (0.7340) lr 1.9098e-04 eta 0:00:20\n",
            "epoch [10/10] batch [1160/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.3145 (0.7299) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [1180/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.5122 (0.7349) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [1200/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.9453 (0.7307) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [1220/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0149 (0.7288) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [10/10] batch [1240/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0924 (0.7223) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [1260/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0053 (0.7163) lr 1.9098e-04 eta 0:00:15\n",
            "epoch [10/10] batch [1280/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.7036 (0.7159) lr 1.9098e-04 eta 0:00:14\n",
            "epoch [10/10] batch [1300/1632] time 0.042 (0.042) data 0.001 (0.000) loss 1.3379 (0.7096) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [1320/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0842 (0.7112) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [1340/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0997 (0.7166) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [1360/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0871 (0.7095) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [10/10] batch [1380/1632] time 0.046 (0.042) data 0.000 (0.000) loss 0.0138 (0.7121) lr 1.9098e-04 eta 0:00:10\n",
            "epoch [10/10] batch [1400/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0908 (0.7096) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [1420/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.1417 (0.7049) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [1440/1632] time 0.042 (0.042) data 0.000 (0.000) loss 2.0098 (0.7001) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [1460/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0150 (0.6927) lr 1.9098e-04 eta 0:00:07\n",
            "epoch [10/10] batch [1480/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0262 (0.6936) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [1500/1632] time 0.043 (0.042) data 0.000 (0.000) loss 0.0205 (0.6938) lr 1.9098e-04 eta 0:00:05\n",
            "epoch [10/10] batch [1520/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0284 (0.6970) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [10/10] batch [1540/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0363 (0.6975) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [1560/1632] time 0.041 (0.042) data 0.000 (0.000) loss 0.0584 (0.6990) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [1580/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.7632 (0.7035) lr 1.9098e-04 eta 0:00:02\n",
            "epoch [10/10] batch [1600/1632] time 0.042 (0.042) data 0.000 (0.000) loss 0.0069 (0.7016) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [1620/1632] time 0.039 (0.042) data 0.000 (0.000) loss 0.0104 (0.7012) lr 1.9098e-04 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3/prompt_learner/model.pth.tar-10\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 25/25 [00:31<00:00,  1.24s/it]\n",
            "=> result\n",
            "* total: 2,463\n",
            "* correct: 2,243\n",
            "* accuracy: 91.1%\n",
            "* error: 8.9%\n",
            "* macro_f1: 89.8%\n",
            "Elapsed: 0:11:57\n",
            "2024-12-10 11:56:50.307932: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-10 11:56:50.325779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-10 11:56:50.346960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-10 11:56:50.353613: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-10 11:56:50.368920: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-10 11:56:51.411244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/MIP/vit_b16_c4_ep10_batch1.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: 0\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1\n",
            "resume: \n",
            "root: data/\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: MIP\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: data/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: MIP\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               12\n",
            "On-line CPU(s) list:                  0-11\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   6\n",
            "Socket(s):                            1\n",
            "Stepping:                             7\n",
            "BogoMIPS:                             4400.44\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            192 KiB (6 instances)\n",
            "L1i cache:                            192 KiB (6 instances)\n",
            "L2 cache:                             6 MiB (6 instances)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-11\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Not affected\n",
            "Vulnerability Mds:                    Not affected\n",
            "Vulnerability Meltdown:               Not affected\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: MIP\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear1.weight', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.ctx'}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/tensorboard)\n",
            "epoch [1/10] batch [20/592] time 0.034 (0.148) data 0.000 (0.039) loss 3.2949 (1.8866) lr 1.0000e-05 eta 0:14:31\n",
            "epoch [1/10] batch [40/592] time 0.035 (0.094) data 0.000 (0.022) loss 4.1680 (1.7528) lr 1.0000e-05 eta 0:09:14\n",
            "epoch [1/10] batch [60/592] time 0.033 (0.077) data 0.000 (0.018) loss 0.6914 (1.8932) lr 1.0000e-05 eta 0:07:32\n",
            "epoch [1/10] batch [80/592] time 0.034 (0.070) data 0.000 (0.016) loss 1.0215 (1.8526) lr 1.0000e-05 eta 0:06:47\n",
            "epoch [1/10] batch [100/592] time 0.034 (0.065) data 0.000 (0.015) loss 0.0834 (1.8405) lr 1.0000e-05 eta 0:06:18\n",
            "epoch [1/10] batch [120/592] time 0.039 (0.063) data 0.000 (0.016) loss 0.3928 (1.7852) lr 1.0000e-05 eta 0:06:05\n",
            "epoch [1/10] batch [140/592] time 0.033 (0.062) data 0.000 (0.016) loss 1.0449 (1.6956) lr 1.0000e-05 eta 0:05:55\n",
            "epoch [1/10] batch [160/592] time 0.039 (0.058) data 0.000 (0.014) loss 3.8262 (1.6759) lr 1.0000e-05 eta 0:05:35\n",
            "epoch [1/10] batch [180/592] time 0.034 (0.066) data 0.000 (0.023) loss 0.0626 (1.5644) lr 1.0000e-05 eta 0:06:18\n",
            "epoch [1/10] batch [200/592] time 0.032 (0.063) data 0.000 (0.020) loss 2.7148 (1.5708) lr 1.0000e-05 eta 0:05:59\n",
            "epoch [1/10] batch [220/592] time 0.034 (0.062) data 0.000 (0.020) loss 0.7412 (1.5278) lr 1.0000e-05 eta 0:05:52\n",
            "epoch [1/10] batch [240/592] time 0.032 (0.061) data 0.000 (0.020) loss 0.0333 (1.4795) lr 1.0000e-05 eta 0:05:48\n",
            "epoch [1/10] batch [260/592] time 0.035 (0.060) data 0.000 (0.019) loss 0.2546 (1.4064) lr 1.0000e-05 eta 0:05:39\n",
            "epoch [1/10] batch [280/592] time 0.550 (0.061) data 0.516 (0.021) loss 3.8613 (1.3727) lr 1.0000e-05 eta 0:05:41\n",
            "epoch [1/10] batch [300/592] time 0.032 (0.059) data 0.000 (0.020) loss 0.8433 (1.3409) lr 1.0000e-05 eta 0:05:32\n",
            "epoch [1/10] batch [320/592] time 0.038 (0.059) data 0.000 (0.020) loss 0.7544 (1.3024) lr 1.0000e-05 eta 0:05:31\n",
            "epoch [1/10] batch [340/592] time 0.032 (0.059) data 0.000 (0.020) loss 0.0374 (1.2555) lr 1.0000e-05 eta 0:05:26\n",
            "epoch [1/10] batch [360/592] time 0.032 (0.058) data 0.000 (0.019) loss 0.1276 (1.2138) lr 1.0000e-05 eta 0:05:19\n",
            "epoch [1/10] batch [380/592] time 0.035 (0.057) data 0.000 (0.018) loss 2.2090 (1.1926) lr 1.0000e-05 eta 0:05:14\n",
            "epoch [1/10] batch [400/592] time 0.034 (0.057) data 0.000 (0.018) loss 1.5732 (1.1810) lr 1.0000e-05 eta 0:05:12\n",
            "epoch [1/10] batch [420/592] time 0.043 (0.056) data 0.000 (0.018) loss 0.0224 (1.1668) lr 1.0000e-05 eta 0:05:08\n",
            "epoch [1/10] batch [440/592] time 0.034 (0.056) data 0.000 (0.018) loss 0.5396 (1.1637) lr 1.0000e-05 eta 0:05:05\n",
            "epoch [1/10] batch [460/592] time 0.036 (0.055) data 0.000 (0.017) loss 0.7915 (1.1638) lr 1.0000e-05 eta 0:05:01\n",
            "epoch [1/10] batch [480/592] time 0.034 (0.054) data 0.000 (0.017) loss 0.0182 (1.1619) lr 1.0000e-05 eta 0:04:56\n",
            "epoch [1/10] batch [500/592] time 0.035 (0.054) data 0.000 (0.016) loss 2.0762 (1.1446) lr 1.0000e-05 eta 0:04:53\n",
            "epoch [1/10] batch [520/592] time 0.032 (0.054) data 0.000 (0.016) loss 2.5078 (1.1442) lr 1.0000e-05 eta 0:04:52\n",
            "epoch [1/10] batch [540/592] time 0.033 (0.053) data 0.000 (0.016) loss 0.0318 (1.1283) lr 1.0000e-05 eta 0:04:47\n",
            "epoch [1/10] batch [560/592] time 0.033 (0.053) data 0.000 (0.016) loss 1.2441 (1.1218) lr 1.0000e-05 eta 0:04:46\n",
            "epoch [1/10] batch [580/592] time 0.032 (0.053) data 0.000 (0.016) loss 2.1484 (1.1216) lr 1.0000e-05 eta 0:04:43\n",
            "epoch [2/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 0.2864 (1.2179) lr 2.0000e-03 eta 0:04:22\n",
            "epoch [2/10] batch [40/592] time 0.035 (0.042) data 0.000 (0.007) loss 3.0508 (1.1028) lr 2.0000e-03 eta 0:03:43\n",
            "epoch [2/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.005) loss 1.0918 (1.0976) lr 2.0000e-03 eta 0:03:30\n",
            "epoch [2/10] batch [80/592] time 0.035 (0.039) data 0.000 (0.004) loss 0.0687 (1.0324) lr 2.0000e-03 eta 0:03:23\n",
            "epoch [2/10] batch [100/592] time 0.035 (0.038) data 0.000 (0.003) loss 0.0297 (0.9797) lr 2.0000e-03 eta 0:03:19\n",
            "epoch [2/10] batch [120/592] time 0.035 (0.038) data 0.000 (0.002) loss 4.3789 (0.9156) lr 2.0000e-03 eta 0:03:16\n",
            "epoch [2/10] batch [140/592] time 0.038 (0.037) data 0.000 (0.002) loss 1.7266 (0.8920) lr 2.0000e-03 eta 0:03:14\n",
            "epoch [2/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 1.1182 (0.9694) lr 2.0000e-03 eta 0:03:13\n",
            "epoch [2/10] batch [180/592] time 0.038 (0.037) data 0.000 (0.002) loss 0.0043 (0.9348) lr 2.0000e-03 eta 0:03:12\n",
            "epoch [2/10] batch [200/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.2700 (0.9061) lr 2.0000e-03 eta 0:03:11\n",
            "epoch [2/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0349 (0.8789) lr 2.0000e-03 eta 0:03:10\n",
            "epoch [2/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.2852 (0.9277) lr 2.0000e-03 eta 0:03:08\n",
            "epoch [2/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0153 (0.8977) lr 2.0000e-03 eta 0:03:07\n",
            "epoch [2/10] batch [280/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.3984 (0.8755) lr 2.0000e-03 eta 0:03:05\n",
            "epoch [2/10] batch [300/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0003 (0.8570) lr 2.0000e-03 eta 0:03:03\n",
            "epoch [2/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.2988 (0.8463) lr 2.0000e-03 eta 0:03:02\n",
            "epoch [2/10] batch [340/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0020 (0.8170) lr 2.0000e-03 eta 0:03:00\n",
            "epoch [2/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2146 (0.8083) lr 2.0000e-03 eta 0:02:59\n",
            "epoch [2/10] batch [380/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0144 (0.8118) lr 2.0000e-03 eta 0:02:58\n",
            "epoch [2/10] batch [400/592] time 0.036 (0.036) data 0.000 (0.001) loss 1.3047 (0.8323) lr 2.0000e-03 eta 0:02:57\n",
            "epoch [2/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0002 (0.8147) lr 2.0000e-03 eta 0:02:56\n",
            "epoch [2/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0387 (0.8108) lr 2.0000e-03 eta 0:02:55\n",
            "epoch [2/10] batch [460/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.4146 (0.8031) lr 2.0000e-03 eta 0:02:54\n",
            "epoch [2/10] batch [480/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0717 (0.7923) lr 2.0000e-03 eta 0:02:54\n",
            "epoch [2/10] batch [500/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0793 (0.7894) lr 2.0000e-03 eta 0:02:53\n",
            "epoch [2/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.4709 (0.8027) lr 2.0000e-03 eta 0:02:52\n",
            "epoch [2/10] batch [540/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0018 (0.8093) lr 2.0000e-03 eta 0:02:52\n",
            "epoch [2/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 2.5684 (0.8055) lr 2.0000e-03 eta 0:02:51\n",
            "epoch [2/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0085 (0.7919) lr 2.0000e-03 eta 0:02:50\n",
            "epoch [3/10] batch [20/592] time 0.035 (0.051) data 0.000 (0.013) loss 0.0699 (0.6609) lr 1.9511e-03 eta 0:03:58\n",
            "epoch [3/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.006) loss 0.0807 (0.7322) lr 1.9511e-03 eta 0:03:19\n",
            "epoch [3/10] batch [60/592] time 0.038 (0.040) data 0.000 (0.004) loss 0.0230 (0.6515) lr 1.9511e-03 eta 0:03:07\n",
            "epoch [3/10] batch [80/592] time 0.034 (0.038) data 0.000 (0.003) loss 0.0613 (0.6646) lr 1.9511e-03 eta 0:02:59\n",
            "epoch [3/10] batch [100/592] time 0.034 (0.038) data 0.000 (0.003) loss 0.1082 (0.6889) lr 1.9511e-03 eta 0:02:54\n",
            "epoch [3/10] batch [120/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.1619 (0.7179) lr 1.9511e-03 eta 0:02:51\n",
            "epoch [3/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0082 (0.7080) lr 1.9511e-03 eta 0:02:48\n",
            "epoch [3/10] batch [160/592] time 0.041 (0.036) data 0.000 (0.002) loss 0.3523 (0.7509) lr 1.9511e-03 eta 0:02:46\n",
            "epoch [3/10] batch [180/592] time 0.034 (0.036) data 0.000 (0.002) loss 2.1875 (0.7578) lr 1.9511e-03 eta 0:02:45\n",
            "epoch [3/10] batch [200/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0062 (0.7222) lr 1.9511e-03 eta 0:02:43\n",
            "epoch [3/10] batch [220/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.8125 (0.7647) lr 1.9511e-03 eta 0:02:41\n",
            "epoch [3/10] batch [240/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.2112 (0.7951) lr 1.9511e-03 eta 0:02:40\n",
            "epoch [3/10] batch [260/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.8623 (0.7659) lr 1.9511e-03 eta 0:02:39\n",
            "epoch [3/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 2.3809 (0.7714) lr 1.9511e-03 eta 0:02:38\n",
            "epoch [3/10] batch [300/592] time 0.035 (0.036) data 0.000 (0.001) loss 1.1777 (0.7736) lr 1.9511e-03 eta 0:02:37\n",
            "epoch [3/10] batch [320/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.2542 (0.7441) lr 1.9511e-03 eta 0:02:36\n",
            "epoch [3/10] batch [340/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.5640 (0.7334) lr 1.9511e-03 eta 0:02:36\n",
            "epoch [3/10] batch [360/592] time 0.043 (0.036) data 0.000 (0.001) loss 0.1124 (0.7420) lr 1.9511e-03 eta 0:02:36\n",
            "epoch [3/10] batch [380/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0782 (0.7459) lr 1.9511e-03 eta 0:02:35\n",
            "epoch [3/10] batch [400/592] time 0.042 (0.036) data 0.000 (0.001) loss 0.0609 (0.7419) lr 1.9511e-03 eta 0:02:35\n",
            "epoch [3/10] batch [420/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0090 (0.7301) lr 1.9511e-03 eta 0:02:34\n",
            "epoch [3/10] batch [440/592] time 0.035 (0.036) data 0.000 (0.001) loss 1.1631 (0.7135) lr 1.9511e-03 eta 0:02:34\n",
            "epoch [3/10] batch [460/592] time 0.038 (0.036) data 0.000 (0.001) loss 4.5117 (0.7277) lr 1.9511e-03 eta 0:02:33\n",
            "epoch [3/10] batch [480/592] time 0.035 (0.036) data 0.000 (0.001) loss 3.0820 (0.7265) lr 1.9511e-03 eta 0:02:32\n",
            "epoch [3/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0611 (0.7150) lr 1.9511e-03 eta 0:02:31\n",
            "epoch [3/10] batch [520/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.2333 (0.7111) lr 1.9511e-03 eta 0:02:30\n",
            "epoch [3/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0005 (0.7024) lr 1.9511e-03 eta 0:02:29\n",
            "epoch [3/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1064 (0.7069) lr 1.9511e-03 eta 0:02:29\n",
            "epoch [3/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.3789 (0.7019) lr 1.9511e-03 eta 0:02:28\n",
            "epoch [4/10] batch [20/592] time 0.035 (0.049) data 0.001 (0.012) loss 0.0089 (0.2531) lr 1.8090e-03 eta 0:03:20\n",
            "epoch [4/10] batch [40/592] time 0.035 (0.042) data 0.000 (0.006) loss 0.0136 (0.2859) lr 1.8090e-03 eta 0:02:50\n",
            "epoch [4/10] batch [60/592] time 0.036 (0.039) data 0.000 (0.004) loss 1.9805 (0.4157) lr 1.8090e-03 eta 0:02:39\n",
            "epoch [4/10] batch [80/592] time 0.035 (0.038) data 0.000 (0.003) loss 0.1199 (0.4587) lr 1.8090e-03 eta 0:02:34\n",
            "epoch [4/10] batch [100/592] time 0.036 (0.037) data 0.000 (0.003) loss 0.0250 (0.5207) lr 1.8090e-03 eta 0:02:30\n",
            "epoch [4/10] batch [120/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0167 (0.5082) lr 1.8090e-03 eta 0:02:29\n",
            "epoch [4/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.5713 (0.5450) lr 1.8090e-03 eta 0:02:27\n",
            "epoch [4/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0102 (0.5506) lr 1.8090e-03 eta 0:02:26\n",
            "epoch [4/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 1.9658 (0.5757) lr 1.8090e-03 eta 0:02:25\n",
            "epoch [4/10] batch [200/592] time 0.037 (0.037) data 0.000 (0.001) loss 0.6426 (0.6129) lr 1.8090e-03 eta 0:02:24\n",
            "epoch [4/10] batch [220/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0412 (0.6208) lr 1.8090e-03 eta 0:02:23\n",
            "epoch [4/10] batch [240/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.8760 (0.5993) lr 1.8090e-03 eta 0:02:22\n",
            "epoch [4/10] batch [260/592] time 0.033 (0.036) data 0.000 (0.001) loss 3.4629 (0.6079) lr 1.8090e-03 eta 0:02:21\n",
            "epoch [4/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0189 (0.5916) lr 1.8090e-03 eta 0:02:19\n",
            "epoch [4/10] batch [300/592] time 0.036 (0.036) data 0.000 (0.001) loss 1.2881 (0.5938) lr 1.8090e-03 eta 0:02:19\n",
            "epoch [4/10] batch [320/592] time 0.035 (0.036) data 0.000 (0.001) loss 2.9121 (0.6149) lr 1.8090e-03 eta 0:02:18\n",
            "epoch [4/10] batch [340/592] time 0.035 (0.036) data 0.000 (0.001) loss 3.8730 (0.6309) lr 1.8090e-03 eta 0:02:17\n",
            "epoch [4/10] batch [360/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.5581 (0.6382) lr 1.8090e-03 eta 0:02:16\n",
            "epoch [4/10] batch [380/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0834 (0.6277) lr 1.8090e-03 eta 0:02:15\n",
            "epoch [4/10] batch [400/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.5503 (0.6527) lr 1.8090e-03 eta 0:02:15\n",
            "epoch [4/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0082 (0.6460) lr 1.8090e-03 eta 0:02:14\n",
            "epoch [4/10] batch [440/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0173 (0.6576) lr 1.8090e-03 eta 0:02:13\n",
            "epoch [4/10] batch [460/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0076 (0.6648) lr 1.8090e-03 eta 0:02:12\n",
            "epoch [4/10] batch [480/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0462 (0.6499) lr 1.8090e-03 eta 0:02:12\n",
            "epoch [4/10] batch [500/592] time 0.037 (0.036) data 0.000 (0.001) loss 1.9023 (0.6532) lr 1.8090e-03 eta 0:02:11\n",
            "epoch [4/10] batch [520/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0121 (0.6465) lr 1.8090e-03 eta 0:02:10\n",
            "epoch [4/10] batch [540/592] time 0.040 (0.036) data 0.000 (0.001) loss 1.8545 (0.6529) lr 1.8090e-03 eta 0:02:09\n",
            "epoch [4/10] batch [560/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.6201 (0.6612) lr 1.8090e-03 eta 0:02:09\n",
            "epoch [4/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0002 (0.6647) lr 1.8090e-03 eta 0:02:08\n",
            "epoch [5/10] batch [20/592] time 0.036 (0.051) data 0.000 (0.015) loss 0.0031 (0.5306) lr 1.5878e-03 eta 0:03:01\n",
            "epoch [5/10] batch [40/592] time 0.035 (0.044) data 0.000 (0.007) loss 0.3992 (0.6492) lr 1.5878e-03 eta 0:02:33\n",
            "epoch [5/10] batch [60/592] time 0.036 (0.041) data 0.000 (0.005) loss 0.2451 (0.6352) lr 1.5878e-03 eta 0:02:23\n",
            "epoch [5/10] batch [80/592] time 0.035 (0.040) data 0.000 (0.004) loss 0.0457 (0.6718) lr 1.5878e-03 eta 0:02:18\n",
            "epoch [5/10] batch [100/592] time 0.036 (0.039) data 0.000 (0.003) loss 0.0021 (0.6460) lr 1.5878e-03 eta 0:02:14\n",
            "epoch [5/10] batch [120/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.0215 (0.6217) lr 1.5878e-03 eta 0:02:12\n",
            "epoch [5/10] batch [140/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0050 (0.6433) lr 1.5878e-03 eta 0:02:09\n",
            "epoch [5/10] batch [160/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0501 (0.6222) lr 1.5878e-03 eta 0:02:07\n",
            "epoch [5/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0394 (0.6060) lr 1.5878e-03 eta 0:02:06\n",
            "epoch [5/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0095 (0.6212) lr 1.5878e-03 eta 0:02:04\n",
            "epoch [5/10] batch [220/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.4353 (0.6273) lr 1.5878e-03 eta 0:02:03\n",
            "epoch [5/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1472 (0.6598) lr 1.5878e-03 eta 0:02:01\n",
            "epoch [5/10] batch [260/592] time 0.036 (0.037) data 0.000 (0.001) loss 3.5898 (0.6755) lr 1.5878e-03 eta 0:02:00\n",
            "epoch [5/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0263 (0.6401) lr 1.5878e-03 eta 0:01:59\n",
            "epoch [5/10] batch [300/592] time 0.036 (0.036) data 0.000 (0.001) loss 2.7207 (0.6584) lr 1.5878e-03 eta 0:01:58\n",
            "epoch [5/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1628 (0.6677) lr 1.5878e-03 eta 0:01:57\n",
            "epoch [5/10] batch [340/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0515 (0.6459) lr 1.5878e-03 eta 0:01:56\n",
            "epoch [5/10] batch [360/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0191 (0.6557) lr 1.5878e-03 eta 0:01:55\n",
            "epoch [5/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0068 (0.6486) lr 1.5878e-03 eta 0:01:54\n",
            "epoch [5/10] batch [400/592] time 0.040 (0.036) data 0.000 (0.001) loss 0.0612 (0.6621) lr 1.5878e-03 eta 0:01:54\n",
            "epoch [5/10] batch [420/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.3103 (0.6604) lr 1.5878e-03 eta 0:01:53\n",
            "epoch [5/10] batch [440/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.5601 (0.6563) lr 1.5878e-03 eta 0:01:52\n",
            "epoch [5/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2345 (0.6469) lr 1.5878e-03 eta 0:01:52\n",
            "epoch [5/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 3.8320 (0.6531) lr 1.5878e-03 eta 0:01:51\n",
            "epoch [5/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0060 (0.6704) lr 1.5878e-03 eta 0:01:50\n",
            "epoch [5/10] batch [520/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.1622 (0.6575) lr 1.5878e-03 eta 0:01:49\n",
            "epoch [5/10] batch [540/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.6167 (0.6490) lr 1.5878e-03 eta 0:01:48\n",
            "epoch [5/10] batch [560/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0654 (0.6541) lr 1.5878e-03 eta 0:01:48\n",
            "epoch [5/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.3818 (0.6498) lr 1.5878e-03 eta 0:01:47\n",
            "epoch [6/10] batch [20/592] time 0.034 (0.048) data 0.000 (0.012) loss 0.0422 (0.3051) lr 1.3090e-03 eta 0:02:21\n",
            "epoch [6/10] batch [40/592] time 0.034 (0.041) data 0.000 (0.006) loss 0.9048 (0.3696) lr 1.3090e-03 eta 0:02:00\n",
            "epoch [6/10] batch [60/592] time 0.034 (0.039) data 0.000 (0.004) loss 0.0048 (0.4775) lr 1.3090e-03 eta 0:01:52\n",
            "epoch [6/10] batch [80/592] time 0.035 (0.038) data 0.000 (0.003) loss 0.2018 (0.4821) lr 1.3090e-03 eta 0:01:48\n",
            "epoch [6/10] batch [100/592] time 0.035 (0.037) data 0.000 (0.003) loss 0.0182 (0.4705) lr 1.3090e-03 eta 0:01:46\n",
            "epoch [6/10] batch [120/592] time 0.036 (0.037) data 0.000 (0.002) loss 0.0001 (0.5531) lr 1.3090e-03 eta 0:01:45\n",
            "epoch [6/10] batch [140/592] time 0.035 (0.037) data 0.000 (0.002) loss 2.1035 (0.5150) lr 1.3090e-03 eta 0:01:44\n",
            "epoch [6/10] batch [160/592] time 0.040 (0.037) data 0.000 (0.002) loss 0.0287 (0.4799) lr 1.3090e-03 eta 0:01:43\n",
            "epoch [6/10] batch [180/592] time 0.038 (0.037) data 0.000 (0.002) loss 0.0014 (0.4738) lr 1.3090e-03 eta 0:01:43\n",
            "epoch [6/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.1935 (0.4801) lr 1.3090e-03 eta 0:01:42\n",
            "epoch [6/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1453 (0.4616) lr 1.3090e-03 eta 0:01:40\n",
            "epoch [6/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1879 (0.5340) lr 1.3090e-03 eta 0:01:39\n",
            "epoch [6/10] batch [260/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0193 (0.5110) lr 1.3090e-03 eta 0:01:38\n",
            "epoch [6/10] batch [280/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0063 (0.5026) lr 1.3090e-03 eta 0:01:37\n",
            "epoch [6/10] batch [300/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0234 (0.5087) lr 1.3090e-03 eta 0:01:36\n",
            "epoch [6/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0519 (0.5112) lr 1.3090e-03 eta 0:01:35\n",
            "epoch [6/10] batch [340/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0188 (0.5518) lr 1.3090e-03 eta 0:01:34\n",
            "epoch [6/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0014 (0.5591) lr 1.3090e-03 eta 0:01:33\n",
            "epoch [6/10] batch [380/592] time 0.037 (0.036) data 0.000 (0.001) loss 3.6582 (0.5595) lr 1.3090e-03 eta 0:01:33\n",
            "epoch [6/10] batch [400/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.4871 (0.5662) lr 1.3090e-03 eta 0:01:32\n",
            "epoch [6/10] batch [420/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.9644 (0.5849) lr 1.3090e-03 eta 0:01:31\n",
            "epoch [6/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0623 (0.5820) lr 1.3090e-03 eta 0:01:30\n",
            "epoch [6/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0037 (0.5690) lr 1.3090e-03 eta 0:01:29\n",
            "epoch [6/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0063 (0.5648) lr 1.3090e-03 eta 0:01:29\n",
            "epoch [6/10] batch [500/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0039 (0.5757) lr 1.3090e-03 eta 0:01:28\n",
            "epoch [6/10] batch [520/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0638 (0.5773) lr 1.3090e-03 eta 0:01:27\n",
            "epoch [6/10] batch [540/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.4814 (0.5717) lr 1.3090e-03 eta 0:01:26\n",
            "epoch [6/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.4827 (0.5707) lr 1.3090e-03 eta 0:01:26\n",
            "epoch [6/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0244 (0.5745) lr 1.3090e-03 eta 0:01:25\n",
            "epoch [7/10] batch [20/592] time 0.038 (0.050) data 0.000 (0.014) loss 0.2046 (0.2609) lr 1.0000e-03 eta 0:01:58\n",
            "epoch [7/10] batch [40/592] time 0.034 (0.043) data 0.000 (0.007) loss 0.0101 (0.2150) lr 1.0000e-03 eta 0:01:39\n",
            "epoch [7/10] batch [60/592] time 0.039 (0.041) data 0.000 (0.005) loss 1.0674 (0.4377) lr 1.0000e-03 eta 0:01:33\n",
            "epoch [7/10] batch [80/592] time 0.036 (0.040) data 0.000 (0.004) loss 0.1534 (0.4070) lr 1.0000e-03 eta 0:01:31\n",
            "epoch [7/10] batch [100/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.0000 (0.4525) lr 1.0000e-03 eta 0:01:28\n",
            "epoch [7/10] batch [120/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0138 (0.5152) lr 1.0000e-03 eta 0:01:26\n",
            "epoch [7/10] batch [140/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.8921 (0.5494) lr 1.0000e-03 eta 0:01:24\n",
            "epoch [7/10] batch [160/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0031 (0.5235) lr 1.0000e-03 eta 0:01:22\n",
            "epoch [7/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0296 (0.5576) lr 1.0000e-03 eta 0:01:21\n",
            "epoch [7/10] batch [200/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.1609 (0.6145) lr 1.0000e-03 eta 0:01:20\n",
            "epoch [7/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0653 (0.5949) lr 1.0000e-03 eta 0:01:18\n",
            "epoch [7/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.3799 (0.5905) lr 1.0000e-03 eta 0:01:17\n",
            "epoch [7/10] batch [260/592] time 0.033 (0.036) data 0.000 (0.001) loss 7.9180 (0.6058) lr 1.0000e-03 eta 0:01:16\n",
            "epoch [7/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1494 (0.5932) lr 1.0000e-03 eta 0:01:15\n",
            "epoch [7/10] batch [300/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.6611 (0.5832) lr 1.0000e-03 eta 0:01:14\n",
            "epoch [7/10] batch [320/592] time 0.036 (0.036) data 0.000 (0.001) loss 3.5859 (0.5738) lr 1.0000e-03 eta 0:01:13\n",
            "epoch [7/10] batch [340/592] time 0.041 (0.036) data 0.000 (0.001) loss 0.6245 (0.5829) lr 1.0000e-03 eta 0:01:13\n",
            "epoch [7/10] batch [360/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.7563 (0.5967) lr 1.0000e-03 eta 0:01:12\n",
            "epoch [7/10] batch [380/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0048 (0.5961) lr 1.0000e-03 eta 0:01:11\n",
            "epoch [7/10] batch [400/592] time 0.040 (0.036) data 0.000 (0.001) loss 0.0174 (0.6069) lr 1.0000e-03 eta 0:01:11\n",
            "epoch [7/10] batch [420/592] time 0.033 (0.036) data 0.000 (0.001) loss 1.5342 (0.6183) lr 1.0000e-03 eta 0:01:10\n",
            "epoch [7/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0006 (0.6268) lr 1.0000e-03 eta 0:01:09\n",
            "epoch [7/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.4526 (0.6312) lr 1.0000e-03 eta 0:01:08\n",
            "epoch [7/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0960 (0.6333) lr 1.0000e-03 eta 0:01:07\n",
            "epoch [7/10] batch [500/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0089 (0.6339) lr 1.0000e-03 eta 0:01:06\n",
            "epoch [7/10] batch [520/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.1395 (0.6379) lr 1.0000e-03 eta 0:01:06\n",
            "epoch [7/10] batch [540/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0172 (0.6543) lr 1.0000e-03 eta 0:01:05\n",
            "epoch [7/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0075 (0.6538) lr 1.0000e-03 eta 0:01:04\n",
            "epoch [7/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0032 (0.6527) lr 1.0000e-03 eta 0:01:03\n",
            "epoch [8/10] batch [20/592] time 0.034 (0.050) data 0.000 (0.013) loss 0.0197 (0.4165) lr 6.9098e-04 eta 0:01:27\n",
            "epoch [8/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 3.0859 (0.4299) lr 6.9098e-04 eta 0:01:14\n",
            "epoch [8/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.005) loss 0.0836 (0.5949) lr 6.9098e-04 eta 0:01:08\n",
            "epoch [8/10] batch [80/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.0235 (0.5769) lr 6.9098e-04 eta 0:01:06\n",
            "epoch [8/10] batch [100/592] time 0.034 (0.038) data 0.000 (0.003) loss 0.0343 (0.5924) lr 6.9098e-04 eta 0:01:04\n",
            "epoch [8/10] batch [120/592] time 0.037 (0.038) data 0.000 (0.002) loss 0.0512 (0.5920) lr 6.9098e-04 eta 0:01:02\n",
            "epoch [8/10] batch [140/592] time 0.036 (0.038) data 0.000 (0.002) loss 0.0166 (0.6302) lr 6.9098e-04 eta 0:01:01\n",
            "epoch [8/10] batch [160/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.0159 (0.6073) lr 6.9098e-04 eta 0:01:00\n",
            "epoch [8/10] batch [180/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.2413 (0.6132) lr 6.9098e-04 eta 0:00:59\n",
            "epoch [8/10] batch [200/592] time 0.039 (0.037) data 0.000 (0.002) loss 0.0585 (0.5908) lr 6.9098e-04 eta 0:00:58\n",
            "epoch [8/10] batch [220/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.0054 (0.5726) lr 6.9098e-04 eta 0:00:57\n",
            "epoch [8/10] batch [240/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.1335 (0.5883) lr 6.9098e-04 eta 0:00:56\n",
            "epoch [8/10] batch [260/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0792 (0.5757) lr 6.9098e-04 eta 0:00:55\n",
            "epoch [8/10] batch [280/592] time 0.039 (0.037) data 0.000 (0.001) loss 1.6113 (0.5828) lr 6.9098e-04 eta 0:00:54\n",
            "epoch [8/10] batch [300/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.2413 (0.5730) lr 6.9098e-04 eta 0:00:53\n",
            "epoch [8/10] batch [320/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0669 (0.5552) lr 6.9098e-04 eta 0:00:52\n",
            "epoch [8/10] batch [340/592] time 0.040 (0.036) data 0.000 (0.001) loss 0.6680 (0.5835) lr 6.9098e-04 eta 0:00:52\n",
            "epoch [8/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0996 (0.5989) lr 6.9098e-04 eta 0:00:51\n",
            "epoch [8/10] batch [380/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.3784 (0.5846) lr 6.9098e-04 eta 0:00:50\n",
            "epoch [8/10] batch [400/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.4326 (0.5764) lr 6.9098e-04 eta 0:00:49\n",
            "epoch [8/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0620 (0.5801) lr 6.9098e-04 eta 0:00:48\n",
            "epoch [8/10] batch [440/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0398 (0.5849) lr 6.9098e-04 eta 0:00:48\n",
            "epoch [8/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.3730 (0.5835) lr 6.9098e-04 eta 0:00:47\n",
            "epoch [8/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0161 (0.5750) lr 6.9098e-04 eta 0:00:46\n",
            "epoch [8/10] batch [500/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0040 (0.5669) lr 6.9098e-04 eta 0:00:45\n",
            "epoch [8/10] batch [520/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.1630 (0.5643) lr 6.9098e-04 eta 0:00:45\n",
            "epoch [8/10] batch [540/592] time 0.037 (0.036) data 0.000 (0.001) loss 2.9629 (0.5639) lr 6.9098e-04 eta 0:00:44\n",
            "epoch [8/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.3525 (0.5681) lr 6.9098e-04 eta 0:00:43\n",
            "epoch [8/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.3076 (0.5675) lr 6.9098e-04 eta 0:00:42\n",
            "epoch [9/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 3.0566 (0.4371) lr 4.1221e-04 eta 0:00:58\n",
            "epoch [9/10] batch [40/592] time 0.034 (0.043) data 0.000 (0.006) loss 0.1885 (0.4027) lr 4.1221e-04 eta 0:00:49\n",
            "epoch [9/10] batch [60/592] time 0.036 (0.041) data 0.000 (0.004) loss 0.5737 (0.4567) lr 4.1221e-04 eta 0:00:45\n",
            "epoch [9/10] batch [80/592] time 0.034 (0.039) data 0.000 (0.003) loss 1.0186 (0.4924) lr 4.1221e-04 eta 0:00:43\n",
            "epoch [9/10] batch [100/592] time 0.033 (0.038) data 0.000 (0.003) loss 0.2769 (0.5032) lr 4.1221e-04 eta 0:00:41\n",
            "epoch [9/10] batch [120/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0090 (0.4829) lr 4.1221e-04 eta 0:00:40\n",
            "epoch [9/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.1443 (0.5300) lr 4.1221e-04 eta 0:00:39\n",
            "epoch [9/10] batch [160/592] time 0.038 (0.037) data 0.000 (0.002) loss 0.0898 (0.5164) lr 4.1221e-04 eta 0:00:38\n",
            "epoch [9/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 1.8779 (0.5136) lr 4.1221e-04 eta 0:00:37\n",
            "epoch [9/10] batch [200/592] time 0.041 (0.037) data 0.000 (0.001) loss 0.0289 (0.5558) lr 4.1221e-04 eta 0:00:36\n",
            "epoch [9/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0492 (0.5922) lr 4.1221e-04 eta 0:00:35\n",
            "epoch [9/10] batch [240/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0287 (0.5933) lr 4.1221e-04 eta 0:00:34\n",
            "epoch [9/10] batch [260/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1655 (0.6053) lr 4.1221e-04 eta 0:00:33\n",
            "epoch [9/10] batch [280/592] time 0.037 (0.036) data 0.000 (0.001) loss 2.8047 (0.6117) lr 4.1221e-04 eta 0:00:32\n",
            "epoch [9/10] batch [300/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0007 (0.6530) lr 4.1221e-04 eta 0:00:32\n",
            "epoch [9/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2488 (0.6489) lr 4.1221e-04 eta 0:00:31\n",
            "epoch [9/10] batch [340/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0113 (0.6339) lr 4.1221e-04 eta 0:00:30\n",
            "epoch [9/10] batch [360/592] time 0.044 (0.036) data 0.000 (0.001) loss 0.0248 (0.6182) lr 4.1221e-04 eta 0:00:29\n",
            "epoch [9/10] batch [380/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.6343 (0.6290) lr 4.1221e-04 eta 0:00:29\n",
            "epoch [9/10] batch [400/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0648 (0.6389) lr 4.1221e-04 eta 0:00:28\n",
            "epoch [9/10] batch [420/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.4490 (0.6457) lr 4.1221e-04 eta 0:00:27\n",
            "epoch [9/10] batch [440/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0273 (0.6292) lr 4.1221e-04 eta 0:00:26\n",
            "epoch [9/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.3306 (0.6214) lr 4.1221e-04 eta 0:00:26\n",
            "epoch [9/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0218 (0.6335) lr 4.1221e-04 eta 0:00:25\n",
            "epoch [9/10] batch [500/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0020 (0.6138) lr 4.1221e-04 eta 0:00:24\n",
            "epoch [9/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2220 (0.6012) lr 4.1221e-04 eta 0:00:23\n",
            "epoch [9/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0057 (0.5966) lr 4.1221e-04 eta 0:00:23\n",
            "epoch [9/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0316 (0.5947) lr 4.1221e-04 eta 0:00:22\n",
            "epoch [9/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.1012 (0.5919) lr 4.1221e-04 eta 0:00:21\n",
            "epoch [10/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 0.5874 (0.8678) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 0.3499 (0.6238) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.005) loss 3.1992 (0.5179) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [10/10] batch [80/592] time 0.038 (0.039) data 0.000 (0.004) loss 1.3730 (0.5115) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [100/592] time 0.035 (0.038) data 0.000 (0.003) loss 0.5791 (0.6385) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [120/592] time 0.035 (0.038) data 0.000 (0.002) loss 4.2578 (0.6947) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [10/10] batch [140/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.0297 (0.6304) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [160/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.2544 (0.6070) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [180/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.1318 (0.5713) lr 1.9098e-04 eta 0:00:15\n",
            "epoch [10/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0412 (0.6127) lr 1.9098e-04 eta 0:00:14\n",
            "epoch [10/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0273 (0.6029) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 3.3223 (0.6516) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 1.1602 (0.6407) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [280/592] time 0.033 (0.037) data 0.000 (0.001) loss 0.1890 (0.6274) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [10/10] batch [300/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.4072 (0.6141) lr 1.9098e-04 eta 0:00:10\n",
            "epoch [10/10] batch [320/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0242 (0.5940) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [340/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0351 (0.5763) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2817 (0.5748) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.8711 (0.5596) lr 1.9098e-04 eta 0:00:07\n",
            "epoch [10/10] batch [400/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.3394 (0.5522) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0496 (0.5555) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0609 (0.5534) lr 1.9098e-04 eta 0:00:05\n",
            "epoch [10/10] batch [460/592] time 0.039 (0.036) data 0.000 (0.001) loss 5.6328 (0.5563) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [10/10] batch [480/592] time 0.035 (0.036) data 0.000 (0.001) loss 1.1426 (0.5606) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0659 (0.5741) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [520/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0014 (0.5795) lr 1.9098e-04 eta 0:00:02\n",
            "epoch [10/10] batch [540/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0067 (0.5656) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0695 (0.5657) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0019 (0.5635) lr 1.9098e-04 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-10\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:40<00:00,  1.08s/it]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,403\n",
            "* accuracy: 92.8%\n",
            "* error: 7.2%\n",
            "* macro_f1: 92.7%\n",
            "Elapsed: 0:04:23\n",
            "2024-12-10 12:01:28.844120: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-10 12:01:28.861402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-10 12:01:28.882605: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-10 12:01:28.889033: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-10 12:01:28.903981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-10 12:01:29.952948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/MIP/vit_b16_c4_ep10_batch1.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: 0\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2\n",
            "resume: \n",
            "root: data/\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: MIP\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: data/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: MIP\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               12\n",
            "On-line CPU(s) list:                  0-11\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   6\n",
            "Socket(s):                            1\n",
            "Stepping:                             7\n",
            "BogoMIPS:                             4400.44\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            192 KiB (6 instances)\n",
            "L1i cache:                            192 KiB (6 instances)\n",
            "L2 cache:                             6 MiB (6 instances)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-11\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Not affected\n",
            "Vulnerability Mds:                    Not affected\n",
            "Vulnerability Meltdown:               Not affected\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: MIP\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets/split_fewshot/shot_16-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.meta_net.linear2.bias', 'prompt_learner.ctx', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear1.weight', 'prompt_learner.meta_net.linear1.bias'}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2/tensorboard)\n",
            "epoch [1/10] batch [20/592] time 0.039 (0.127) data 0.000 (0.016) loss 0.8682 (1.8021) lr 1.0000e-05 eta 0:12:27\n",
            "epoch [1/10] batch [40/592] time 0.035 (0.084) data 0.000 (0.012) loss 5.5547 (1.9415) lr 1.0000e-05 eta 0:08:14\n",
            "epoch [1/10] batch [60/592] time 0.033 (0.073) data 0.000 (0.013) loss 7.4922 (1.9388) lr 1.0000e-05 eta 0:07:05\n",
            "epoch [1/10] batch [80/592] time 0.032 (0.063) data 0.000 (0.010) loss 0.0875 (1.8153) lr 1.0000e-05 eta 0:06:07\n",
            "epoch [1/10] batch [100/592] time 0.131 (0.062) data 0.098 (0.013) loss 2.6934 (1.6611) lr 1.0000e-05 eta 0:06:02\n",
            "epoch [1/10] batch [120/592] time 0.038 (0.059) data 0.000 (0.011) loss 0.2615 (1.5425) lr 1.0000e-05 eta 0:05:40\n",
            "epoch [1/10] batch [140/592] time 0.035 (0.057) data 0.000 (0.012) loss 2.8301 (1.5704) lr 1.0000e-05 eta 0:05:30\n",
            "epoch [1/10] batch [160/592] time 0.036 (0.057) data 0.000 (0.013) loss 0.3894 (1.5330) lr 1.0000e-05 eta 0:05:26\n",
            "epoch [1/10] batch [180/592] time 0.033 (0.056) data 0.000 (0.013) loss 0.0173 (1.4520) lr 1.0000e-05 eta 0:05:22\n",
            "epoch [1/10] batch [200/592] time 0.035 (0.060) data 0.000 (0.018) loss 0.1300 (1.3958) lr 1.0000e-05 eta 0:05:42\n",
            "epoch [1/10] batch [220/592] time 0.034 (0.059) data 0.000 (0.017) loss 1.0088 (1.3871) lr 1.0000e-05 eta 0:05:35\n",
            "epoch [1/10] batch [240/592] time 0.033 (0.058) data 0.000 (0.016) loss 5.2656 (1.3414) lr 1.0000e-05 eta 0:05:26\n",
            "epoch [1/10] batch [260/592] time 0.033 (0.056) data 0.001 (0.016) loss 4.3828 (1.3040) lr 1.0000e-05 eta 0:05:19\n",
            "epoch [1/10] batch [280/592] time 0.041 (0.060) data 0.000 (0.019) loss 0.7866 (1.2848) lr 1.0000e-05 eta 0:05:37\n",
            "epoch [1/10] batch [300/592] time 0.125 (0.058) data 0.000 (0.018) loss 0.4287 (1.2998) lr 1.0000e-05 eta 0:05:28\n",
            "epoch [1/10] batch [320/592] time 0.035 (0.057) data 0.000 (0.018) loss 0.1409 (1.2841) lr 1.0000e-05 eta 0:05:21\n",
            "epoch [1/10] batch [340/592] time 0.034 (0.056) data 0.000 (0.017) loss 2.2266 (1.2729) lr 1.0000e-05 eta 0:05:13\n",
            "epoch [1/10] batch [360/592] time 0.034 (0.055) data 0.000 (0.016) loss 0.1975 (1.2699) lr 1.0000e-05 eta 0:05:07\n",
            "epoch [1/10] batch [380/592] time 0.037 (0.054) data 0.000 (0.015) loss 3.4980 (1.2661) lr 1.0000e-05 eta 0:05:00\n",
            "epoch [1/10] batch [400/592] time 0.032 (0.054) data 0.000 (0.015) loss 0.6802 (1.2523) lr 1.0000e-05 eta 0:04:57\n",
            "epoch [1/10] batch [420/592] time 0.138 (0.054) data 0.100 (0.016) loss 0.6582 (1.2581) lr 1.0000e-05 eta 0:04:59\n",
            "epoch [1/10] batch [440/592] time 0.038 (0.054) data 0.000 (0.015) loss 3.2109 (1.2716) lr 1.0000e-05 eta 0:04:53\n",
            "epoch [1/10] batch [460/592] time 0.035 (0.053) data 0.000 (0.015) loss 2.2578 (1.2629) lr 1.0000e-05 eta 0:04:49\n",
            "epoch [1/10] batch [480/592] time 0.035 (0.058) data 0.000 (0.020) loss 0.2656 (1.2359) lr 1.0000e-05 eta 0:05:17\n",
            "epoch [1/10] batch [500/592] time 0.038 (0.057) data 0.000 (0.019) loss 0.0933 (1.2458) lr 1.0000e-05 eta 0:05:11\n",
            "epoch [1/10] batch [520/592] time 0.032 (0.057) data 0.000 (0.019) loss 0.2849 (1.2291) lr 1.0000e-05 eta 0:05:07\n",
            "epoch [1/10] batch [540/592] time 0.032 (0.057) data 0.000 (0.019) loss 0.0203 (1.2083) lr 1.0000e-05 eta 0:05:07\n",
            "epoch [1/10] batch [560/592] time 0.033 (0.056) data 0.000 (0.019) loss 1.2979 (1.2112) lr 1.0000e-05 eta 0:05:02\n",
            "epoch [1/10] batch [580/592] time 0.033 (0.056) data 0.000 (0.018) loss 0.0415 (1.1983) lr 1.0000e-05 eta 0:04:57\n",
            "epoch [2/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 0.1970 (0.7614) lr 2.0000e-03 eta 0:04:24\n",
            "epoch [2/10] batch [40/592] time 0.035 (0.042) data 0.000 (0.007) loss 0.5464 (0.6699) lr 2.0000e-03 eta 0:03:44\n",
            "epoch [2/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.005) loss 0.1125 (0.7082) lr 2.0000e-03 eta 0:03:30\n",
            "epoch [2/10] batch [80/592] time 0.034 (0.039) data 0.000 (0.004) loss 0.0018 (0.6464) lr 2.0000e-03 eta 0:03:23\n",
            "epoch [2/10] batch [100/592] time 0.036 (0.038) data 0.000 (0.003) loss 0.0168 (0.6078) lr 2.0000e-03 eta 0:03:18\n",
            "epoch [2/10] batch [120/592] time 0.038 (0.037) data 0.000 (0.002) loss 0.6904 (0.6492) lr 2.0000e-03 eta 0:03:15\n",
            "epoch [2/10] batch [140/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0263 (0.5794) lr 2.0000e-03 eta 0:03:12\n",
            "epoch [2/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0625 (0.6585) lr 2.0000e-03 eta 0:03:11\n",
            "epoch [2/10] batch [180/592] time 0.036 (0.037) data 0.001 (0.002) loss 0.9121 (0.6616) lr 2.0000e-03 eta 0:03:10\n",
            "epoch [2/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.002) loss 3.0977 (0.6980) lr 2.0000e-03 eta 0:03:09\n",
            "epoch [2/10] batch [220/592] time 0.041 (0.037) data 0.000 (0.001) loss 0.5020 (0.6812) lr 2.0000e-03 eta 0:03:08\n",
            "epoch [2/10] batch [240/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.3193 (0.7100) lr 2.0000e-03 eta 0:03:07\n",
            "epoch [2/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 4.0156 (0.7517) lr 2.0000e-03 eta 0:03:06\n",
            "epoch [2/10] batch [280/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.0280 (0.7454) lr 2.0000e-03 eta 0:03:05\n",
            "epoch [2/10] batch [300/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0056 (0.7324) lr 2.0000e-03 eta 0:03:04\n",
            "epoch [2/10] batch [320/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0020 (0.7223) lr 2.0000e-03 eta 0:03:03\n",
            "epoch [2/10] batch [340/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0281 (0.6888) lr 2.0000e-03 eta 0:03:02\n",
            "epoch [2/10] batch [360/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.0055 (0.6880) lr 2.0000e-03 eta 0:03:01\n",
            "epoch [2/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0086 (0.6829) lr 2.0000e-03 eta 0:03:00\n",
            "epoch [2/10] batch [400/592] time 0.042 (0.036) data 0.000 (0.001) loss 0.0352 (0.6888) lr 2.0000e-03 eta 0:02:59\n",
            "epoch [2/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0400 (0.7032) lr 2.0000e-03 eta 0:02:58\n",
            "epoch [2/10] batch [440/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.1559 (0.7122) lr 2.0000e-03 eta 0:02:57\n",
            "epoch [2/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1189 (0.6942) lr 2.0000e-03 eta 0:02:56\n",
            "epoch [2/10] batch [480/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0641 (0.6894) lr 2.0000e-03 eta 0:02:55\n",
            "epoch [2/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0924 (0.6876) lr 2.0000e-03 eta 0:02:54\n",
            "epoch [2/10] batch [520/592] time 0.037 (0.036) data 0.000 (0.001) loss 1.2715 (0.6835) lr 2.0000e-03 eta 0:02:53\n",
            "epoch [2/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.8413 (0.6916) lr 2.0000e-03 eta 0:02:52\n",
            "epoch [2/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 5.5586 (0.7038) lr 2.0000e-03 eta 0:02:52\n",
            "epoch [2/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0883 (0.6910) lr 2.0000e-03 eta 0:02:51\n",
            "epoch [3/10] batch [20/592] time 0.036 (0.050) data 0.000 (0.013) loss 0.1188 (0.4967) lr 1.9511e-03 eta 0:03:57\n",
            "epoch [3/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 0.0524 (0.4921) lr 1.9511e-03 eta 0:03:20\n",
            "epoch [3/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.005) loss 5.0859 (0.5565) lr 1.9511e-03 eta 0:03:08\n",
            "epoch [3/10] batch [80/592] time 0.038 (0.039) data 0.000 (0.004) loss 0.0120 (0.5522) lr 1.9511e-03 eta 0:03:02\n",
            "epoch [3/10] batch [100/592] time 0.035 (0.038) data 0.000 (0.003) loss 1.4268 (0.5424) lr 1.9511e-03 eta 0:02:57\n",
            "epoch [3/10] batch [120/592] time 0.038 (0.038) data 0.000 (0.002) loss 1.1084 (0.5297) lr 1.9511e-03 eta 0:02:54\n",
            "epoch [3/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0308 (0.6102) lr 1.9511e-03 eta 0:02:50\n",
            "epoch [3/10] batch [160/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0108 (0.5889) lr 1.9511e-03 eta 0:02:48\n",
            "epoch [3/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 5.4883 (0.6381) lr 1.9511e-03 eta 0:02:46\n",
            "epoch [3/10] batch [200/592] time 0.034 (0.036) data 0.000 (0.002) loss 0.0018 (0.6462) lr 1.9511e-03 eta 0:02:45\n",
            "epoch [3/10] batch [220/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0063 (0.6234) lr 1.9511e-03 eta 0:02:44\n",
            "epoch [3/10] batch [240/592] time 0.035 (0.036) data 0.000 (0.001) loss 4.4141 (0.6149) lr 1.9511e-03 eta 0:02:43\n",
            "epoch [3/10] batch [260/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0009 (0.6066) lr 1.9511e-03 eta 0:02:42\n",
            "epoch [3/10] batch [280/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0012 (0.6302) lr 1.9511e-03 eta 0:02:41\n",
            "epoch [3/10] batch [300/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0009 (0.6071) lr 1.9511e-03 eta 0:02:40\n",
            "epoch [3/10] batch [320/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0629 (0.6082) lr 1.9511e-03 eta 0:02:39\n",
            "epoch [3/10] batch [340/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0709 (0.6046) lr 1.9511e-03 eta 0:02:38\n",
            "epoch [3/10] batch [360/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0908 (0.6214) lr 1.9511e-03 eta 0:02:37\n",
            "epoch [3/10] batch [380/592] time 0.035 (0.036) data 0.000 (0.001) loss 1.4980 (0.6345) lr 1.9511e-03 eta 0:02:37\n",
            "epoch [3/10] batch [400/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0078 (0.6399) lr 1.9511e-03 eta 0:02:36\n",
            "epoch [3/10] batch [420/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.5039 (0.6500) lr 1.9511e-03 eta 0:02:36\n",
            "epoch [3/10] batch [440/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.4985 (0.6487) lr 1.9511e-03 eta 0:02:35\n",
            "epoch [3/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.1594 (0.6324) lr 1.9511e-03 eta 0:02:34\n",
            "epoch [3/10] batch [480/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0224 (0.6200) lr 1.9511e-03 eta 0:02:33\n",
            "epoch [3/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0072 (0.6168) lr 1.9511e-03 eta 0:02:32\n",
            "epoch [3/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0006 (0.6063) lr 1.9511e-03 eta 0:02:31\n",
            "epoch [3/10] batch [540/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.1705 (0.6113) lr 1.9511e-03 eta 0:02:30\n",
            "epoch [3/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 1.4912 (0.6008) lr 1.9511e-03 eta 0:02:29\n",
            "epoch [3/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.6294 (0.6144) lr 1.9511e-03 eta 0:02:28\n",
            "epoch [4/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 1.2939 (0.5449) lr 1.8090e-03 eta 0:03:27\n",
            "epoch [4/10] batch [40/592] time 0.034 (0.043) data 0.000 (0.007) loss 0.3577 (0.3522) lr 1.8090e-03 eta 0:02:57\n",
            "epoch [4/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.004) loss 0.0859 (0.4897) lr 1.8090e-03 eta 0:02:45\n",
            "epoch [4/10] batch [80/592] time 0.039 (0.039) data 0.000 (0.003) loss 0.2410 (0.5369) lr 1.8090e-03 eta 0:02:39\n",
            "epoch [4/10] batch [100/592] time 0.034 (0.039) data 0.000 (0.003) loss 0.0238 (0.4943) lr 1.8090e-03 eta 0:02:37\n",
            "epoch [4/10] batch [120/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.3174 (0.5056) lr 1.8090e-03 eta 0:02:34\n",
            "epoch [4/10] batch [140/592] time 0.036 (0.038) data 0.000 (0.002) loss 0.0022 (0.4868) lr 1.8090e-03 eta 0:02:32\n",
            "epoch [4/10] batch [160/592] time 0.036 (0.038) data 0.000 (0.002) loss 0.4014 (0.4740) lr 1.8090e-03 eta 0:02:30\n",
            "epoch [4/10] batch [180/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0141 (0.4597) lr 1.8090e-03 eta 0:02:30\n",
            "epoch [4/10] batch [200/592] time 0.037 (0.038) data 0.000 (0.002) loss 1.4502 (0.4440) lr 1.8090e-03 eta 0:02:28\n",
            "epoch [4/10] batch [220/592] time 0.035 (0.038) data 0.000 (0.001) loss 0.2742 (0.4240) lr 1.8090e-03 eta 0:02:27\n",
            "epoch [4/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1797 (0.4011) lr 1.8090e-03 eta 0:02:26\n",
            "epoch [4/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0247 (0.3984) lr 1.8090e-03 eta 0:02:24\n",
            "epoch [4/10] batch [280/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1614 (0.4145) lr 1.8090e-03 eta 0:02:23\n",
            "epoch [4/10] batch [300/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.4062 (0.4141) lr 1.8090e-03 eta 0:02:22\n",
            "epoch [4/10] batch [320/592] time 0.034 (0.037) data 0.000 (0.001) loss 2.3594 (0.4415) lr 1.8090e-03 eta 0:02:20\n",
            "epoch [4/10] batch [340/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0207 (0.4540) lr 1.8090e-03 eta 0:02:19\n",
            "epoch [4/10] batch [360/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0059 (0.4674) lr 1.8090e-03 eta 0:02:18\n",
            "epoch [4/10] batch [380/592] time 0.036 (0.037) data 0.000 (0.001) loss 2.4375 (0.4724) lr 1.8090e-03 eta 0:02:17\n",
            "epoch [4/10] batch [400/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0274 (0.4745) lr 1.8090e-03 eta 0:02:16\n",
            "epoch [4/10] batch [420/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0216 (0.4855) lr 1.8090e-03 eta 0:02:15\n",
            "epoch [4/10] batch [440/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.1268 (0.4957) lr 1.8090e-03 eta 0:02:14\n",
            "epoch [4/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2198 (0.4980) lr 1.8090e-03 eta 0:02:13\n",
            "epoch [4/10] batch [480/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.3423 (0.5089) lr 1.8090e-03 eta 0:02:13\n",
            "epoch [4/10] batch [500/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0659 (0.5104) lr 1.8090e-03 eta 0:02:12\n",
            "epoch [4/10] batch [520/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.5527 (0.5210) lr 1.8090e-03 eta 0:02:11\n",
            "epoch [4/10] batch [540/592] time 0.040 (0.036) data 0.000 (0.001) loss 1.8623 (0.5332) lr 1.8090e-03 eta 0:02:11\n",
            "epoch [4/10] batch [560/592] time 0.038 (0.036) data 0.000 (0.001) loss 2.4336 (0.5356) lr 1.8090e-03 eta 0:02:10\n",
            "epoch [4/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0549 (0.5556) lr 1.8090e-03 eta 0:02:09\n",
            "epoch [5/10] batch [20/592] time 0.036 (0.050) data 0.000 (0.014) loss 0.0047 (0.2495) lr 1.5878e-03 eta 0:02:58\n",
            "epoch [5/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 0.0145 (0.3260) lr 1.5878e-03 eta 0:02:31\n",
            "epoch [5/10] batch [60/592] time 0.041 (0.041) data 0.000 (0.005) loss 0.1678 (0.4777) lr 1.5878e-03 eta 0:02:21\n",
            "epoch [5/10] batch [80/592] time 0.035 (0.039) data 0.000 (0.004) loss 0.0221 (0.5407) lr 1.5878e-03 eta 0:02:16\n",
            "epoch [5/10] batch [100/592] time 0.037 (0.039) data 0.000 (0.003) loss 0.3447 (0.5452) lr 1.5878e-03 eta 0:02:12\n",
            "epoch [5/10] batch [120/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0041 (0.5711) lr 1.5878e-03 eta 0:02:10\n",
            "epoch [5/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.1608 (0.5214) lr 1.5878e-03 eta 0:02:07\n",
            "epoch [5/10] batch [160/592] time 0.036 (0.037) data 0.000 (0.002) loss 0.0251 (0.5302) lr 1.5878e-03 eta 0:02:06\n",
            "epoch [5/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0958 (0.5039) lr 1.5878e-03 eta 0:02:04\n",
            "epoch [5/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.4978 (0.5331) lr 1.5878e-03 eta 0:02:03\n",
            "epoch [5/10] batch [220/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.6836 (0.5578) lr 1.5878e-03 eta 0:02:01\n",
            "epoch [5/10] batch [240/592] time 0.034 (0.036) data 0.000 (0.001) loss 3.5645 (0.5826) lr 1.5878e-03 eta 0:02:00\n",
            "epoch [5/10] batch [260/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.6543 (0.5895) lr 1.5878e-03 eta 0:01:59\n",
            "epoch [5/10] batch [280/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0333 (0.6024) lr 1.5878e-03 eta 0:01:58\n",
            "epoch [5/10] batch [300/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0102 (0.6209) lr 1.5878e-03 eta 0:01:57\n",
            "epoch [5/10] batch [320/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.4602 (0.6089) lr 1.5878e-03 eta 0:01:56\n",
            "epoch [5/10] batch [340/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.3291 (0.5897) lr 1.5878e-03 eta 0:01:55\n",
            "epoch [5/10] batch [360/592] time 0.040 (0.036) data 0.000 (0.001) loss 1.3242 (0.5890) lr 1.5878e-03 eta 0:01:55\n",
            "epoch [5/10] batch [380/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0416 (0.5847) lr 1.5878e-03 eta 0:01:54\n",
            "epoch [5/10] batch [400/592] time 0.041 (0.036) data 0.000 (0.001) loss 0.0001 (0.5763) lr 1.5878e-03 eta 0:01:54\n",
            "epoch [5/10] batch [420/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0079 (0.5703) lr 1.5878e-03 eta 0:01:53\n",
            "epoch [5/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0370 (0.5545) lr 1.5878e-03 eta 0:01:52\n",
            "epoch [5/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.1254 (0.5467) lr 1.5878e-03 eta 0:01:51\n",
            "epoch [5/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1229 (0.5617) lr 1.5878e-03 eta 0:01:50\n",
            "epoch [5/10] batch [500/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0105 (0.5532) lr 1.5878e-03 eta 0:01:49\n",
            "epoch [5/10] batch [520/592] time 0.042 (0.036) data 0.000 (0.001) loss 0.1249 (0.5542) lr 1.5878e-03 eta 0:01:48\n",
            "epoch [5/10] batch [540/592] time 0.036 (0.036) data 0.000 (0.001) loss 1.1152 (0.5430) lr 1.5878e-03 eta 0:01:48\n",
            "epoch [5/10] batch [560/592] time 0.036 (0.036) data 0.000 (0.001) loss 4.2344 (0.5451) lr 1.5878e-03 eta 0:01:47\n",
            "epoch [5/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0348 (0.5464) lr 1.5878e-03 eta 0:01:46\n",
            "epoch [6/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.014) loss 0.1984 (0.5171) lr 1.3090e-03 eta 0:02:27\n",
            "epoch [6/10] batch [40/592] time 0.034 (0.042) data 0.000 (0.007) loss 0.0096 (0.4973) lr 1.3090e-03 eta 0:02:03\n",
            "epoch [6/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.005) loss 0.5269 (0.5713) lr 1.3090e-03 eta 0:01:56\n",
            "epoch [6/10] batch [80/592] time 0.040 (0.039) data 0.000 (0.004) loss 0.0036 (0.5290) lr 1.3090e-03 eta 0:01:52\n",
            "epoch [6/10] batch [100/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.6743 (0.6002) lr 1.3090e-03 eta 0:01:50\n",
            "epoch [6/10] batch [120/592] time 0.037 (0.038) data 0.000 (0.002) loss 0.0647 (0.6001) lr 1.3090e-03 eta 0:01:49\n",
            "epoch [6/10] batch [140/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.4465 (0.5587) lr 1.3090e-03 eta 0:01:47\n",
            "epoch [6/10] batch [160/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.1858 (0.5684) lr 1.3090e-03 eta 0:01:46\n",
            "epoch [6/10] batch [180/592] time 0.035 (0.038) data 0.000 (0.002) loss 2.1973 (0.5891) lr 1.3090e-03 eta 0:01:45\n",
            "epoch [6/10] batch [200/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.0222 (0.6020) lr 1.3090e-03 eta 0:01:44\n",
            "epoch [6/10] batch [220/592] time 0.034 (0.038) data 0.000 (0.001) loss 0.0405 (0.5819) lr 1.3090e-03 eta 0:01:43\n",
            "epoch [6/10] batch [240/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0305 (0.6310) lr 1.3090e-03 eta 0:01:41\n",
            "epoch [6/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 4.8359 (0.6287) lr 1.3090e-03 eta 0:01:40\n",
            "epoch [6/10] batch [280/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.0005 (0.6062) lr 1.3090e-03 eta 0:01:39\n",
            "epoch [6/10] batch [300/592] time 0.035 (0.037) data 0.000 (0.001) loss 1.4160 (0.6393) lr 1.3090e-03 eta 0:01:38\n",
            "epoch [6/10] batch [320/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.4866 (0.6395) lr 1.3090e-03 eta 0:01:37\n",
            "epoch [6/10] batch [340/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.0460 (0.6399) lr 1.3090e-03 eta 0:01:36\n",
            "epoch [6/10] batch [360/592] time 0.036 (0.037) data 0.000 (0.001) loss 3.4688 (0.6465) lr 1.3090e-03 eta 0:01:36\n",
            "epoch [6/10] batch [380/592] time 0.036 (0.037) data 0.000 (0.001) loss 1.0059 (0.6539) lr 1.3090e-03 eta 0:01:35\n",
            "epoch [6/10] batch [400/592] time 0.037 (0.037) data 0.000 (0.001) loss 2.2793 (0.6418) lr 1.3090e-03 eta 0:01:34\n",
            "epoch [6/10] batch [420/592] time 0.038 (0.037) data 0.000 (0.001) loss 3.2383 (0.6488) lr 1.3090e-03 eta 0:01:33\n",
            "epoch [6/10] batch [440/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.8833 (0.6349) lr 1.3090e-03 eta 0:01:32\n",
            "epoch [6/10] batch [460/592] time 0.038 (0.037) data 0.000 (0.001) loss 0.0838 (0.6349) lr 1.3090e-03 eta 0:01:31\n",
            "epoch [6/10] batch [480/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.0137 (0.6258) lr 1.3090e-03 eta 0:01:30\n",
            "epoch [6/10] batch [500/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1367 (0.6154) lr 1.3090e-03 eta 0:01:30\n",
            "epoch [6/10] batch [520/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.1659 (0.6205) lr 1.3090e-03 eta 0:01:29\n",
            "epoch [6/10] batch [540/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.7310 (0.6161) lr 1.3090e-03 eta 0:01:28\n",
            "epoch [6/10] batch [560/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.1512 (0.6100) lr 1.3090e-03 eta 0:01:27\n",
            "epoch [6/10] batch [580/592] time 0.032 (0.037) data 0.000 (0.001) loss 1.0742 (0.6161) lr 1.3090e-03 eta 0:01:26\n",
            "epoch [7/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 0.0687 (0.3134) lr 1.0000e-03 eta 0:01:56\n",
            "epoch [7/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 0.0465 (0.2641) lr 1.0000e-03 eta 0:01:39\n",
            "epoch [7/10] batch [60/592] time 0.034 (0.040) data 0.000 (0.005) loss 0.0106 (0.2540) lr 1.0000e-03 eta 0:01:32\n",
            "epoch [7/10] batch [80/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.0001 (0.3507) lr 1.0000e-03 eta 0:01:28\n",
            "epoch [7/10] batch [100/592] time 0.035 (0.038) data 0.000 (0.003) loss 0.0407 (0.4527) lr 1.0000e-03 eta 0:01:26\n",
            "epoch [7/10] batch [120/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0512 (0.4225) lr 1.0000e-03 eta 0:01:24\n",
            "epoch [7/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0378 (0.4671) lr 1.0000e-03 eta 0:01:22\n",
            "epoch [7/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.3960 (0.4445) lr 1.0000e-03 eta 0:01:21\n",
            "epoch [7/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.2285 (0.4425) lr 1.0000e-03 eta 0:01:19\n",
            "epoch [7/10] batch [200/592] time 0.034 (0.036) data 0.000 (0.002) loss 0.0919 (0.4479) lr 1.0000e-03 eta 0:01:19\n",
            "epoch [7/10] batch [220/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0029 (0.4224) lr 1.0000e-03 eta 0:01:18\n",
            "epoch [7/10] batch [240/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.2345 (0.4621) lr 1.0000e-03 eta 0:01:17\n",
            "epoch [7/10] batch [260/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0797 (0.4545) lr 1.0000e-03 eta 0:01:16\n",
            "epoch [7/10] batch [280/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0184 (0.4721) lr 1.0000e-03 eta 0:01:15\n",
            "epoch [7/10] batch [300/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0127 (0.4980) lr 1.0000e-03 eta 0:01:14\n",
            "epoch [7/10] batch [320/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0895 (0.5242) lr 1.0000e-03 eta 0:01:14\n",
            "epoch [7/10] batch [340/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.3079 (0.5344) lr 1.0000e-03 eta 0:01:13\n",
            "epoch [7/10] batch [360/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0418 (0.5439) lr 1.0000e-03 eta 0:01:12\n",
            "epoch [7/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0062 (0.5270) lr 1.0000e-03 eta 0:01:12\n",
            "epoch [7/10] batch [400/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.9458 (0.5242) lr 1.0000e-03 eta 0:01:11\n",
            "epoch [7/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0294 (0.5226) lr 1.0000e-03 eta 0:01:10\n",
            "epoch [7/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.7344 (0.5300) lr 1.0000e-03 eta 0:01:09\n",
            "epoch [7/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0511 (0.5270) lr 1.0000e-03 eta 0:01:08\n",
            "epoch [7/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0329 (0.5208) lr 1.0000e-03 eta 0:01:08\n",
            "epoch [7/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.2500 (0.5416) lr 1.0000e-03 eta 0:01:07\n",
            "epoch [7/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2725 (0.5431) lr 1.0000e-03 eta 0:01:06\n",
            "epoch [7/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 2.1562 (0.5322) lr 1.0000e-03 eta 0:01:05\n",
            "epoch [7/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1880 (0.5466) lr 1.0000e-03 eta 0:01:04\n",
            "epoch [7/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0977 (0.5461) lr 1.0000e-03 eta 0:01:03\n",
            "epoch [8/10] batch [20/592] time 0.034 (0.049) data 0.000 (0.013) loss 0.0781 (0.3828) lr 6.9098e-04 eta 0:01:25\n",
            "epoch [8/10] batch [40/592] time 0.034 (0.042) data 0.000 (0.007) loss 0.3201 (0.2931) lr 6.9098e-04 eta 0:01:12\n",
            "epoch [8/10] batch [60/592] time 0.033 (0.039) data 0.000 (0.004) loss 0.0336 (0.2608) lr 6.9098e-04 eta 0:01:07\n",
            "epoch [8/10] batch [80/592] time 0.034 (0.038) data 0.000 (0.003) loss 4.1094 (0.5408) lr 6.9098e-04 eta 0:01:04\n",
            "epoch [8/10] batch [100/592] time 0.036 (0.038) data 0.000 (0.003) loss 0.0074 (0.5916) lr 6.9098e-04 eta 0:01:03\n",
            "epoch [8/10] batch [120/592] time 0.036 (0.037) data 0.000 (0.002) loss 0.0024 (0.5739) lr 6.9098e-04 eta 0:01:01\n",
            "epoch [8/10] batch [140/592] time 0.036 (0.037) data 0.000 (0.002) loss 3.8203 (0.5903) lr 6.9098e-04 eta 0:01:00\n",
            "epoch [8/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.4214 (0.6020) lr 6.9098e-04 eta 0:00:59\n",
            "epoch [8/10] batch [180/592] time 0.041 (0.037) data 0.000 (0.002) loss 0.0537 (0.5846) lr 6.9098e-04 eta 0:00:58\n",
            "epoch [8/10] batch [200/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.3345 (0.5772) lr 6.9098e-04 eta 0:00:58\n",
            "epoch [8/10] batch [220/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0080 (0.6121) lr 6.9098e-04 eta 0:00:57\n",
            "epoch [8/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0041 (0.6268) lr 6.9098e-04 eta 0:00:56\n",
            "epoch [8/10] batch [260/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.4817 (0.6034) lr 6.9098e-04 eta 0:00:55\n",
            "epoch [8/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0089 (0.5707) lr 6.9098e-04 eta 0:00:54\n",
            "epoch [8/10] batch [300/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.4385 (0.5604) lr 6.9098e-04 eta 0:00:53\n",
            "epoch [8/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0660 (0.5440) lr 6.9098e-04 eta 0:00:52\n",
            "epoch [8/10] batch [340/592] time 0.035 (0.036) data 0.000 (0.001) loss 1.2275 (0.5429) lr 6.9098e-04 eta 0:00:51\n",
            "epoch [8/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0030 (0.5382) lr 6.9098e-04 eta 0:00:50\n",
            "epoch [8/10] batch [380/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0222 (0.5488) lr 6.9098e-04 eta 0:00:50\n",
            "epoch [8/10] batch [400/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.3904 (0.5448) lr 6.9098e-04 eta 0:00:49\n",
            "epoch [8/10] batch [420/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.2620 (0.5413) lr 6.9098e-04 eta 0:00:48\n",
            "epoch [8/10] batch [440/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0227 (0.5337) lr 6.9098e-04 eta 0:00:47\n",
            "epoch [8/10] batch [460/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0959 (0.5339) lr 6.9098e-04 eta 0:00:47\n",
            "epoch [8/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1226 (0.5479) lr 6.9098e-04 eta 0:00:46\n",
            "epoch [8/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.6621 (0.5388) lr 6.9098e-04 eta 0:00:45\n",
            "epoch [8/10] batch [520/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0710 (0.5280) lr 6.9098e-04 eta 0:00:44\n",
            "epoch [8/10] batch [540/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0018 (0.5285) lr 6.9098e-04 eta 0:00:44\n",
            "epoch [8/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0522 (0.5245) lr 6.9098e-04 eta 0:00:43\n",
            "epoch [8/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0826 (0.5162) lr 6.9098e-04 eta 0:00:42\n",
            "epoch [9/10] batch [20/592] time 0.034 (0.049) data 0.000 (0.013) loss 0.8428 (0.2352) lr 4.1221e-04 eta 0:00:57\n",
            "epoch [9/10] batch [40/592] time 0.036 (0.042) data 0.000 (0.007) loss 0.0254 (0.1599) lr 4.1221e-04 eta 0:00:47\n",
            "epoch [9/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.004) loss 1.2803 (0.2642) lr 4.1221e-04 eta 0:00:44\n",
            "epoch [9/10] batch [80/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.0087 (0.2667) lr 4.1221e-04 eta 0:00:42\n",
            "epoch [9/10] batch [100/592] time 0.036 (0.038) data 0.000 (0.003) loss 1.6914 (0.3104) lr 4.1221e-04 eta 0:00:41\n",
            "epoch [9/10] batch [120/592] time 0.037 (0.038) data 0.000 (0.002) loss 0.0331 (0.2946) lr 4.1221e-04 eta 0:00:40\n",
            "epoch [9/10] batch [140/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.0170 (0.3726) lr 4.1221e-04 eta 0:00:39\n",
            "epoch [9/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0028 (0.3472) lr 4.1221e-04 eta 0:00:38\n",
            "epoch [9/10] batch [180/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0658 (0.3529) lr 4.1221e-04 eta 0:00:37\n",
            "epoch [9/10] batch [200/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0533 (0.3697) lr 4.1221e-04 eta 0:00:36\n",
            "epoch [9/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0771 (0.4122) lr 4.1221e-04 eta 0:00:35\n",
            "epoch [9/10] batch [240/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2000 (0.3945) lr 4.1221e-04 eta 0:00:34\n",
            "epoch [9/10] batch [260/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0071 (0.3987) lr 4.1221e-04 eta 0:00:33\n",
            "epoch [9/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0303 (0.4024) lr 4.1221e-04 eta 0:00:32\n",
            "epoch [9/10] batch [300/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0806 (0.4134) lr 4.1221e-04 eta 0:00:31\n",
            "epoch [9/10] batch [320/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0038 (0.4081) lr 4.1221e-04 eta 0:00:31\n",
            "epoch [9/10] batch [340/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.3081 (0.4096) lr 4.1221e-04 eta 0:00:30\n",
            "epoch [9/10] batch [360/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0288 (0.4051) lr 4.1221e-04 eta 0:00:29\n",
            "epoch [9/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1288 (0.4097) lr 4.1221e-04 eta 0:00:28\n",
            "epoch [9/10] batch [400/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.0586 (0.4135) lr 4.1221e-04 eta 0:00:28\n",
            "epoch [9/10] batch [420/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.7207 (0.4226) lr 4.1221e-04 eta 0:00:27\n",
            "epoch [9/10] batch [440/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0251 (0.4173) lr 4.1221e-04 eta 0:00:26\n",
            "epoch [9/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.6758 (0.4293) lr 4.1221e-04 eta 0:00:25\n",
            "epoch [9/10] batch [480/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.2732 (0.4305) lr 4.1221e-04 eta 0:00:25\n",
            "epoch [9/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.5806 (0.4258) lr 4.1221e-04 eta 0:00:24\n",
            "epoch [9/10] batch [520/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0400 (0.4189) lr 4.1221e-04 eta 0:00:23\n",
            "epoch [9/10] batch [540/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0099 (0.4148) lr 4.1221e-04 eta 0:00:23\n",
            "epoch [9/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0182 (0.4145) lr 4.1221e-04 eta 0:00:22\n",
            "epoch [9/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0136 (0.4150) lr 4.1221e-04 eta 0:00:21\n",
            "epoch [10/10] batch [20/592] time 0.035 (0.049) data 0.000 (0.013) loss 0.0797 (0.3104) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [40/592] time 0.034 (0.042) data 0.000 (0.007) loss 0.0211 (0.4393) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [60/592] time 0.035 (0.040) data 0.000 (0.005) loss 0.0250 (0.4451) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [10/10] batch [80/592] time 0.034 (0.038) data 0.000 (0.003) loss 0.0337 (0.4224) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [100/592] time 0.037 (0.038) data 0.000 (0.003) loss 1.4980 (0.4466) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [120/592] time 0.036 (0.038) data 0.000 (0.002) loss 0.0037 (0.4442) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [10/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 2.7402 (0.4808) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.1517 (0.4615) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [180/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.4441 (0.4807) lr 1.9098e-04 eta 0:00:15\n",
            "epoch [10/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0104 (0.4983) lr 1.9098e-04 eta 0:00:14\n",
            "epoch [10/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0725 (0.5022) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 1.0449 (0.5283) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 4.8906 (0.5447) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.3604 (0.5431) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [10/10] batch [300/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0934 (0.5430) lr 1.9098e-04 eta 0:00:10\n",
            "epoch [10/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 2.5723 (0.5502) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [340/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0402 (0.5884) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0796 (0.5728) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0165 (0.5650) lr 1.9098e-04 eta 0:00:07\n",
            "epoch [10/10] batch [400/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0019 (0.5554) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0035 (0.5432) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.3059 (0.5419) lr 1.9098e-04 eta 0:00:05\n",
            "epoch [10/10] batch [460/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.1731 (0.5418) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [10/10] batch [480/592] time 0.033 (0.035) data 0.000 (0.001) loss 0.9038 (0.5389) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [500/592] time 0.034 (0.035) data 0.000 (0.001) loss 0.2607 (0.5365) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [520/592] time 0.036 (0.035) data 0.000 (0.001) loss 0.0956 (0.5399) lr 1.9098e-04 eta 0:00:02\n",
            "epoch [10/10] batch [540/592] time 0.036 (0.035) data 0.000 (0.001) loss 0.2097 (0.5322) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [560/592] time 0.034 (0.035) data 0.000 (0.001) loss 0.1698 (0.5256) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [580/592] time 0.033 (0.035) data 0.000 (0.001) loss 0.4370 (0.5436) lr 1.9098e-04 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2/prompt_learner/model.pth.tar-10\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:40<00:00,  1.10s/it]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,420\n",
            "* accuracy: 93.2%\n",
            "* error: 6.8%\n",
            "* macro_f1: 93.2%\n",
            "Elapsed: 0:04:26\n",
            "2024-12-10 12:06:09.802552: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-10 12:06:09.819872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-10 12:06:09.841163: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-10 12:06:09.847629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-10 12:06:09.862574: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-10 12:06:10.917107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/MIP/vit_b16_c4_ep10_batch1.yaml\n",
            "dataset_config_file: configs/datasets/oxford_pets.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: 0\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16']\n",
            "output_dir: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3\n",
            "resume: \n",
            "root: data/\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: MIP\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 1\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordPets\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: data/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: MIP\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: NVIDIA A100-SXM4-40GB\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               12\n",
            "On-line CPU(s) list:                  0-11\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   6\n",
            "Socket(s):                            1\n",
            "Stepping:                             7\n",
            "BogoMIPS:                             4400.44\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            192 KiB (6 instances)\n",
            "L1i cache:                            192 KiB (6 instances)\n",
            "L2 cache:                             6 MiB (6 instances)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0-11\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Not affected\n",
            "Vulnerability Mds:                    Not affected\n",
            "Vulnerability Meltdown:               Not affected\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Vulnerable; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.13.1\n",
            "[pip3] torch==2.5.1+cu121\n",
            "[pip3] torchaudio==2.5.1+cu121\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchvision==0.20.1+cu121\n",
            "[conda] Could not collect\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: MIP\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/drive/MyDrive/Colab Notebooks/MIP/data/oxford_pets/split_fewshot/shot_16-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  592\n",
            "# val      148\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'prompt_learner.ctx', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear1.weight', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear2.weight'}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3/tensorboard)\n",
            "epoch [1/10] batch [20/592] time 0.034 (0.155) data 0.000 (0.045) loss 0.3909 (1.7202) lr 1.0000e-05 eta 0:15:13\n",
            "epoch [1/10] batch [40/592] time 0.034 (0.095) data 0.000 (0.022) loss 2.0293 (1.8805) lr 1.0000e-05 eta 0:09:15\n",
            "epoch [1/10] batch [60/592] time 0.037 (0.075) data 0.000 (0.016) loss 6.9648 (1.6695) lr 1.0000e-05 eta 0:07:21\n",
            "epoch [1/10] batch [80/592] time 0.040 (0.066) data 0.000 (0.012) loss 0.1148 (1.6267) lr 1.0000e-05 eta 0:06:23\n",
            "epoch [1/10] batch [100/592] time 0.033 (0.060) data 0.000 (0.010) loss 0.1317 (1.4796) lr 1.0000e-05 eta 0:05:46\n",
            "epoch [1/10] batch [120/592] time 0.188 (0.057) data 0.154 (0.010) loss 0.2542 (1.4021) lr 1.0000e-05 eta 0:05:32\n",
            "epoch [1/10] batch [140/592] time 0.040 (0.055) data 0.000 (0.009) loss 0.0002 (1.3786) lr 1.0000e-05 eta 0:05:18\n",
            "epoch [1/10] batch [160/592] time 0.034 (0.053) data 0.000 (0.008) loss 0.8867 (1.3953) lr 1.0000e-05 eta 0:05:03\n",
            "epoch [1/10] batch [180/592] time 0.033 (0.051) data 0.000 (0.008) loss 0.4312 (1.3892) lr 1.0000e-05 eta 0:04:54\n",
            "epoch [1/10] batch [200/592] time 0.039 (0.051) data 0.000 (0.008) loss 0.5386 (1.4193) lr 1.0000e-05 eta 0:04:50\n",
            "epoch [1/10] batch [220/592] time 0.050 (0.049) data 0.000 (0.007) loss 0.9619 (1.4523) lr 1.0000e-05 eta 0:04:42\n",
            "epoch [1/10] batch [240/592] time 0.036 (0.052) data 0.000 (0.010) loss 1.8184 (1.4877) lr 1.0000e-05 eta 0:04:53\n",
            "epoch [1/10] batch [260/592] time 0.036 (0.050) data 0.001 (0.010) loss 0.6934 (1.4377) lr 1.0000e-05 eta 0:04:45\n",
            "epoch [1/10] batch [280/592] time 0.039 (0.049) data 0.000 (0.009) loss 0.3154 (1.3884) lr 1.0000e-05 eta 0:04:38\n",
            "epoch [1/10] batch [300/592] time 0.033 (0.049) data 0.000 (0.009) loss 2.8809 (1.4326) lr 1.0000e-05 eta 0:04:34\n",
            "epoch [1/10] batch [320/592] time 0.035 (0.049) data 0.000 (0.009) loss 0.9326 (1.3948) lr 1.0000e-05 eta 0:04:32\n",
            "epoch [1/10] batch [340/592] time 0.058 (0.048) data 0.021 (0.008) loss 0.3030 (1.3783) lr 1.0000e-05 eta 0:04:27\n",
            "epoch [1/10] batch [360/592] time 0.037 (0.048) data 0.000 (0.008) loss 3.2930 (1.3634) lr 1.0000e-05 eta 0:04:25\n",
            "epoch [1/10] batch [380/592] time 0.033 (0.047) data 0.000 (0.008) loss 0.4250 (1.3558) lr 1.0000e-05 eta 0:04:20\n",
            "epoch [1/10] batch [400/592] time 0.032 (0.047) data 0.000 (0.008) loss 0.1920 (1.3686) lr 1.0000e-05 eta 0:04:19\n",
            "epoch [1/10] batch [420/592] time 0.036 (0.046) data 0.000 (0.008) loss 0.1844 (1.3647) lr 1.0000e-05 eta 0:04:15\n",
            "epoch [1/10] batch [440/592] time 0.033 (0.050) data 0.000 (0.012) loss 1.8281 (1.3743) lr 1.0000e-05 eta 0:04:35\n",
            "epoch [1/10] batch [460/592] time 0.035 (0.050) data 0.000 (0.012) loss 0.2871 (1.3597) lr 1.0000e-05 eta 0:04:33\n",
            "epoch [1/10] batch [480/592] time 0.036 (0.050) data 0.000 (0.011) loss 1.0889 (1.3292) lr 1.0000e-05 eta 0:04:29\n",
            "epoch [1/10] batch [500/592] time 0.035 (0.049) data 0.000 (0.011) loss 1.4893 (1.3050) lr 1.0000e-05 eta 0:04:25\n",
            "epoch [1/10] batch [520/592] time 0.036 (0.049) data 0.000 (0.011) loss 3.9570 (1.3013) lr 1.0000e-05 eta 0:04:23\n",
            "epoch [1/10] batch [540/592] time 0.033 (0.048) data 0.000 (0.010) loss 1.0742 (1.2994) lr 1.0000e-05 eta 0:04:20\n",
            "epoch [1/10] batch [560/592] time 0.037 (0.048) data 0.000 (0.010) loss 1.2178 (1.2898) lr 1.0000e-05 eta 0:04:17\n",
            "epoch [1/10] batch [580/592] time 0.032 (0.048) data 0.000 (0.010) loss 0.0515 (1.2967) lr 1.0000e-05 eta 0:04:14\n",
            "epoch [2/10] batch [20/592] time 0.034 (0.050) data 0.000 (0.014) loss 0.0217 (1.1579) lr 2.0000e-03 eta 0:04:27\n",
            "epoch [2/10] batch [40/592] time 0.036 (0.043) data 0.000 (0.007) loss 0.4958 (0.9102) lr 2.0000e-03 eta 0:03:47\n",
            "epoch [2/10] batch [60/592] time 0.037 (0.041) data 0.000 (0.005) loss 4.4844 (1.0003) lr 2.0000e-03 eta 0:03:35\n",
            "epoch [2/10] batch [80/592] time 0.036 (0.040) data 0.000 (0.004) loss 0.0331 (0.8995) lr 2.0000e-03 eta 0:03:28\n",
            "epoch [2/10] batch [100/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.2812 (0.8353) lr 2.0000e-03 eta 0:03:24\n",
            "epoch [2/10] batch [120/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0507 (0.8197) lr 2.0000e-03 eta 0:03:19\n",
            "epoch [2/10] batch [140/592] time 0.037 (0.038) data 0.000 (0.002) loss 0.8477 (0.7585) lr 2.0000e-03 eta 0:03:16\n",
            "epoch [2/10] batch [160/592] time 0.036 (0.038) data 0.000 (0.002) loss 0.8555 (0.7475) lr 2.0000e-03 eta 0:03:14\n",
            "epoch [2/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.5176 (0.7413) lr 2.0000e-03 eta 0:03:12\n",
            "epoch [2/10] batch [200/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.1058 (0.7270) lr 2.0000e-03 eta 0:03:10\n",
            "epoch [2/10] batch [220/592] time 0.037 (0.037) data 0.000 (0.001) loss 0.1248 (0.7076) lr 2.0000e-03 eta 0:03:08\n",
            "epoch [2/10] batch [240/592] time 0.035 (0.037) data 0.000 (0.001) loss 3.2734 (0.7013) lr 2.0000e-03 eta 0:03:06\n",
            "epoch [2/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.4517 (0.7099) lr 2.0000e-03 eta 0:03:05\n",
            "epoch [2/10] batch [280/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.1194 (0.7301) lr 2.0000e-03 eta 0:03:04\n",
            "epoch [2/10] batch [300/592] time 0.036 (0.036) data 0.000 (0.001) loss 1.7773 (0.7376) lr 2.0000e-03 eta 0:03:02\n",
            "epoch [2/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0478 (0.7179) lr 2.0000e-03 eta 0:03:01\n",
            "epoch [2/10] batch [340/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0938 (0.7257) lr 2.0000e-03 eta 0:03:00\n",
            "epoch [2/10] batch [360/592] time 0.036 (0.036) data 0.000 (0.001) loss 4.2422 (0.7257) lr 2.0000e-03 eta 0:02:59\n",
            "epoch [2/10] batch [380/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0779 (0.7315) lr 2.0000e-03 eta 0:02:58\n",
            "epoch [2/10] batch [400/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0020 (0.7335) lr 2.0000e-03 eta 0:02:57\n",
            "epoch [2/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0888 (0.7526) lr 2.0000e-03 eta 0:02:56\n",
            "epoch [2/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.5439 (0.7424) lr 2.0000e-03 eta 0:02:56\n",
            "epoch [2/10] batch [460/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0123 (0.7425) lr 2.0000e-03 eta 0:02:55\n",
            "epoch [2/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0004 (0.7367) lr 2.0000e-03 eta 0:02:54\n",
            "epoch [2/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1869 (0.7441) lr 2.0000e-03 eta 0:02:53\n",
            "epoch [2/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.6548 (0.7403) lr 2.0000e-03 eta 0:02:52\n",
            "epoch [2/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.6133 (0.7467) lr 2.0000e-03 eta 0:02:51\n",
            "epoch [2/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0018 (0.7393) lr 2.0000e-03 eta 0:02:50\n",
            "epoch [2/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.7021 (0.7407) lr 2.0000e-03 eta 0:02:49\n",
            "epoch [3/10] batch [20/592] time 0.033 (0.050) data 0.000 (0.014) loss 0.0206 (0.2885) lr 1.9511e-03 eta 0:03:55\n",
            "epoch [3/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 1.6260 (0.5337) lr 1.9511e-03 eta 0:03:20\n",
            "epoch [3/10] batch [60/592] time 0.033 (0.040) data 0.000 (0.005) loss 0.4473 (0.4841) lr 1.9511e-03 eta 0:03:09\n",
            "epoch [3/10] batch [80/592] time 0.036 (0.039) data 0.000 (0.004) loss 3.5938 (0.5083) lr 1.9511e-03 eta 0:03:03\n",
            "epoch [3/10] batch [100/592] time 0.034 (0.039) data 0.001 (0.003) loss 0.5312 (0.6343) lr 1.9511e-03 eta 0:03:01\n",
            "epoch [3/10] batch [120/592] time 0.034 (0.039) data 0.000 (0.002) loss 0.0093 (0.6472) lr 1.9511e-03 eta 0:02:58\n",
            "epoch [3/10] batch [140/592] time 0.033 (0.038) data 0.000 (0.002) loss 0.5840 (0.6804) lr 1.9511e-03 eta 0:02:55\n",
            "epoch [3/10] batch [160/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.1552 (0.6748) lr 1.9511e-03 eta 0:02:52\n",
            "epoch [3/10] batch [180/592] time 0.033 (0.037) data 0.000 (0.002) loss 0.1570 (0.6918) lr 1.9511e-03 eta 0:02:49\n",
            "epoch [3/10] batch [200/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.3948 (0.6737) lr 1.9511e-03 eta 0:02:47\n",
            "epoch [3/10] batch [220/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0149 (0.6687) lr 1.9511e-03 eta 0:02:45\n",
            "epoch [3/10] batch [240/592] time 0.035 (0.037) data 0.000 (0.001) loss 2.3438 (0.6531) lr 1.9511e-03 eta 0:02:44\n",
            "epoch [3/10] batch [260/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.2474 (0.6209) lr 1.9511e-03 eta 0:02:42\n",
            "epoch [3/10] batch [280/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.2073 (0.6318) lr 1.9511e-03 eta 0:02:41\n",
            "epoch [3/10] batch [300/592] time 0.035 (0.036) data 0.000 (0.001) loss 2.0254 (0.6236) lr 1.9511e-03 eta 0:02:40\n",
            "epoch [3/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0164 (0.6072) lr 1.9511e-03 eta 0:02:39\n",
            "epoch [3/10] batch [340/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.4185 (0.6073) lr 1.9511e-03 eta 0:02:38\n",
            "epoch [3/10] batch [360/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.7700 (0.6139) lr 1.9511e-03 eta 0:02:37\n",
            "epoch [3/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.4880 (0.6224) lr 1.9511e-03 eta 0:02:36\n",
            "epoch [3/10] batch [400/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0176 (0.6272) lr 1.9511e-03 eta 0:02:35\n",
            "epoch [3/10] batch [420/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0475 (0.6226) lr 1.9511e-03 eta 0:02:34\n",
            "epoch [3/10] batch [440/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0145 (0.6322) lr 1.9511e-03 eta 0:02:34\n",
            "epoch [3/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0821 (0.6564) lr 1.9511e-03 eta 0:02:33\n",
            "epoch [3/10] batch [480/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0329 (0.6540) lr 1.9511e-03 eta 0:02:32\n",
            "epoch [3/10] batch [500/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0355 (0.6555) lr 1.9511e-03 eta 0:02:32\n",
            "epoch [3/10] batch [520/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0690 (0.6455) lr 1.9511e-03 eta 0:02:31\n",
            "epoch [3/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0008 (0.6381) lr 1.9511e-03 eta 0:02:30\n",
            "epoch [3/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.3669 (0.6490) lr 1.9511e-03 eta 0:02:29\n",
            "epoch [3/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0092 (0.6346) lr 1.9511e-03 eta 0:02:28\n",
            "epoch [4/10] batch [20/592] time 0.034 (0.049) data 0.001 (0.013) loss 0.2247 (0.4994) lr 1.8090e-03 eta 0:03:21\n",
            "epoch [4/10] batch [40/592] time 0.034 (0.042) data 0.000 (0.007) loss 0.0493 (0.3887) lr 1.8090e-03 eta 0:02:50\n",
            "epoch [4/10] batch [60/592] time 0.034 (0.039) data 0.000 (0.004) loss 1.7979 (0.4962) lr 1.8090e-03 eta 0:02:40\n",
            "epoch [4/10] batch [80/592] time 0.034 (0.038) data 0.000 (0.003) loss 0.0308 (0.5805) lr 1.8090e-03 eta 0:02:34\n",
            "epoch [4/10] batch [100/592] time 0.034 (0.037) data 0.000 (0.003) loss 1.6035 (0.5965) lr 1.8090e-03 eta 0:02:30\n",
            "epoch [4/10] batch [120/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.4758 (0.6167) lr 1.8090e-03 eta 0:02:27\n",
            "epoch [4/10] batch [140/592] time 0.034 (0.036) data 0.000 (0.002) loss 3.4355 (0.6066) lr 1.8090e-03 eta 0:02:25\n",
            "epoch [4/10] batch [160/592] time 0.034 (0.036) data 0.000 (0.002) loss 0.2493 (0.6220) lr 1.8090e-03 eta 0:02:24\n",
            "epoch [4/10] batch [180/592] time 0.034 (0.036) data 0.000 (0.002) loss 0.3477 (0.5955) lr 1.8090e-03 eta 0:02:22\n",
            "epoch [4/10] batch [200/592] time 0.035 (0.036) data 0.000 (0.002) loss 1.1562 (0.5925) lr 1.8090e-03 eta 0:02:21\n",
            "epoch [4/10] batch [220/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2954 (0.5892) lr 1.8090e-03 eta 0:02:20\n",
            "epoch [4/10] batch [240/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.3391 (0.6200) lr 1.8090e-03 eta 0:02:19\n",
            "epoch [4/10] batch [260/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.3962 (0.6187) lr 1.8090e-03 eta 0:02:19\n",
            "epoch [4/10] batch [280/592] time 0.036 (0.036) data 0.000 (0.001) loss 4.0352 (0.6074) lr 1.8090e-03 eta 0:02:18\n",
            "epoch [4/10] batch [300/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0195 (0.6061) lr 1.8090e-03 eta 0:02:18\n",
            "epoch [4/10] batch [320/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0142 (0.6133) lr 1.8090e-03 eta 0:02:17\n",
            "epoch [4/10] batch [340/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0254 (0.5879) lr 1.8090e-03 eta 0:02:16\n",
            "epoch [4/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0067 (0.5786) lr 1.8090e-03 eta 0:02:16\n",
            "epoch [4/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0975 (0.5638) lr 1.8090e-03 eta 0:02:15\n",
            "epoch [4/10] batch [400/592] time 0.033 (0.036) data 0.000 (0.001) loss 1.4531 (0.5643) lr 1.8090e-03 eta 0:02:14\n",
            "epoch [4/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.8062 (0.5726) lr 1.8090e-03 eta 0:02:13\n",
            "epoch [4/10] batch [440/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0051 (0.5772) lr 1.8090e-03 eta 0:02:12\n",
            "epoch [4/10] batch [460/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0157 (0.5723) lr 1.8090e-03 eta 0:02:11\n",
            "epoch [4/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.4038 (0.5765) lr 1.8090e-03 eta 0:02:10\n",
            "epoch [4/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0512 (0.5757) lr 1.8090e-03 eta 0:02:09\n",
            "epoch [4/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2219 (0.5767) lr 1.8090e-03 eta 0:02:09\n",
            "epoch [4/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0905 (0.5793) lr 1.8090e-03 eta 0:02:08\n",
            "epoch [4/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0047 (0.5709) lr 1.8090e-03 eta 0:02:07\n",
            "epoch [4/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0499 (0.5737) lr 1.8090e-03 eta 0:02:06\n",
            "epoch [5/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 0.0157 (0.7940) lr 1.5878e-03 eta 0:02:56\n",
            "epoch [5/10] batch [40/592] time 0.037 (0.043) data 0.000 (0.007) loss 0.3948 (0.6492) lr 1.5878e-03 eta 0:02:31\n",
            "epoch [5/10] batch [60/592] time 0.039 (0.041) data 0.000 (0.005) loss 0.0028 (0.6367) lr 1.5878e-03 eta 0:02:23\n",
            "epoch [5/10] batch [80/592] time 0.034 (0.040) data 0.000 (0.003) loss 0.0974 (0.6165) lr 1.5878e-03 eta 0:02:18\n",
            "epoch [5/10] batch [100/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.0004 (0.7330) lr 1.5878e-03 eta 0:02:14\n",
            "epoch [5/10] batch [120/592] time 0.033 (0.038) data 0.000 (0.002) loss 0.0343 (0.6386) lr 1.5878e-03 eta 0:02:11\n",
            "epoch [5/10] batch [140/592] time 0.045 (0.038) data 0.000 (0.002) loss 1.1562 (0.5931) lr 1.5878e-03 eta 0:02:10\n",
            "epoch [5/10] batch [160/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.1727 (0.5645) lr 1.5878e-03 eta 0:02:08\n",
            "epoch [5/10] batch [180/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0011 (0.5465) lr 1.5878e-03 eta 0:02:06\n",
            "epoch [5/10] batch [200/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.1005 (0.5052) lr 1.5878e-03 eta 0:02:05\n",
            "epoch [5/10] batch [220/592] time 0.033 (0.037) data 0.000 (0.001) loss 3.3477 (0.4954) lr 1.5878e-03 eta 0:02:03\n",
            "epoch [5/10] batch [240/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.3208 (0.4787) lr 1.5878e-03 eta 0:02:02\n",
            "epoch [5/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.5186 (0.4962) lr 1.5878e-03 eta 0:02:01\n",
            "epoch [5/10] batch [280/592] time 0.034 (0.037) data 0.000 (0.001) loss 1.4648 (0.5068) lr 1.5878e-03 eta 0:02:00\n",
            "epoch [5/10] batch [300/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0234 (0.5005) lr 1.5878e-03 eta 0:01:59\n",
            "epoch [5/10] batch [320/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1277 (0.5023) lr 1.5878e-03 eta 0:01:58\n",
            "epoch [5/10] batch [340/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.2241 (0.4900) lr 1.5878e-03 eta 0:01:57\n",
            "epoch [5/10] batch [360/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.3005 (0.4799) lr 1.5878e-03 eta 0:01:56\n",
            "epoch [5/10] batch [380/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.1339 (0.4757) lr 1.5878e-03 eta 0:01:55\n",
            "epoch [5/10] batch [400/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.5806 (0.4899) lr 1.5878e-03 eta 0:01:54\n",
            "epoch [5/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0099 (0.4926) lr 1.5878e-03 eta 0:01:53\n",
            "epoch [5/10] batch [440/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.1400 (0.4993) lr 1.5878e-03 eta 0:01:52\n",
            "epoch [5/10] batch [460/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.3486 (0.4909) lr 1.5878e-03 eta 0:01:52\n",
            "epoch [5/10] batch [480/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.0153 (0.4975) lr 1.5878e-03 eta 0:01:51\n",
            "epoch [5/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.4800 (0.5026) lr 1.5878e-03 eta 0:01:50\n",
            "epoch [5/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0203 (0.5029) lr 1.5878e-03 eta 0:01:50\n",
            "epoch [5/10] batch [540/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0967 (0.5037) lr 1.5878e-03 eta 0:01:49\n",
            "epoch [5/10] batch [560/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.2747 (0.5011) lr 1.5878e-03 eta 0:01:48\n",
            "epoch [5/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0433 (0.5095) lr 1.5878e-03 eta 0:01:48\n",
            "epoch [6/10] batch [20/592] time 0.037 (0.051) data 0.000 (0.014) loss 0.0100 (0.3177) lr 1.3090e-03 eta 0:02:28\n",
            "epoch [6/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 0.8618 (0.5883) lr 1.3090e-03 eta 0:02:05\n",
            "epoch [6/10] batch [60/592] time 0.035 (0.041) data 0.000 (0.005) loss 0.1438 (0.7117) lr 1.3090e-03 eta 0:01:57\n",
            "epoch [6/10] batch [80/592] time 0.035 (0.039) data 0.000 (0.004) loss 0.4341 (0.7433) lr 1.3090e-03 eta 0:01:52\n",
            "epoch [6/10] batch [100/592] time 0.035 (0.038) data 0.000 (0.003) loss 0.0099 (0.6925) lr 1.3090e-03 eta 0:01:49\n",
            "epoch [6/10] batch [120/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.0015 (0.6729) lr 1.3090e-03 eta 0:01:46\n",
            "epoch [6/10] batch [140/592] time 0.034 (0.037) data 0.000 (0.002) loss 1.2949 (0.7199) lr 1.3090e-03 eta 0:01:44\n",
            "epoch [6/10] batch [160/592] time 0.036 (0.037) data 0.000 (0.002) loss 0.0547 (0.7122) lr 1.3090e-03 eta 0:01:43\n",
            "epoch [6/10] batch [180/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.9727 (0.7025) lr 1.3090e-03 eta 0:01:42\n",
            "epoch [6/10] batch [200/592] time 0.037 (0.037) data 0.000 (0.002) loss 1.4561 (0.7014) lr 1.3090e-03 eta 0:01:41\n",
            "epoch [6/10] batch [220/592] time 0.038 (0.037) data 0.000 (0.001) loss 0.9199 (0.6819) lr 1.3090e-03 eta 0:01:40\n",
            "epoch [6/10] batch [240/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.1404 (0.6793) lr 1.3090e-03 eta 0:01:39\n",
            "epoch [6/10] batch [260/592] time 0.038 (0.037) data 0.000 (0.001) loss 0.7310 (0.7276) lr 1.3090e-03 eta 0:01:39\n",
            "epoch [6/10] batch [280/592] time 0.036 (0.037) data 0.000 (0.001) loss 1.4951 (0.7061) lr 1.3090e-03 eta 0:01:38\n",
            "epoch [6/10] batch [300/592] time 0.038 (0.037) data 0.000 (0.001) loss 0.1104 (0.6901) lr 1.3090e-03 eta 0:01:37\n",
            "epoch [6/10] batch [320/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.0531 (0.6712) lr 1.3090e-03 eta 0:01:36\n",
            "epoch [6/10] batch [340/592] time 0.036 (0.037) data 0.000 (0.001) loss 1.2637 (0.6634) lr 1.3090e-03 eta 0:01:35\n",
            "epoch [6/10] batch [360/592] time 0.038 (0.037) data 0.000 (0.001) loss 0.8823 (0.6694) lr 1.3090e-03 eta 0:01:35\n",
            "epoch [6/10] batch [380/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0474 (0.6539) lr 1.3090e-03 eta 0:01:34\n",
            "epoch [6/10] batch [400/592] time 0.034 (0.036) data 0.000 (0.001) loss 2.0859 (0.6433) lr 1.3090e-03 eta 0:01:33\n",
            "epoch [6/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0142 (0.6508) lr 1.3090e-03 eta 0:01:32\n",
            "epoch [6/10] batch [440/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0370 (0.6475) lr 1.3090e-03 eta 0:01:31\n",
            "epoch [6/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.1071 (0.6445) lr 1.3090e-03 eta 0:01:30\n",
            "epoch [6/10] batch [480/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0589 (0.6401) lr 1.3090e-03 eta 0:01:29\n",
            "epoch [6/10] batch [500/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0202 (0.6357) lr 1.3090e-03 eta 0:01:28\n",
            "epoch [6/10] batch [520/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0121 (0.6256) lr 1.3090e-03 eta 0:01:28\n",
            "epoch [6/10] batch [540/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.2571 (0.6259) lr 1.3090e-03 eta 0:01:27\n",
            "epoch [6/10] batch [560/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0756 (0.6400) lr 1.3090e-03 eta 0:01:26\n",
            "epoch [6/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0173 (0.6567) lr 1.3090e-03 eta 0:01:25\n",
            "epoch [7/10] batch [20/592] time 0.034 (0.052) data 0.000 (0.014) loss 0.6606 (0.5665) lr 1.0000e-03 eta 0:02:01\n",
            "epoch [7/10] batch [40/592] time 0.040 (0.045) data 0.000 (0.007) loss 1.4785 (0.3661) lr 1.0000e-03 eta 0:01:44\n",
            "epoch [7/10] batch [60/592] time 0.038 (0.042) data 0.000 (0.005) loss 0.2094 (0.3982) lr 1.0000e-03 eta 0:01:38\n",
            "epoch [7/10] batch [80/592] time 0.041 (0.042) data 0.000 (0.004) loss 1.0869 (0.3698) lr 1.0000e-03 eta 0:01:35\n",
            "epoch [7/10] batch [100/592] time 0.036 (0.041) data 0.000 (0.003) loss 0.0047 (0.3543) lr 1.0000e-03 eta 0:01:32\n",
            "epoch [7/10] batch [120/592] time 0.037 (0.040) data 0.000 (0.003) loss 0.1831 (0.4242) lr 1.0000e-03 eta 0:01:30\n",
            "epoch [7/10] batch [140/592] time 0.037 (0.040) data 0.000 (0.002) loss 0.6792 (0.4357) lr 1.0000e-03 eta 0:01:28\n",
            "epoch [7/10] batch [160/592] time 0.036 (0.039) data 0.000 (0.002) loss 0.0124 (0.4289) lr 1.0000e-03 eta 0:01:26\n",
            "epoch [7/10] batch [180/592] time 0.035 (0.039) data 0.000 (0.002) loss 0.0031 (0.4436) lr 1.0000e-03 eta 0:01:25\n",
            "epoch [7/10] batch [200/592] time 0.036 (0.039) data 0.000 (0.002) loss 0.2421 (0.4803) lr 1.0000e-03 eta 0:01:23\n",
            "epoch [7/10] batch [220/592] time 0.036 (0.038) data 0.001 (0.002) loss 0.0559 (0.5057) lr 1.0000e-03 eta 0:01:22\n",
            "epoch [7/10] batch [240/592] time 0.036 (0.038) data 0.000 (0.001) loss 0.0646 (0.5244) lr 1.0000e-03 eta 0:01:21\n",
            "epoch [7/10] batch [260/592] time 0.034 (0.038) data 0.000 (0.001) loss 0.4016 (0.5336) lr 1.0000e-03 eta 0:01:20\n",
            "epoch [7/10] batch [280/592] time 0.034 (0.038) data 0.000 (0.001) loss 0.0101 (0.5116) lr 1.0000e-03 eta 0:01:18\n",
            "epoch [7/10] batch [300/592] time 0.035 (0.037) data 0.000 (0.001) loss 4.5469 (0.5329) lr 1.0000e-03 eta 0:01:17\n",
            "epoch [7/10] batch [320/592] time 0.034 (0.037) data 0.000 (0.001) loss 2.3125 (0.5339) lr 1.0000e-03 eta 0:01:16\n",
            "epoch [7/10] batch [340/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0003 (0.5323) lr 1.0000e-03 eta 0:01:15\n",
            "epoch [7/10] batch [360/592] time 0.035 (0.037) data 0.000 (0.001) loss 1.6279 (0.5434) lr 1.0000e-03 eta 0:01:14\n",
            "epoch [7/10] batch [380/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0076 (0.5410) lr 1.0000e-03 eta 0:01:13\n",
            "epoch [7/10] batch [400/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.0006 (0.5424) lr 1.0000e-03 eta 0:01:12\n",
            "epoch [7/10] batch [420/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.1400 (0.5316) lr 1.0000e-03 eta 0:01:11\n",
            "epoch [7/10] batch [440/592] time 0.034 (0.037) data 0.000 (0.001) loss 2.8984 (0.5239) lr 1.0000e-03 eta 0:01:10\n",
            "epoch [7/10] batch [460/592] time 0.037 (0.037) data 0.000 (0.001) loss 0.6255 (0.5207) lr 1.0000e-03 eta 0:01:09\n",
            "epoch [7/10] batch [480/592] time 0.035 (0.037) data 0.000 (0.001) loss 1.2578 (0.5247) lr 1.0000e-03 eta 0:01:09\n",
            "epoch [7/10] batch [500/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.0629 (0.5268) lr 1.0000e-03 eta 0:01:08\n",
            "epoch [7/10] batch [520/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0276 (0.5183) lr 1.0000e-03 eta 0:01:07\n",
            "epoch [7/10] batch [540/592] time 0.036 (0.037) data 0.000 (0.001) loss 4.0742 (0.5337) lr 1.0000e-03 eta 0:01:06\n",
            "epoch [7/10] batch [560/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.1057 (0.5289) lr 1.0000e-03 eta 0:01:06\n",
            "epoch [7/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.6323 (0.5293) lr 1.0000e-03 eta 0:01:05\n",
            "epoch [8/10] batch [20/592] time 0.035 (0.050) data 0.000 (0.013) loss 0.1293 (0.4660) lr 6.9098e-04 eta 0:01:27\n",
            "epoch [8/10] batch [40/592] time 0.035 (0.043) data 0.000 (0.007) loss 0.0923 (0.6375) lr 6.9098e-04 eta 0:01:13\n",
            "epoch [8/10] batch [60/592] time 0.036 (0.040) data 0.000 (0.005) loss 0.7178 (0.6977) lr 6.9098e-04 eta 0:01:09\n",
            "epoch [8/10] batch [80/592] time 0.034 (0.039) data 0.000 (0.003) loss 0.0050 (0.7307) lr 6.9098e-04 eta 0:01:06\n",
            "epoch [8/10] batch [100/592] time 0.035 (0.038) data 0.000 (0.003) loss 2.3496 (0.7213) lr 6.9098e-04 eta 0:01:04\n",
            "epoch [8/10] batch [120/592] time 0.035 (0.038) data 0.000 (0.002) loss 0.0623 (0.6712) lr 6.9098e-04 eta 0:01:02\n",
            "epoch [8/10] batch [140/592] time 0.037 (0.037) data 0.000 (0.002) loss 0.3745 (0.6416) lr 6.9098e-04 eta 0:01:01\n",
            "epoch [8/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0002 (0.5819) lr 6.9098e-04 eta 0:01:00\n",
            "epoch [8/10] batch [180/592] time 0.036 (0.037) data 0.000 (0.002) loss 0.7988 (0.5597) lr 6.9098e-04 eta 0:00:59\n",
            "epoch [8/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0071 (0.5290) lr 6.9098e-04 eta 0:00:58\n",
            "epoch [8/10] batch [220/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.1328 (0.5161) lr 6.9098e-04 eta 0:00:57\n",
            "epoch [8/10] batch [240/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.5200 (0.5152) lr 6.9098e-04 eta 0:00:56\n",
            "epoch [8/10] batch [260/592] time 0.039 (0.037) data 0.000 (0.001) loss 1.1768 (0.5245) lr 6.9098e-04 eta 0:00:55\n",
            "epoch [8/10] batch [280/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.0150 (0.5060) lr 6.9098e-04 eta 0:00:55\n",
            "epoch [8/10] batch [300/592] time 0.038 (0.037) data 0.000 (0.001) loss 0.7402 (0.5251) lr 6.9098e-04 eta 0:00:54\n",
            "epoch [8/10] batch [320/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.5405 (0.5181) lr 6.9098e-04 eta 0:00:53\n",
            "epoch [8/10] batch [340/592] time 0.036 (0.037) data 0.000 (0.001) loss 3.0801 (0.5304) lr 6.9098e-04 eta 0:00:53\n",
            "epoch [8/10] batch [360/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.2004 (0.5343) lr 6.9098e-04 eta 0:00:52\n",
            "epoch [8/10] batch [380/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0650 (0.5318) lr 6.9098e-04 eta 0:00:51\n",
            "epoch [8/10] batch [400/592] time 0.035 (0.037) data 0.000 (0.001) loss 0.3152 (0.5350) lr 6.9098e-04 eta 0:00:50\n",
            "epoch [8/10] batch [420/592] time 0.036 (0.037) data 0.000 (0.001) loss 0.0768 (0.5345) lr 6.9098e-04 eta 0:00:49\n",
            "epoch [8/10] batch [440/592] time 0.037 (0.036) data 0.000 (0.001) loss 0.2639 (0.5222) lr 6.9098e-04 eta 0:00:48\n",
            "epoch [8/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.9956 (0.5232) lr 6.9098e-04 eta 0:00:48\n",
            "epoch [8/10] batch [480/592] time 0.036 (0.036) data 0.000 (0.001) loss 2.3027 (0.5238) lr 6.9098e-04 eta 0:00:47\n",
            "epoch [8/10] batch [500/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0204 (0.5151) lr 6.9098e-04 eta 0:00:46\n",
            "epoch [8/10] batch [520/592] time 0.035 (0.036) data 0.000 (0.001) loss 2.0957 (0.5281) lr 6.9098e-04 eta 0:00:45\n",
            "epoch [8/10] batch [540/592] time 0.035 (0.036) data 0.000 (0.001) loss 2.7305 (0.5305) lr 6.9098e-04 eta 0:00:44\n",
            "epoch [8/10] batch [560/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0017 (0.5404) lr 6.9098e-04 eta 0:00:44\n",
            "epoch [8/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0064 (0.5484) lr 6.9098e-04 eta 0:00:43\n",
            "epoch [9/10] batch [20/592] time 0.035 (0.049) data 0.000 (0.013) loss 0.0049 (0.7199) lr 4.1221e-04 eta 0:00:57\n",
            "epoch [9/10] batch [40/592] time 0.034 (0.043) data 0.000 (0.007) loss 0.0034 (0.4836) lr 4.1221e-04 eta 0:00:48\n",
            "epoch [9/10] batch [60/592] time 0.037 (0.040) data 0.000 (0.004) loss 0.0787 (0.4585) lr 4.1221e-04 eta 0:00:45\n",
            "epoch [9/10] batch [80/592] time 0.035 (0.039) data 0.000 (0.003) loss 0.1914 (0.4268) lr 4.1221e-04 eta 0:00:43\n",
            "epoch [9/10] batch [100/592] time 0.034 (0.038) data 0.000 (0.003) loss 0.0072 (0.4055) lr 4.1221e-04 eta 0:00:41\n",
            "epoch [9/10] batch [120/592] time 0.035 (0.038) data 0.000 (0.002) loss 1.2236 (0.3923) lr 4.1221e-04 eta 0:00:40\n",
            "epoch [9/10] batch [140/592] time 0.034 (0.038) data 0.000 (0.002) loss 0.0009 (0.3908) lr 4.1221e-04 eta 0:00:39\n",
            "epoch [9/10] batch [160/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.1015 (0.5039) lr 4.1221e-04 eta 0:00:37\n",
            "epoch [9/10] batch [180/592] time 0.034 (0.037) data 0.000 (0.002) loss 0.0513 (0.4865) lr 4.1221e-04 eta 0:00:36\n",
            "epoch [9/10] batch [200/592] time 0.037 (0.036) data 0.000 (0.002) loss 0.0276 (0.4871) lr 4.1221e-04 eta 0:00:35\n",
            "epoch [9/10] batch [220/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0020 (0.5208) lr 4.1221e-04 eta 0:00:34\n",
            "epoch [9/10] batch [240/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0039 (0.5186) lr 4.1221e-04 eta 0:00:34\n",
            "epoch [9/10] batch [260/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0055 (0.5126) lr 4.1221e-04 eta 0:00:33\n",
            "epoch [9/10] batch [280/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0004 (0.5023) lr 4.1221e-04 eta 0:00:32\n",
            "epoch [9/10] batch [300/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1575 (0.5011) lr 4.1221e-04 eta 0:00:31\n",
            "epoch [9/10] batch [320/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0690 (0.4880) lr 4.1221e-04 eta 0:00:30\n",
            "epoch [9/10] batch [340/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0092 (0.5021) lr 4.1221e-04 eta 0:00:30\n",
            "epoch [9/10] batch [360/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.0220 (0.5001) lr 4.1221e-04 eta 0:00:29\n",
            "epoch [9/10] batch [380/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.7607 (0.4891) lr 4.1221e-04 eta 0:00:28\n",
            "epoch [9/10] batch [400/592] time 0.038 (0.036) data 0.000 (0.001) loss 3.1797 (0.4877) lr 4.1221e-04 eta 0:00:27\n",
            "epoch [9/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 1.7295 (0.4949) lr 4.1221e-04 eta 0:00:27\n",
            "epoch [9/10] batch [440/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0054 (0.5061) lr 4.1221e-04 eta 0:00:26\n",
            "epoch [9/10] batch [460/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0363 (0.4993) lr 4.1221e-04 eta 0:00:25\n",
            "epoch [9/10] batch [480/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.1279 (0.4876) lr 4.1221e-04 eta 0:00:25\n",
            "epoch [9/10] batch [500/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0084 (0.4973) lr 4.1221e-04 eta 0:00:24\n",
            "epoch [9/10] batch [520/592] time 0.035 (0.036) data 0.000 (0.001) loss 0.0857 (0.4946) lr 4.1221e-04 eta 0:00:23\n",
            "epoch [9/10] batch [540/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0148 (0.4983) lr 4.1221e-04 eta 0:00:22\n",
            "epoch [9/10] batch [560/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.1129 (0.4961) lr 4.1221e-04 eta 0:00:22\n",
            "epoch [9/10] batch [580/592] time 0.032 (0.036) data 0.000 (0.001) loss 0.0127 (0.4962) lr 4.1221e-04 eta 0:00:21\n",
            "epoch [10/10] batch [20/592] time 0.034 (0.049) data 0.000 (0.013) loss 0.7710 (0.4845) lr 1.9098e-04 eta 0:00:28\n",
            "epoch [10/10] batch [40/592] time 0.037 (0.042) data 0.000 (0.007) loss 0.0066 (0.4663) lr 1.9098e-04 eta 0:00:23\n",
            "epoch [10/10] batch [60/592] time 0.034 (0.040) data 0.000 (0.005) loss 0.0281 (0.3890) lr 1.9098e-04 eta 0:00:21\n",
            "epoch [10/10] batch [80/592] time 0.038 (0.039) data 0.000 (0.003) loss 0.3535 (0.3964) lr 1.9098e-04 eta 0:00:19\n",
            "epoch [10/10] batch [100/592] time 0.033 (0.038) data 0.000 (0.003) loss 0.2808 (0.4532) lr 1.9098e-04 eta 0:00:18\n",
            "epoch [10/10] batch [120/592] time 0.038 (0.037) data 0.000 (0.002) loss 0.0001 (0.4555) lr 1.9098e-04 eta 0:00:17\n",
            "epoch [10/10] batch [140/592] time 0.039 (0.037) data 0.000 (0.002) loss 0.0185 (0.4711) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [160/592] time 0.035 (0.037) data 0.000 (0.002) loss 1.9629 (0.4691) lr 1.9098e-04 eta 0:00:16\n",
            "epoch [10/10] batch [180/592] time 0.038 (0.037) data 0.000 (0.002) loss 0.0003 (0.4838) lr 1.9098e-04 eta 0:00:15\n",
            "epoch [10/10] batch [200/592] time 0.035 (0.037) data 0.000 (0.002) loss 0.0016 (0.4813) lr 1.9098e-04 eta 0:00:14\n",
            "epoch [10/10] batch [220/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.0203 (0.4690) lr 1.9098e-04 eta 0:00:13\n",
            "epoch [10/10] batch [240/592] time 0.037 (0.037) data 0.000 (0.001) loss 0.0989 (0.4869) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [260/592] time 0.034 (0.037) data 0.000 (0.001) loss 0.0030 (0.4719) lr 1.9098e-04 eta 0:00:12\n",
            "epoch [10/10] batch [280/592] time 0.035 (0.037) data 0.000 (0.001) loss 1.5938 (0.4841) lr 1.9098e-04 eta 0:00:11\n",
            "epoch [10/10] batch [300/592] time 0.039 (0.037) data 0.000 (0.001) loss 0.0024 (0.4793) lr 1.9098e-04 eta 0:00:10\n",
            "epoch [10/10] batch [320/592] time 0.035 (0.036) data 0.000 (0.001) loss 2.0586 (0.4786) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [340/592] time 0.039 (0.036) data 0.000 (0.001) loss 0.1665 (0.4597) lr 1.9098e-04 eta 0:00:09\n",
            "epoch [10/10] batch [360/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.1611 (0.4781) lr 1.9098e-04 eta 0:00:08\n",
            "epoch [10/10] batch [380/592] time 0.038 (0.036) data 0.000 (0.001) loss 0.7627 (0.4707) lr 1.9098e-04 eta 0:00:07\n",
            "epoch [10/10] batch [400/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0167 (0.4683) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [420/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.2247 (0.4567) lr 1.9098e-04 eta 0:00:06\n",
            "epoch [10/10] batch [440/592] time 0.033 (0.036) data 0.000 (0.001) loss 2.2656 (0.4527) lr 1.9098e-04 eta 0:00:05\n",
            "epoch [10/10] batch [460/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.0113 (0.4682) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [10/10] batch [480/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0712 (0.4911) lr 1.9098e-04 eta 0:00:04\n",
            "epoch [10/10] batch [500/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.6899 (0.4994) lr 1.9098e-04 eta 0:00:03\n",
            "epoch [10/10] batch [520/592] time 0.034 (0.036) data 0.000 (0.001) loss 0.0032 (0.4888) lr 1.9098e-04 eta 0:00:02\n",
            "epoch [10/10] batch [540/592] time 0.036 (0.036) data 0.000 (0.001) loss 0.0076 (0.4917) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [560/592] time 0.036 (0.036) data 0.000 (0.001) loss 1.7197 (0.5016) lr 1.9098e-04 eta 0:00:01\n",
            "epoch [10/10] batch [580/592] time 0.033 (0.036) data 0.000 (0.001) loss 0.1774 (0.4991) lr 1.9098e-04 eta 0:00:00\n",
            "Checkpoint saved to output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3/prompt_learner/model.pth.tar-10\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 37/37 [00:40<00:00,  1.09s/it]\n",
            "=> result\n",
            "* total: 3,669\n",
            "* correct: 3,402\n",
            "* accuracy: 92.7%\n",
            "* error: 7.3%\n",
            "* macro_f1: 92.7%\n",
            "Elapsed: 0:04:21\n"
          ]
        }
      ],
      "source": [
        "!rm -r output\n",
        "\n",
        "!bash scripts/mip/main.sh eurosat vit_b16_c4_ep10_batch1 end 16 16 False\n",
        "!bash scripts/mip/main.sh oxford_flowers vit_b16_c4_ep10_batch1 end 16 16 False\n",
        "!bash scripts/mip/main.sh oxford_pets vit_b16_c4_ep10_batch1 end 16 16 False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python interpret_prompt.py output/eurosat/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-10 3\n",
        "!python interpret_prompt.py output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-10 3\n",
        "!python interpret_prompt.py output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-10 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QoOJNpUDk8B",
        "outputId": "2262714f-1279-41bb-d5e2-e22f4131e602"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return the top-3 matched words\n",
            "Size of token embedding: torch.Size([49408, 512])\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/interpret_prompt.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  prompt_learner = torch.load(fpath, map_location=\"cpu\")[\"state_dict\"]\n",
            "Size of context: torch.Size([16, 512])\n",
            "Size of distance matrix: torch.Size([16, 49408])\n",
            "1: ['1</w>', 'l</w>', 'lanta</w>'] ['1.2338', '1.2370', '1.2382']\n",
            "2: ['1</w>', '4</w>', '6</w>'] ['1.0621', '1.0747', '1.0748']\n",
            "3: ['1</w>', '5</w>', '3</w>'] ['1.0311', '1.0412', '1.0417']\n",
            "4: ['1</w>', '2</w>', '5</w>'] ['1.0206', '1.0294', '1.0298']\n",
            "5: ['1</w>', '2</w>', '5</w>'] ['1.0204', '1.0275', '1.0287']\n",
            "6: ['1</w>', '2</w>', '5</w>'] ['1.0199', '1.0261', '1.0278']\n",
            "7: ['1</w>', '2</w>', '5</w>'] ['1.0193', '1.0253', '1.0272']\n",
            "8: ['1</w>', '2</w>', '5</w>'] ['1.0182', '1.0241', '1.0262']\n",
            "9: ['1</w>', '2</w>', '5</w>'] ['1.0190', '1.0246', '1.0272']\n",
            "10: ['1</w>', '2</w>', '3</w>'] ['1.0207', '1.0259', '1.0284']\n",
            "11: ['1</w>', '2</w>', '3</w>'] ['1.0244', '1.0292', '1.0316']\n",
            "12: ['1</w>', '2</w>', '3</w>'] ['1.0283', '1.0330', '1.0349']\n",
            "13: ['1</w>', '2</w>', 'near</w>'] ['1.0322', '1.0371', '1.0384']\n",
            "14: ['1</w>', 'enjoying</w>', 'workinprogress</w>'] ['1.0410', '1.0429', '1.0436']\n",
            "15: ['enjoying</w>', 'workinprogress</w>', 'showcased</w>'] ['1.0606', '1.0621', '1.0631']\n",
            "16: ['joy', 'inhal', 'showcased</w>'] ['1.3204', '1.3221', '1.3224']\n",
            "Return the top-3 matched words\n",
            "Size of token embedding: torch.Size([49408, 512])\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/interpret_prompt.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  prompt_learner = torch.load(fpath, map_location=\"cpu\")[\"state_dict\"]\n",
            "Size of context: torch.Size([16, 512])\n",
            "Size of distance matrix: torch.Size([16, 49408])\n",
            "1: ['1</w>', '2</w>', 'four</w>'] ['1.1834', '1.1845', '1.1869']\n",
            "2: ['1</w>', 'de', 'feat</w>'] ['1.0507', '1.0559', '1.0565']\n",
            "3: ['1</w>', 'wear</w>', '2</w>'] ['1.0130', '1.0227', '1.0238']\n",
            "4: ['1</w>', '5</w>', '2</w>'] ['1.0069', '1.0163', '1.0173']\n",
            "5: ['1</w>', '5</w>', '2</w>'] ['1.0040', '1.0138', '1.0149']\n",
            "6: ['1</w>', '5</w>', '2</w>'] ['1.0027', '1.0125', '1.0138']\n",
            "7: ['1</w>', '5</w>', '2</w>'] ['1.0030', '1.0122', '1.0139']\n",
            "8: ['1</w>', '5</w>', '2</w>'] ['1.0020', '1.0108', '1.0128']\n",
            "9: ['1</w>', '5</w>', '2</w>'] ['1.0021', '1.0109', '1.0126']\n",
            "10: ['1</w>', '5</w>', '2</w>'] ['1.0020', '1.0104', '1.0123']\n",
            "11: ['1</w>', '5</w>', '4</w>'] ['1.0032', '1.0113', '1.0135']\n",
            "12: ['1</w>', '5</w>', '4</w>'] ['1.0057', '1.0140', '1.0161']\n",
            "13: ['1</w>', '5</w>', '4</w>'] ['1.0108', '1.0189', '1.0207']\n",
            "14: ['1</w>', '5</w>', '4</w>'] ['1.0220', '1.0291', '1.0307']\n",
            "15: ['1</w>', '4</w>', 'tv</w>'] ['1.0415', '1.0509', '1.0509']\n",
            "16: ['tribal</w>', 'argyll</w>', 'oaxaca</w>'] ['1.5883', '1.5903', '1.5908']\n",
            "Return the top-3 matched words\n",
            "Size of token embedding: torch.Size([49408, 512])\n",
            "/content/drive/MyDrive/Colab Notebooks/MIP/interpret_prompt.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  prompt_learner = torch.load(fpath, map_location=\"cpu\")[\"state_dict\"]\n",
            "Size of context: torch.Size([16, 512])\n",
            "Size of distance matrix: torch.Size([16, 49408])\n",
            "1: ['gloucestershire</w>', 'sur</w>', 'color</w>'] ['2.6987', '2.6989', '2.7009']\n",
            "2: ['hikers</w>', 'beverage</w>', 'flavor</w>'] ['1.3904', '1.3975', '1.3988']\n",
            "3: ['bi', 'ess</w>', 'elovers</w>'] ['1.0936', '1.0940', '1.0964']\n",
            "4: ['alized</w>', 'bi', 'housing</w>'] ['1.0388', '1.0405', '1.0422']\n",
            "5: ['alized</w>', 'le</w>', 'movie</w>'] ['1.0225', '1.0228', '1.0231']\n",
            "6: ['stones</w>', 'le</w>', 'bi'] ['1.0098', '1.0099', '1.0109']\n",
            "7: ['1</w>', '2</w>', 'le</w>'] ['1.0011', '1.0027', '1.0033']\n",
            "8: ['1</w>', '2</w>', 'wood</w>'] ['0.9928', '0.9942', '0.9965']\n",
            "9: ['1</w>', '2</w>', 'wood</w>'] ['0.9857', '0.9885', '0.9900']\n",
            "10: ['1</w>', 'wood</w>', '2</w>'] ['0.9887', '0.9914', '0.9919']\n",
            "11: ['wood</w>', '1</w>', '2</w>'] ['1.0104', '1.0111', '1.0148']\n",
            "12: ['wood</w>', '1</w>', 'near</w>'] ['1.0205', '1.0216', '1.0244']\n",
            "13: ['wood</w>', '1</w>', 'inside</w>'] ['1.0484', '1.0506', '1.0524']\n",
            "14: ['cont</w>', 'inter', 'ing'] ['1.1521', '1.1533', '1.1536']\n",
            "15: ['remember</w>', 'tracker</w>', 'rumble</w>'] ['1.3485', '1.3486', '1.3491']\n",
            "16: ['wax', 'flann', 'portu'] ['4.2921', '4.2949', '4.2980']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python parse_test_res.py output/eurosat/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
        "!python parse_test_res.py output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
        "!python parse_test_res.py output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDB6HZhEE9x5",
        "outputId": "1edc4ea1-4fc7-4b3a-b0c7-42099008e3c1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing files in output/eurosat/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
            "file: output/eurosat/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/log.txt. accuracy: 72.50%. \n",
            "file: output/eurosat/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2/log.txt. accuracy: 69.20%. \n",
            "file: output/eurosat/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3/log.txt. accuracy: 71.40%. \n",
            "===\n",
            "Summary of directory: output/eurosat/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
            "* accuracy: 71.03% +- 1.37%\n",
            "===\n",
            "Parsing files in output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
            "file: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/log.txt. accuracy: 89.80%. \n",
            "file: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2/log.txt. accuracy: 89.60%. \n",
            "file: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3/log.txt. accuracy: 91.10%. \n",
            "===\n",
            "Summary of directory: output/oxford_flowers/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
            "* accuracy: 90.17% +- 0.66%\n",
            "===\n",
            "Parsing files in output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
            "file: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed1/log.txt. accuracy: 92.80%. \n",
            "file: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed2/log.txt. accuracy: 93.20%. \n",
            "file: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend/seed3/log.txt. accuracy: 92.70%. \n",
            "===\n",
            "Summary of directory: output/oxford_pets/MIP/vit_b16_c4_ep10_batch1_16shots/nctx16_cscFalse_ctpend\n",
            "* accuracy: 92.90% +- 0.22%\n",
            "===\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}